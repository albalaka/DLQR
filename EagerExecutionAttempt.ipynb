{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import control as ct\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as reg\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVNFull():\n",
    "    def __init__(self, loc,covariance_matrix):\n",
    "        self.loc = loc\n",
    "        self.cov = covariance_matrix\n",
    "        self.shape = tf.cast(self.cov.shape[-1],dtype=tf.float64)\n",
    "#         print(self.loc.shape)\n",
    "#         print(self.cov.shape)\n",
    "        assert(self.loc.shape[-2]==self.cov.shape[-1]), \"mean and covariance must have same n\"\n",
    "        assert(self.cov.shape[-1]==self.cov.shape[-2]),'covariance must have shape [...,n,n]'\n",
    "        \n",
    "    def log_prob(self, value):\n",
    "        value = value\n",
    "#         print('Inside prob function')\n",
    "#         print('value',value.shape)\n",
    "        assert(value.shape[-1]==self.loc.shape[-1] and value.shape[-2]==self.loc.shape[-2]),'value must have same last 2 dimensions as loc'\n",
    "#         print('cov',self.cov.shape)\n",
    "\n",
    "#         print('cov',self.cov)\n",
    "        cov_inv = tf.linalg.inv(self.cov+1e-4*tf.eye(self.shape,dtype=tf.float64))\n",
    "#         cov_inv = None\n",
    "#         multiplier = 1e-8\n",
    "#         while cov_inv == None:\n",
    "#             try:\n",
    "#                 if multiplier != 1e-8:\n",
    "#                     print('MVNFull covariance inverser failed, trying identity multiplier ',multiplier)\n",
    "#                 cov_inv = tf.linalg.inv(self.cov+multiplier*tf.eye(self.shape, dtype = tf.float64))\n",
    "#             except:\n",
    "#                 multiplier *= 10\n",
    "\n",
    "                \n",
    "                \n",
    "#         print('cov_inv',cov_inv)\n",
    "#         print('cov_inv',cov_inv.shape)\n",
    "#         print(cov_inv.numpy())\n",
    "        cov_det = tf.linalg.det(self.cov)\n",
    "#         print('cov_det', cov_det.shape)\n",
    "#         print(cov_det.numpy())\n",
    "        denomenator = tf.math.sqrt(tf.math.pow((tf.cast(2*np.pi,dtype = tf.float64)),self.shape)*cov_det)\n",
    "#         print('denomenator', denomenator.numpy())\n",
    "        diff = value-self.loc\n",
    "#         print('diff', diff.numpy())\n",
    "        numerator = tf.squeeze(tf.math.exp((-0.5)*tf.matmul(tf.matmul(diff,cov_inv, transpose_a=True),diff)))\n",
    "#         print('numerator', numerator.numpy())\n",
    "#         print('final value', tf.math.log(tf.math.divide(numerator,denomenator)).numpy())\n",
    "#         print('leaving prob function')\n",
    "        return tf.math.log(tf.math.divide(numerator,denomenator)+1e-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_filter_fn(A,B,u,g,C,sigma,l_a_posteriori,P_a_posteriori,z):\n",
    "    '''Calculates prior distribution based on the previous posterior distribution\n",
    "        and the current residual updates posterior distribution based on the new\n",
    "        prior distribution\n",
    "    '''\n",
    "#     print('z',z)\n",
    "#     print('A', A)\n",
    "#     print('B',B)\n",
    "#     print('u',u)\n",
    "#     print('g',g)\n",
    "#     print('sigma',sigma)\n",
    "#     print('C', C)\n",
    "#     print('l_a_posteriori', l_a_posteriori)\n",
    "#     print('P_a_posteriori', P_a_posteriori)\n",
    "    _I = tf.eye(int(A.shape[0]), dtype = tf.float64)\n",
    "    \n",
    "    z = tf.expand_dims(z,-1)\n",
    "    l_a_priori = tf.matmul(A,l_a_posteriori) + tf.matmul(B,u)\n",
    "#     print('l_a_priori',l_a_priori)\n",
    "    P_a_priori = tf.matmul(tf.matmul(A,P_a_posteriori), A, transpose_b = True) + tf.matmul(g,g, transpose_b=True)\n",
    "#     print('P_a_priori',P_a_priori)\n",
    "    y_pre = z - tf.matmul(C,l_a_priori)\n",
    "#     print('y_pre', y_pre)\n",
    "\n",
    "    S = tf.matmul(sigma, sigma, transpose_b=True) + \\\n",
    "        tf.matmul(tf.matmul(C, P_a_priori), C, transpose_b=True)\n",
    "#     print('S',S)\n",
    "    S_inv = tf.linalg.inv(S+1e-4*tf.eye(int(C.shape[0]), dtype = tf.float64))\n",
    "#     S_inv = None\n",
    "#     try:\n",
    "#         S_inv = tf.linalg.inv(S)\n",
    "#         print('success?')\n",
    "#     except InvalidArgumentError:\n",
    "#         print('Outer except')\n",
    "#         multiplier = 1e-10\n",
    "#         while S_inv == None:\n",
    "#             try:\n",
    "#                 if multiplier != 1e-10:\n",
    "#                     print('KF inv failed, trying identity multiplier ',multiplier)\n",
    "#                 cov_inv = tf.linalg.inv(self.cov+multiplier*tf.eye(S.shape[0], dtype = tf.float64))\n",
    "#             except InvalidArgumentError:\n",
    "#                 multiplier *= 10\n",
    "#         pass\n",
    "\n",
    "\n",
    "#     print('S_inv', S_inv)\n",
    "    K = tf.matmul(tf.matmul(P_a_priori, C, transpose_b=True), S_inv)\n",
    "#     print('K', K)\n",
    "    l_a_posteriori = l_a_priori + tf.matmul(K,y_pre)\n",
    "#     print('l_a_posteriori', l_a_posteriori)\n",
    "    I_KC = _I-tf.matmul(K,C)\n",
    "#     print('I-KC', I_KC)\n",
    "    P_a_posteriori = tf.matmul(tf.matmul(I_KC, P_a_priori), I_KC, transpose_b=True) + \\\n",
    "                        tf.matmul(tf.matmul(K,tf.matmul(sigma, sigma, transpose_b = True)),\n",
    "                                K, transpose_b=True)\n",
    "#     print('P_a_posteriori',P_a_posteriori)\n",
    "    y_post = z-tf.matmul(C,l_a_posteriori)\n",
    "#     print('y_post', y_post)\n",
    "    pred = tf.matmul(C, l_a_posteriori)\n",
    "#     print('pred', pred)\n",
    "        \n",
    "    return A,B,u,g,C,sigma,l_a_posteriori,P_a_posteriori,z, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Epoch(object):\n",
    "    def __init__(self, view = False, initial_state_variation = [0,0,0,0]):\n",
    "        self.view = view\n",
    "        self.m = 4\n",
    "        self.dim_z = self.m\n",
    "        self.n = 4\n",
    "        self.r = 1\n",
    "        self.lstm_input_dim = self.m+4\n",
    "        self.sigma_upper_bound = 10\n",
    "        self.sigma_lower_bound = 0\n",
    "        self.g_upper_bound = 10\n",
    "        self.g_lower_bound = 0\n",
    "        self.mu_0_upper_bound = 10\n",
    "        self.mu_0_lower_bound = 0\n",
    "        self.Sigma_0_upper_bound = 10\n",
    "        self.Sigma_0_lower_bound = 0\n",
    "        self.beta = 0.01\n",
    "        thetaacc_error = 0\n",
    "\n",
    "        self.initial_variance_estimate = 1\n",
    "\n",
    "        self.lstm_sizes = [128,64]\n",
    "        self.env = gym.make('Custom_CartPole-v0', thetaacc_error=thetaacc_error, initial_state=initial_state_variation)\n",
    "        gravity = self.env.gravity\n",
    "        cart_mass = self.env.masscart\n",
    "        pole_mass = self.env.masspole\n",
    "        pole_length = self.env.length\n",
    "        self.env_params = tf.expand_dims(np.array([gravity, cart_mass,pole_mass,pole_length],\n",
    "                                             dtype=np.float64),0)\n",
    "        \n",
    "        self.variables = []\n",
    "        \n",
    "    def build_LSTM(self):\n",
    "        lstms = [tf.contrib.rnn.LSTMCell(size, reuse=tf.get_variable_scope().reuse) for size in self.lstm_sizes]\n",
    "        dropouts = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = 0.5) for lstm in lstms]\n",
    "\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell(dropouts)\n",
    "#         print(self.cell.trainable_variables)\n",
    "#         print(self.cell.trainable_weights)\n",
    "#         self.variables.append(self.cell.trainable_variables)\n",
    "        return self\n",
    "    \n",
    "    def get_variables(self):\n",
    "        return self.variables\n",
    "    def reset_variables(self):\n",
    "        self.variables = []\n",
    "        return self\n",
    "    \n",
    "    def likelihood_fn(self, params, inputs):\n",
    "        A, B, u, g, C, sigma, l_filtered, P_filtered = inputs\n",
    "        mu_1, Sigma_1 = params\n",
    "#         print('A',len(A))\n",
    "#         print('B',len(B))\n",
    "#         print('u',len(u))\n",
    "#         print('C',len(C))\n",
    "#         print('g',len(g))\n",
    "#         print('sigma',len(sigma))\n",
    "#         print('l_filtered',len(l_filtered))\n",
    "#         print('p_filtered',len(P_filtered))\n",
    "#         print('mu_1',mu_1.shape)\n",
    "#         print('Sigma_1',Sigma_1.shape)\n",
    "        mu = [mu_1]\n",
    "        Sigma = [Sigma_1]\n",
    "        assert(len(A)==len(B) and len(B)==len(u) and len(u)==len(C) and len(C)==len(sigma) and \n",
    "               len(sigma)==len(l_filtered) and len(l_filtered)==len(P_filtered)),\"Not all sequences are same length\"\n",
    "        for i in range(len(A)):\n",
    "            mu.append(tf.matmul(C[i], tf.add(tf.matmul(A[i],l_filtered[i]), tf.matmul(B[i],u[i]))))\n",
    "            temp = tf.matmul(tf.matmul(A[i], P_filtered[i]), A[i], transpose_b=True) + \\\n",
    "                        tf.matmul(g[i], g[i], transpose_b=True)\n",
    "            Sigma.append(tf.matmul(tf.matmul(C[i], temp), C[i], transpose_b=True) + \\\n",
    "                        tf.matmul(sigma[i],sigma[i],transpose_b=True))\n",
    "        return mu,Sigma\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.reset_variables()\n",
    "        rewards = 0\n",
    "        A_all = []\n",
    "        B_all = []\n",
    "        u_all = []\n",
    "        g_all = []\n",
    "        C_all = []\n",
    "        sigma_all = []\n",
    "        l_a_posteriori = []\n",
    "        P_a_posteriori = []\n",
    "        env_states = []\n",
    "        preds = []\n",
    "        all_KF_params = [A_all,B_all,u_all,g_all,C_all,sigma_all,\n",
    "                         l_a_posteriori,P_a_posteriori,env_states,preds]\n",
    "\n",
    "        '''p-quantile loss'''\n",
    "        Q50_numerator = np.zeros(4)\n",
    "        Q90_numerator = np.zeros(4)\n",
    "        \n",
    "        '''Build LSTM'''\n",
    "        self.build_LSTM()\n",
    "        '''Get initial lstm state and input, get first output/state'''\n",
    "        initial_state = self.cell.get_initial_state(batch_size=1,dtype = tf.float64)\n",
    "        initial_input = tf.concat((self.env_params, np.zeros(shape = [1,4])),axis=1)\n",
    "        output_single, state_single = self.cell(inputs=initial_input, state=initial_state)\n",
    "#             mu_0,Sigma_0,l_0 = self.affine_transformation(output_single,first=True)\n",
    "\n",
    "\n",
    "        '''Calculate mu_0,Sigma_0, distribution using initial LSTM output'''\n",
    "        container = tf.contrib.eager.EagerVariableStore()\n",
    "        with container.as_default():\n",
    "            mu_0 = tf.layers.dense(output_single, self.m, kernel_regularizer = reg.l2(self.beta),\n",
    "                                       bias_regularizer = reg.l2(self.beta),\n",
    "                                       name = 'mu_0dense', reuse = True)\n",
    "            Sigma_0 = tf.layers.dense(output_single, self.m, kernel_regularizer = reg.l2(self.beta),\n",
    "                                          bias_regularizer = reg.l2(self.beta),\n",
    "                                          name = 'Sigma_0dense', reuse = True)\n",
    "        mu_0 = tf.reshape(mu_0, shape = (self.m,1))\n",
    "#         mu_0 = tf.reshape(layers.Dense(self.m, kernel_regularizer = reg.l2(self.beta),\n",
    "#                                        bias_regularizer = reg.l2(self.beta),\n",
    "#                                        name = 'mu_0dense')(output_single), shape=(self.m,1))\n",
    "        mu_0 = ((self.mu_0_upper_bound-self.mu_0_lower_bound)/(1+tf.exp(-mu_0)))+self.mu_0_lower_bound\n",
    "        Sigma_0 = tf.reshape(Sigma_0, shape = (self.m,1))\n",
    "#         Sigma_0 = tf.reshape(layers.Dense(self.m, kernel_regularizer = reg.l2(self.beta),\n",
    "#                                           bias_regularizer = reg.l2(self.beta),\n",
    "#                                           name = 'Sigma_0dense')(output_single),shape = (self.m,1))\n",
    "        Sigma_0 = ((self.Sigma_0_upper_bound-self.Sigma_0_lower_bound)/(1+tf.exp(-Sigma_0)))+\\\n",
    "                        self.Sigma_0_lower_bound\n",
    "        Sigma_0 = tf.matmul(Sigma_0,Sigma_0,transpose_b=True)+tf.eye(4, dtype=tf.float64)*1e-8\n",
    "        l_0_distribution = tfd.MultivariateNormalFullCovariance(loc = tf.squeeze(mu_0),\n",
    "                                                                covariance_matrix= Sigma_0,\n",
    "                                                                validate_args=True)\n",
    "        l_0 = tf.expand_dims(l_0_distribution.sample(),1)\n",
    "        l_a_posteriori.append(l_0)\n",
    "        P_a_posteriori.append(self.initial_variance_estimate*tf.eye(self.m, dtype = tf.float64))\n",
    "\n",
    "        observation=self.env.reset()\n",
    "        first_pass = True\n",
    "        done = False\n",
    "        while not done:\n",
    "            if self.view:\n",
    "                self.env.render()\n",
    "            '''Get lstm outputs'''\n",
    "            with container.as_default():\n",
    "                A = tf.layers.dense(output_single, self.m*self.n, kernel_regularizer = reg.l2(self.beta),\n",
    "                                 bias_regularizer = reg.l2(self.beta),\n",
    "                                 name = 'A_dense', reuse = True)\n",
    "                B = tf.layers.dense(output_single, self.m*self.r, kernel_regularizer = reg.l2(self.beta),\n",
    "                                    bias_regularizer = reg.l2(self.beta),\n",
    "                                    name = 'B_dense', reuse = True)\n",
    "                g = tf.layers.dense(output_single, self.m, kernel_regularizer = reg.l2(self.beta),\n",
    "                                    bias_regularizer = reg.l2(self.beta),\n",
    "                                    name = 'g_dense', reuse = True)\n",
    "                sigma = tf.layers.dense(output_single, self.dim_z, kernel_regularizer = reg.l2(self.beta),\n",
    "                                        bias_regularizer = reg.l2(self.beta),\n",
    "                                        name = 'sigma_dense', reuse = True)\n",
    "#             print('container total',len(container.variables()))\n",
    "#             print('container trainable',len(container.trainable_variables()))\n",
    "            if first_pass:\n",
    "                self.variables.extend(container.trainable_variables())\n",
    "            first_pass = False\n",
    "            A = tf.reshape(A, shape = (self.m,self.n))\n",
    "            B = tf.reshape(B, shape = (self.m,self.r))\n",
    "            g = tf.reshape(g, shape = (self.m, 1))\n",
    "            g = ((self.g_upper_bound-self.g_lower_bound)/(1+tf.exp(-g)))+self.g_lower_bound\n",
    "            sigma = tf.reshape(sigma, shape = (self.dim_z,1))\n",
    "            sigma = ((self.sigma_upper_bound-self.sigma_lower_bound)/(1+tf.exp(-sigma)))+self.sigma_lower_bound\n",
    "            no_control = tf.zeros(shape = [1,self.r], dtype=tf.float64)\n",
    "            C = tf.eye(self.dim_z, dtype = tf.float64)\n",
    "            observation, reward, done, info = self.env.step(tf.squeeze(no_control))\n",
    "            '''Calculate:\n",
    "                A,B,u,g,C,sigma,l_a_posteriori,P_a_posteriori,env_states'''\n",
    "            KF_update = forward_filter_fn(A, B, no_control,g, C, sigma,l_a_posteriori[-1],P_a_posteriori[-1],\n",
    "                                          tf.convert_to_tensor(observation,dtype=tf.float64))\n",
    "            '''Update lists:\n",
    "                A_all,B_all,u_all,g_all,C_all,sigma_all,l_a_posteriori,P_a_posteriori,env_states'''\n",
    "            for KF_single,KF_param  in zip(KF_update,all_KF_params):\n",
    "                KF_param.append(KF_single)\n",
    "\n",
    "            rewards+=1\n",
    "\n",
    "            next_input = tf.concat((self.env_params,tf.transpose(env_states[-1])),axis=1)\n",
    "            output_single,state_single=self.cell(inputs=next_input,state=state_single)\n",
    "        if self.view:\n",
    "            self.env.close()\n",
    "\n",
    "#         param_names = ['A_all','B_all','u_all','g_all','C_all','sigma_all',\n",
    "#                        'l_a_posteriori','P_a_posteriori','env_states','preds']\n",
    "#             for name,KF_param in zip(param_names,all_KF_params):\n",
    "#                 print(name,len(KF_param), KF_param[0].shape)\n",
    "        self.variables.extend(self.cell.trainable_variables)\n",
    "#         print('LSTM cell trainable',len(self.cell.trainable_variables))\n",
    "#         print('Rewards', self.rewards)\n",
    "#         print('VARIABLES',[x.name for x in self.cell.trainable_variables])\n",
    "#         print('\\n\\n\\nWEIGHTS',[x.name for x in self.cell.trainable_weights])\n",
    "\n",
    "        mu_1 = tf.add(tf.matmul(A_all[0], mu_0),tf.matmul(B_all[0],u_all[0]))\n",
    "        Sigma_1 = tf.add(tf.matmul(tf.matmul(C_all[0],Sigma_0),C_all[0], transpose_b=True),\n",
    "                     tf.matmul(sigma_all[0],sigma_all[0],transpose_b=True))\n",
    "#         print(mu_1.shape)\n",
    "#         print(Sigma_1.shape)\n",
    "        if rewards > 1:\n",
    "            mu,Sigma = self.likelihood_fn((mu_1,Sigma_1),(A_all,B_all,u_all,g_all,\n",
    "                                                     C_all,sigma_all,\n",
    "                                                     l_a_posteriori[1:],\n",
    "                                                     P_a_posteriori[1:]))\n",
    "#         for item in self.variables:\n",
    "#             print(item)\n",
    "\n",
    "        '''p-quantile loss'''\n",
    "#         print(rewards)\n",
    "#         print(len(preds))\n",
    "        for i in range(Q50_numerator.shape[0]):\n",
    "            for j in range(rewards):\n",
    "                Q50_numerator[i] += QL(0.5, preds[j][i], env_states[j][i])\n",
    "                Q90_numerator[i] += QL(0.9, preds[j][i], env_states[j][i])\n",
    "#         print('Q50',Q50_numerator.shape)\n",
    "#         print('Q90',Q90_numerator.shape)\n",
    "#         print('abs',np.abs(np.squeeze(np.array(env_states))).shape)\n",
    "        Q_denomenator = np.sum(np.abs(np.squeeze(np.array(env_states))), axis = 0)\n",
    "        for idx,slot in enumerate(Q_denomenator):\n",
    "            if slot==0:\n",
    "                Q_denomenator[idx]+=np.abs(np.random.normal(loc = 0.0, scale = 1e-10))\n",
    "        pq50_loss = 2*np.divide(Q50_numerator,Q_denomenator)\n",
    "        pq90_loss = 2*np.divide(Q90_numerator,Q_denomenator)\n",
    "#         print('Q_denom', Q_denomenator)\n",
    "#         print('pq50',pq50_loss)\n",
    "#         print('pq90',pq90_loss)\n",
    "        \n",
    "        '''Compute Likelihood of observations given KF evaluation'''\n",
    "        likelihoods = []\n",
    "        for i in range(rewards):\n",
    "            z_distribution = MVNFull(loc = mu[i], covariance_matrix = Sigma[i])\n",
    "            likelihoods.append(z_distribution.log_prob(env_states[i]))\n",
    "            \n",
    "        return likelihoods, rewards, pq50_loss, pq90_loss, preds\n",
    "\n",
    "def QL(rho, z, z_pred):\n",
    "    if z > z_pred:\n",
    "        return rho*(z-z_pred)\n",
    "    else:\n",
    "        return (1-rho)*(z_pred-z)\n",
    "    \n",
    "def loss(epoch):\n",
    "#     print(epoch.__dict__)\n",
    "#     mu, Sigma, rewards = epoch()\n",
    "#     likelihoods = []\n",
    "#     print('mu',len(mu),mu[0].shape)\n",
    "#     print('Sigma',len(Sigma),Sigma[0].shape)\n",
    "#         print('rewards',self.rewards)\n",
    "#         print('env_states', len(env_states))\n",
    "#     for i in range(rewards):\n",
    "#         z_distribution = MVNFull(loc = mu[i], covariance_matrix = Sigma[i])\n",
    "#                                  0.0001*tf.eye(epoch.m, dtype = tf.float64)+Sigma[i])\n",
    "#             print(id(z_distribution))\n",
    "#         likelihoods.append(z_distribution.log_prob(epoch.env_states[i]))\n",
    "    likelihoods, rewards, pq50_loss, pq90_loss, preds = epoch()\n",
    "    loss = tf.Variable([0.0], trainable = False, dtype = tf.float64)\n",
    "#         print(len(likelihoods),id(likelihoods))\n",
    "    for loss_term in likelihoods:\n",
    "#         print(item)\n",
    "        loss = tf.add(loss,-loss_term)\n",
    "    return loss, rewards, pq50_loss, pq90_loss, preds\n",
    "    \n",
    "\n",
    "def grad(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value, rewards, pq50_loss, pq90_loss, preds = loss(epoch)\n",
    "#         print(tape.gradient(loss_value,epoch.get_variables()))\n",
    "#         print(len(tape.watched_variables()))\n",
    "#         for var in tape.watched_variables():\n",
    "#               print(var.name)\n",
    "    return tape.gradient(loss_value, epoch.get_variables()), loss_value.numpy(), rewards, pq50_loss, pq90_loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(epoch.get_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in epoch.get_variables():\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = Epoch(initial_state_variation=[0,0,0,0])\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "losses = []\n",
    "rewards = []\n",
    "p50_losses = []\n",
    "p90_losses = []\n",
    "epoch_vars = ['A_dense/bias:0', 'A_dense/kernel:0', 'B_dense/bias:0', 'B_dense/kernel:0',\n",
    "              'Sigma_0dense/bias:0', 'Sigma_0dense/kernel:0', 'g_dense/bias:0', 'g_dense/kernel:0',\n",
    "              'mu_0dense/bias:0', 'mu_0dense/kernel:0', 'sigma_dense/bias:0', 'sigma_dense/kernel:0',\n",
    "              'multi_rnn_cell/cell_0/lstm_cell/kernel:0', 'multi_rnn_cell/cell_0/lstm_cell/bias:0',\n",
    "              'multi_rnn_cell/cell_1/lstm_cell/kernel:0', 'multi_rnn_cell/cell_1/lstm_cell/bias:0']\n",
    "grad_norms = [[var] for var in epoch_vars]\n",
    "predicted_trajectories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(100):\n",
    "    grads, loss_, reward_, pq50, pq90, pred = grad(epoch)\n",
    "    losses.extend(loss_)\n",
    "    rewards.append(reward_)\n",
    "    p50_losses.append(pq50)\n",
    "    p90_losses.append(pq90)\n",
    "#     print([var.name for var in epoch.get_variables()])\n",
    "#     print('grad',grads)\n",
    "#     print('grad norm', [np.linalg.norm(grad_) for grad_ in grads])\n",
    "    for idx, grad_ in enumerate(grads):\n",
    "        grad_norms[idx].append(np.linalg.norm(grad_))\n",
    "    predicted_trajectories.append(pred)\n",
    "    \n",
    "    clipped_grads = [tf.clip_by_norm(grad_, 1.) for grad_ in grads]\n",
    "#     for idx in range(len(grads)):\n",
    "#         print(np.max(grads[idx].numpy()), np.min(grads[idx].numpy()),\n",
    "#               np.max(clipped_grads[idx].numpy()), np.min(clipped_grads[idx].numpy()))\n",
    "\n",
    "#     for grad_, clipped_grad in zip(grads,clipped_grads):\n",
    "#         print(grad_.shape, clipped_grad.shape)\n",
    "    optimizer.apply_gradients(zip(clipped_grads,epoch.get_variables()))\n",
    "#     if (i+1)%50 == 0:\n",
    "#         print('Epoch {}'.format(i+1))\n",
    "#         print('Minutes elapsed: {}'.format((time.time()-start)/60))\n",
    "#         print('last 50 average loss: {}'.format(np.mean(losses[-50:])))\n",
    "    print('Epoch {}'.format(i+1))\n",
    "    print('Minutes elapsed: {}'.format((time.time()-start)/60))\n",
    "    print('last loss: {}, reward: {}, loss/reward: {}'.format(loss_, reward_, loss_/reward_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for norm in grad_norms:\n",
    "    plt.plot(norm[1:])\n",
    "#     plt.title(norm[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([loss/reward for loss, reward in zip(np.array(p50_losses)[:,2],rewards)], label  ='p50')\n",
    "plt.plot([loss/reward for loss,reward in zip(np.array(p90_losses)[:,2],rewards)], label = 'p90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(np.array(p50_losses).shape[1]):\n",
    "    plt.plot(np.array(p50_losses)[:,i], label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(np.array(p90_losses).shape[1]):\n",
    "    plt.plot(np.array(p90_losses)[:,i], label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([loss/reward for loss,reward in zip(losses,rewards)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.train(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
