{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as reg\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class non_linear_Model(object):\n",
    "    def __init__(self,thetaacc_error = 0, env_params_variation = [0,0,0,0], initial_state_variation = [0,0,0,0]):\n",
    "        self.lstm_sizes = [128,64]\n",
    "        self.thetaacc_error = thetaacc_error\n",
    "        self.global_epoch = 0\n",
    "        self.env = gym.make('Custom_CartPole-v0', thetaacc_error=self.thetaacc_error, env_params_var=env_params_variation, initial_state_var=initial_state_variation)\n",
    "        gravity = self.env.gravity\n",
    "        cart_mass = self.env.masscart\n",
    "        pole_mass = self.env.masspole\n",
    "        pole_length = self.env.length\n",
    "        self.env_params = tf.expand_dims(np.array([gravity, cart_mass,pole_mass,pole_length],\n",
    "                                                  dtype=np.float64),0)\n",
    "        self.prediction = tf.keras.Sequential([layers.Dense(64, activation=tf.nn.leaky_relu, kernel_regularizer = reg.l2(.01),bias_regularizer = reg.l2(.01), name = 'prediction1'),\n",
    "                                               layers.Dense(64, activation=tf.nn.leaky_relu, kernel_regularizer = reg.l2(.01),bias_regularizer = reg.l2(.01), name = 'prediction2'),\n",
    "                                               layers.Dense(32, activation=tf.nn.leaky_relu, kernel_regularizer = reg.l2(.01),bias_regularizer = reg.l2(.01), name = 'prediction3'),\n",
    "                                               layers.Dense(4, activation=tf.nn.leaky_relu, kernel_regularizer = reg.l2(.01),bias_regularizer = reg.l2(.01), name = 'prediction5')])\n",
    "        self.u = tf.zeros(shape=[1,1], dtype = tf.float64)\n",
    "        \n",
    "#     def prediction(self,input):\n",
    "        \n",
    "        \n",
    "    def build_LSTM(self):\n",
    "        lstms = [tf.contrib.rnn.LSTMCell(size, reuse=tf.get_variable_scope().reuse) for size in self.lstm_sizes]\n",
    "        dropouts = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = 0.5) for lstm in lstms]\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell(dropouts)\n",
    "        return self\n",
    "\n",
    "    def look_ahead_prediction(self, prediction_horizon, observation, output_single, state_single):\n",
    "        LA_preds = [tf.expand_dims(tf.convert_to_tensor(observation,dtype=tf.float64),0)]\n",
    "        LA_output_single = output_single\n",
    "        LA_state_single = state_single\n",
    "        LA_pred = tf.convert_to_tensor(observation,dtype=tf.float64)\n",
    "        for i in range(prediction_horizon-1):\n",
    "            LA_pred = self.prediction(output_single)\n",
    "            LA_next_input = tf.concat((self.env_params,LA_pred),axis=1)\n",
    "            LA_preds.append(LA_pred)\n",
    "            LA_output_single,LA_state_single=self.cell(inputs=LA_next_input,state=LA_state_single)\n",
    "        return LA_preds\n",
    "    \n",
    "    def __call__(self,prediction_horizon):\n",
    "        rewards = 0\n",
    "        predictions = []\n",
    "        LA_predictions = []\n",
    "        trajectory = []\n",
    "        self.build_LSTM()\n",
    "        observation = self.env.reset()\n",
    "        trajectory.append(tf.expand_dims(tf.convert_to_tensor(observation,dtype=tf.float64),0))\n",
    "        initial_state = self.cell.get_initial_state(batch_size=1,dtype = tf.float64)\n",
    "        initial_input = tf.concat((self.env_params,tf.expand_dims(tf.convert_to_tensor(observation,dtype=tf.float64),0)),\n",
    "                                  axis=1)\n",
    "        output_single, state_single = self.cell(inputs=initial_input, state=initial_state)\n",
    "        predictions.append(self.prediction(output_single))\n",
    "        done = False\n",
    "        while not done:\n",
    "            if rewards%prediction_horizon==0:\n",
    "                LA_predictions.extend(self.look_ahead_prediction(prediction_horizon,observation,output_single,state_single))\n",
    "            observation, reward, done, info = self.env.step(tf.squeeze(self.u))\n",
    "            predictions.append(self.prediction(output_single))\n",
    "            trajectory.append(tf.expand_dims(tf.convert_to_tensor(observation,dtype=tf.float64),0))\n",
    "            next_input = tf.concat((self.env_params,tf.expand_dims(tf.convert_to_tensor(observation, dtype=tf.float64),0)),axis=1)\n",
    "            output_single,state_single=self.cell(inputs=next_input,state=state_single)\n",
    "\n",
    "            rewards += 1\n",
    "        self.global_epoch += 1\n",
    "        return predictions, LA_predictions, trajectory, rewards\n",
    "    \n",
    "def compute_loss(model, prediction_horizon):\n",
    "    preds, LA_predictions, trajectory, rewards = model(prediction_horizon)\n",
    "    loss = tf.Variable([0.0],dtype = tf.float64)\n",
    "    for pred,z in zip(LA_predictions,trajectory):\n",
    "        loss = tf.add(loss, tf.nn.l2_loss(pred-z))\n",
    "    return loss, LA_predictions, preds, trajectory, rewards\n",
    "\n",
    "def compute_gradient(model,prediction_horizon):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, LA_predictions, preds, trajectory, rewards = compute_loss(model,prediction_horizon)\n",
    "    return tape.watched_variables(), tape.gradient(loss,tape.watched_variables()), loss, preds, LA_predictions, trajectory, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, optimizer, view_rate, prediction_horizon = 5):\n",
    "    start = time.time()\n",
    "    for i in range(num_epochs):\n",
    "        watched_vars, grads, loss_, pred_, LA_pred_, z_, reward_ = compute_gradient(model, prediction_horizon)\n",
    "        losses.extend(loss_)\n",
    "        preds.append(pred_)\n",
    "        look_ahead_preds.append(LA_pred_)\n",
    "        trajectories.append(z_)\n",
    "        rewards.append(reward_)\n",
    "        optimizer.apply_gradients(zip(grads,watched_vars))\n",
    "        if (model.global_epoch+1)%view_rate == 0:\n",
    "            print('Epoch {}'.format(model.global_epoch+1))\n",
    "            print('Minutes elapsed: {}'.format((time.time()-start)/60))\n",
    "            print('Last {} averages: Loss: {}, reward: {}, loss/reward: {}'.format(view_rate,np.mean(losses[-view_rate:]), np.mean(rewards[-view_rate:]),\n",
    "                                                                                   (np.mean(losses[-view_rate:])/np.mean(rewards[-view_rate:]))))\n",
    "#             print('Model variables:')\n",
    "#             for var in watched_vars:\n",
    "#                 print(var.name)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = non_linear_Model(env_params_variation=[0,0,0,0],initial_state_variation=[1,0.1,0.1,0.1])\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "losses = []\n",
    "preds = []\n",
    "look_ahead_preds = []\n",
    "trajectories = []\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-42a019e7c815>:23: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-2-42a019e7c815>:25: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/alon/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1595: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 50\n",
      "Minutes elapsed: 0.41062499284744264\n",
      "Last 50 averages: Loss: 42.46695945795967, reward: 51.16326530612245, loss/reward: 0.830028326063033\n",
      "\n",
      "Epoch 100\n",
      "Minutes elapsed: 0.7960459113121032\n",
      "Last 50 averages: Loss: 42.070519273433966, reward: 46.3, loss/reward: 0.9086505242642325\n",
      "\n",
      "Epoch 150\n",
      "Minutes elapsed: 1.2421495079994203\n",
      "Last 50 averages: Loss: 41.85532167431816, reward: 50.34, loss/reward: 0.8314525561048502\n",
      "\n",
      "Epoch 200\n",
      "Minutes elapsed: 1.7365263422330222\n",
      "Last 50 averages: Loss: 40.48227470041146, reward: 52.6, loss/reward: 0.7696249943044003\n",
      "\n",
      "Epoch 250\n",
      "Minutes elapsed: 2.308019808928172\n",
      "Last 50 averages: Loss: 42.39376755142394, reward: 52.28, loss/reward: 0.8108983846867625\n",
      "\n",
      "Epoch 300\n",
      "Minutes elapsed: 2.875739276409149\n",
      "Last 50 averages: Loss: 40.5138057241016, reward: 50.76, loss/reward: 0.7981443208057841\n",
      "\n",
      "Epoch 350\n",
      "Minutes elapsed: 3.4782602310180666\n",
      "Last 50 averages: Loss: 41.48652379830144, reward: 46.96, loss/reward: 0.8834438628258399\n",
      "\n",
      "Epoch 400\n",
      "Minutes elapsed: 4.0928292314211525\n",
      "Last 50 averages: Loss: 41.33733127785909, reward: 51.58, loss/reward: 0.8014216998421694\n",
      "\n",
      "Epoch 450\n",
      "Minutes elapsed: 4.729356408119202\n",
      "Last 50 averages: Loss: 42.027216719604354, reward: 51.8, loss/reward: 0.8113362301081922\n",
      "\n",
      "Epoch 500\n",
      "Minutes elapsed: 5.3694604953130085\n",
      "Last 50 averages: Loss: 40.45371068701264, reward: 49.66, loss/reward: 0.814613586125909\n",
      "\n",
      "Epoch 550\n",
      "Minutes elapsed: 6.024559597174327\n",
      "Last 50 averages: Loss: 40.917070256202926, reward: 48.62, loss/reward: 0.8415687012793691\n",
      "\n",
      "Epoch 600\n",
      "Minutes elapsed: 6.78738124370575\n",
      "Last 50 averages: Loss: 42.27906073313292, reward: 53.88, loss/reward: 0.7846893231836103\n",
      "\n",
      "Epoch 650\n",
      "Minutes elapsed: 7.495409075419108\n",
      "Last 50 averages: Loss: 39.867319372837386, reward: 47.7, loss/reward: 0.8357928589693372\n",
      "\n",
      "Epoch 700\n",
      "Minutes elapsed: 8.296216523647308\n",
      "Last 50 averages: Loss: 40.58046947216094, reward: 52.66, loss/reward: 0.7706127890649629\n",
      "\n",
      "Epoch 750\n",
      "Minutes elapsed: 9.088072518507639\n",
      "Last 50 averages: Loss: 40.956105639456084, reward: 49.68, loss/reward: 0.8243982616637698\n",
      "\n",
      "Epoch 800\n",
      "Minutes elapsed: 9.94015515645345\n",
      "Last 50 averages: Loss: 40.37535984266356, reward: 51.16, loss/reward: 0.7891978077142996\n",
      "\n",
      "Epoch 850\n",
      "Minutes elapsed: 10.797391112645467\n",
      "Last 50 averages: Loss: 41.42753981911291, reward: 50.8, loss/reward: 0.8155027523447423\n",
      "\n",
      "Epoch 900\n",
      "Minutes elapsed: 11.582764740784963\n",
      "Last 50 averages: Loss: 40.287821281513516, reward: 48.62, loss/reward: 0.828626517513647\n",
      "\n",
      "Epoch 950\n",
      "Minutes elapsed: 12.452251994609833\n",
      "Last 50 averages: Loss: 41.063484902620004, reward: 51.86, loss/reward: 0.7918142094604705\n",
      "\n",
      "Epoch 1000\n",
      "Minutes elapsed: 13.312061250209808\n",
      "Last 50 averages: Loss: 43.48840837248771, reward: 47.78, loss/reward: 0.910180166858261\n",
      "\n",
      "Epoch 1050\n",
      "Minutes elapsed: 14.235857717196147\n",
      "Last 50 averages: Loss: 40.77837484413092, reward: 50.78, loss/reward: 0.8030400717631139\n",
      "\n",
      "Epoch 1100\n",
      "Minutes elapsed: 15.132114235560099\n",
      "Last 50 averages: Loss: 40.55981075376434, reward: 51.24, loss/reward: 0.7915653933209277\n",
      "\n",
      "Epoch 1150\n",
      "Minutes elapsed: 16.08103551864624\n",
      "Last 50 averages: Loss: 41.46221617479614, reward: 51.62, loss/reward: 0.8032199956372751\n",
      "\n",
      "Epoch 1200\n",
      "Minutes elapsed: 17.075281723340353\n",
      "Last 50 averages: Loss: 42.035491672633945, reward: 52.66, loss/reward: 0.7982432904032273\n",
      "\n",
      "Epoch 1250\n",
      "Minutes elapsed: 18.080929414431253\n",
      "Last 50 averages: Loss: 43.01425382424864, reward: 51.44, loss/reward: 0.8362024460390483\n",
      "\n",
      "Epoch 1300\n",
      "Minutes elapsed: 19.0725949883461\n",
      "Last 50 averages: Loss: 41.07119518498646, reward: 49.4, loss/reward: 0.8314007122466894\n",
      "\n",
      "Epoch 1350\n",
      "Minutes elapsed: 20.125495580832162\n",
      "Last 50 averages: Loss: 41.94190178632025, reward: 50.18, loss/reward: 0.8358290511422928\n",
      "\n",
      "Epoch 1400\n",
      "Minutes elapsed: 21.238819952805837\n",
      "Last 50 averages: Loss: 41.056920113094456, reward: 53.56, loss/reward: 0.7665593747777157\n",
      "\n",
      "Epoch 1450\n",
      "Minutes elapsed: 22.366975339253745\n",
      "Last 50 averages: Loss: 41.64464407120718, reward: 50.78, loss/reward: 0.8200993318473253\n",
      "\n",
      "Epoch 1500\n",
      "Minutes elapsed: 23.628330492973326\n",
      "Last 50 averages: Loss: 42.78732132307439, reward: 48.86, loss/reward: 0.8757126754620218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(test_model, 2000, optimizer,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(losses)\n",
    "plt.title('losses')\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(rewards)\n",
    "plt.title('rewards')\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot([loss/reward for loss,reward in zip(losses,rewards)])\n",
    "plt.title('loss per reward')\n",
    "# plt.ylim(0,500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "truelabels = ['true x','true x dot', 'true theta','true theta dot']\n",
    "predictedlabels = ['predicted x', 'predicted x dot', 'predicted theta', 'predicted theta dot']\n",
    "lookaheadlabels = ['LA x', 'LA x dot', 'LA theta', 'LA theta dot']\n",
    "for i in range(0,5000,100):\n",
    "    plt.figure(figsize=(20,6))\n",
    "    for j in range(4):\n",
    "        plt.plot(np.squeeze(np.array(trajectories[i])).T[j], label = truelabels[j], color = 'k')\n",
    "        plt.plot(np.squeeze(np.array(preds[i])).T[j], label = predictedlabels[j], color = 'y')\n",
    "        plt.plot(np.squeeze(np.array(look_ahead_preds[i])).T[j],label=lookaheadlabels[j],color='r')\n",
    "    plt.legend()\n",
    "    plt.title(i)\n",
    "    plt.xlim(0,200)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
