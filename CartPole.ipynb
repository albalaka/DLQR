{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import control as ct\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter(object):\n",
    "    \"\"\"\n",
    "    This class defines a kalman filter\n",
    "\n",
    "    l - latent state\n",
    "    l_a_priori - A priori state estimate\n",
    "    l_a_posteriori - A posteriori state estimate\n",
    "\n",
    "    P_a_priori - A priori error covariance\n",
    "    P_a_posteriori - A posteriori error covariance\n",
    "\n",
    "    C - observation model\n",
    "    Q - covariance of the process noise    \n",
    "    a, b - observation model and bias\n",
    "    R - covariance of the observation noise\n",
    "    z - observation\n",
    "\n",
    "    y_pre - measurement pre-fit residual\n",
    "    S - Pre-fit residual covariance\n",
    "    K - Kalman gain\n",
    "    y_post - measurement post-fit residual\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, dim_z, batch_size, **kwargs):\n",
    "        self.m = m\n",
    "        self.dim_z = dim_z\n",
    "        self.dim_y = dim_z\n",
    "\n",
    "        # lambda initializer for identity matrices\n",
    "        self.eye_init = lambda shape, dtype = np.float32: np.eye(*shape, dtype = dtype)\n",
    "\n",
    "        self._I = tf.constant(self.eye_init((m, m)), name= 'I')\n",
    "\n",
    "        '''This section also cannot handle missing kwargs'''\n",
    "        self.l_0 = kwargs.pop('l_0', None)\n",
    "        self.P_0 = kwargs.pop('P_0', None)\n",
    "        self.A = kwargs.pop('A', None)\n",
    "        self.B = kwargs.pop('B', None)\n",
    "        self.u = kwargs.pop('u', None)\n",
    "        self.C = kwargs.pop('C', None)\n",
    "        self.g = kwargs.pop('g', None)\n",
    "        self.sigma = kwargs.pop('sigma', None)\n",
    "        self.y_0 = kwargs.pop('y_0', None)\n",
    "        self.z_0 = kwargs.pop('z_0', None)\n",
    "        self.pred_0 = kwargs.pop('pred_0', None)\n",
    "        self.z = kwargs.pop('z', None)\n",
    "        \n",
    "#         self.g_pred = kwargs.pop('g_pred', None)\n",
    "#         self.sigma_pred = kwargs.pop('sigma_pred', None)\n",
    "#         self.l_0_pred = kwargs.pop('l_0_pred', None)\n",
    "#         self.z_0_pred = kwargs.pop('z_0_pred', None)\n",
    "#         self.A_pred = kwargs.pop('A_pred', None)\n",
    "#         self.B_pred = kwargs.pop('B_pred', None)\n",
    "#         self.u_pred = kwargs.pop('u_pred', None)\n",
    "#         self.C_pred = kwargs.pop('C_pred', None)\n",
    "\n",
    "\n",
    "    def forward_filter_fn(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Forward step over a batch\n",
    "        params contains: l_a_posteriori, P_a_posteriori, y_pre\n",
    "        inputs contains: z, F, g, sigma, a, b\n",
    "\n",
    "        Calculates prior distributions based on the given posterior distributions and the current residual\n",
    "                updates posterior distributions based on the new prior distributions\n",
    "        \"\"\"\n",
    "        '''Shapes:\n",
    "            z = (bs, dim_z)\n",
    "            l_a_posteriori = (bs, m, dim_z)\n",
    "            P_a_posteriori = (bs, m, m)\n",
    "            F = (bs, m, m)\n",
    "            Q = (bs, m, m)\n",
    "            R = (bs, dim_z, dim_z)\n",
    "            a = (bs, m, dim_z)\n",
    "            b = (bs, dim_z)\n",
    "        '''\n",
    "        z, A, B, u, g, sigma, C = inputs\n",
    "        l_a_posteriori, P_a_posteriori, y_pre, pred = params\n",
    "\n",
    "#         print('z',z.shape)\n",
    "#         print('A', A.shape)\n",
    "#         print('B',B.shape)\n",
    "#         print('u',u.shape)\n",
    "#         print('g',g.shape)\n",
    "#         print('sigma',sigma.shape)\n",
    "#         print('C', C.shape)\n",
    "#         print('l_a_posteriori', l_a_posteriori.shape)\n",
    "#         print('P_a_posteriori', P_a_posteriori.shape)\n",
    "#         print('y_pre', y_pre.shape)\n",
    "#         print('pred', pred.shape)\n",
    "        \n",
    "        \n",
    "        l_a_priori = tf.matmul(A,l_a_posteriori) + tf.matmul(B,u)\n",
    "#         print('l_a_priori',l_a_priori.shape)\n",
    "        P_a_priori = tf.matmul(tf.matmul(A,P_a_posteriori), A, transpose_b = True) + tf.matmul(g,g, transpose_b=True)\n",
    "#         print('P_a_priori', P_a_priori.shape)\n",
    "\n",
    "        y_pre = tf.expand_dims(z,-1) - tf.matmul(C,l_a_priori)\n",
    "\n",
    "        S = tf.matmul(sigma, sigma, transpose_b=True) + \\\n",
    "            tf.matmul(tf.matmul(C, P_a_priori), C, transpose_b=True)\n",
    "       \n",
    "        '''TODO: Compute inverse using cholesky decomposition? Only works if a is matrix\n",
    "                so z must be multivariate\n",
    "        '''\n",
    "#         S_inv = tf.linalg.inv(S)\n",
    "        S_inv = tfp.math.pinv(S)\n",
    "        \n",
    "        \n",
    "        K = tf.matmul(tf.matmul(P_a_priori, C, transpose_b=True), S_inv)\n",
    "        l_a_posteriori = l_a_priori + tf.matmul(K,y_pre)\n",
    "        I_KC = self._I-tf.matmul(K,C)\n",
    "        P_a_posteriori = tf.matmul(tf.matmul(I_KC, P_a_priori), I_KC, transpose_b=True) + \\\n",
    "                         tf.matmul(tf.matmul(K,tf.matmul(sigma, sigma, transpose_b = True)),\n",
    "                                   K, transpose_b=True)\n",
    "        y_post = z - tf.squeeze(tf.matmul(C,l_a_posteriori),-1)\n",
    "        pred = tf.squeeze(tf.matmul(C, l_a_posteriori),-1)\n",
    "        \n",
    "#         print('l_a_posteriori', l_a_posteriori.shape)\n",
    "#         print('P_a_posteriori', P_a_posteriori.shape)\n",
    "#         print('y_post', y_post.shape)\n",
    "#         print('pred', pred.shape)\n",
    "        return l_a_posteriori, P_a_posteriori, y_post, pred\n",
    "\n",
    "    def forward_filter(self):\n",
    "        \"\"\"\n",
    "        Compute the forward step in Kalman Filter\n",
    "        The forward pass is initialized with p(x_1) = N(self.x, self.P)\n",
    "        We return the mean and covariance for p(x_t|x_tm1) for t=2, ..., T+1\n",
    "        and the filtering distribution p(x_t|z_1:t) for t=1, ..., T\n",
    "        \"\"\"\n",
    "#         print('z',self.z.shape)\n",
    "#         print('A', self.A.shape)\n",
    "#         print('B',self.B.shape)\n",
    "#         print('u',self.u.shape)\n",
    "#         print(type(self.u[0,0]))\n",
    "#         print('g',self.g.shape)\n",
    "#         print('sigma', self.sigma.shape)\n",
    "#         print('C', self.C.shape)\n",
    "#         print('l_0', self.l_0.shape)\n",
    "#         print('P_0', self.P_0.shape)\n",
    "#         print('y_0', self.y_0.shape)\n",
    "#         print('pred_0', self.pred_0.shape)\n",
    "        forward_states = tf.scan(self.forward_filter_fn,\n",
    "                                 elems = (trans(self.z),trans(self.A),\n",
    "                                          trans(self.B),trans(self.u),\n",
    "                                          trans(self.g),trans(self.sigma),\n",
    "                                          trans(self.C)),\n",
    "                                initializer=(self.l_0, self.P_0, self.y_0, self.pred_0))\n",
    "        \n",
    "        return forward_states\n",
    "    \n",
    "    def Kfilter(self):\n",
    "        l_filtered, P_filtered, residuals, filtered_prediction = self.forward_filter()\n",
    "        return trans(l_filtered), trans(P_filtered), trans(residuals), trans(filtered_prediction)\n",
    "        \n",
    "    def forward_predict_fn(self, params, inputs):\n",
    "        \"\"\"Forward step over a batch\n",
    "        params contains l_prev, z_prev\n",
    "        inputs contains F, g, a, b, sigma\"\"\"\n",
    "        \n",
    "        A, B, g, C, sigma = inputs\n",
    "        l_prev, z_prev = params\n",
    "        \n",
    "#         l_next = tfd.MultivariateNormalDiag(loc = tf.matmul(F, l_prev), scale_diag = g).sample()\n",
    "        l_next = tf.matmul(A, l_prev) + tf.matmul(B, u)\n",
    "#         z_next = tfd.Normal(loc = tf.matmul(a, l_prev, transpose_a=True)+b, scale = sigma).sample()\n",
    "        z_next = tf.matmul(C, l_prev)\n",
    "        return l_next, z_next\n",
    "    \n",
    "    def forward_predict(self):\n",
    "        \"\"\"\n",
    "        Compute the predictions in state space model\n",
    "        The forward pass is initialized by l_T = p(l_T|z_1:T)\n",
    "        We return the hidden states l_T+1:T+t and predictions z_T+1:T+t\n",
    "        \"\"\"\n",
    "        \n",
    "        forward_predictions = tf.scan(self.forward_predict_fn,\n",
    "                                      elems = (trans(self.A_pred), trans(self.B_pred),\n",
    "                                               trans(self.g_pred), trans(self.C_pred),\n",
    "                                               trans(self.sigma_pred)),\n",
    "                                      initializer = (self.l_0_pred, self.z_0_pred))\n",
    "        \n",
    "        return forward_predictions\n",
    "        \n",
    "    def Kpredict(self):\n",
    "        \n",
    "        l_predicted, z_predicted = self.forward_predict()\n",
    "        return trans(l_predicted), trans(z_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(tensor):\n",
    "    if len(tensor.shape)==3:\n",
    "        return tf.transpose(tensor, [1,0,2])\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1,0,2,3])\n",
    "    \n",
    "def arrayify(l):\n",
    "    temp = [np.asarray(x, dtype = np.float32) for x in l]\n",
    "    max_trial_len = np.max([trial.shape[0] for trial in temp])\n",
    "    trial_lens = []\n",
    "    new = []\n",
    "    for trial in temp:\n",
    "        if len(trial.shape) == 2:\n",
    "            new.append(np.pad(trial,((0,200-trial.shape[0]),(0,0)),'constant', constant_values=(0)))\n",
    "            trial_lens.append(trial.shape[0])\n",
    "        else:\n",
    "            new.append(np.pad(trial,(0,200-trial.shape[0]),'constant',constant_values=0))\n",
    "            trial_lens.append(trial.shape[0])\n",
    "    new = np.stack(new,axis = 0)\n",
    "    if len(new.shape) >2:\n",
    "        mask = np.zeros(new.shape[:-1], dtype = np.float32)\n",
    "    else:\n",
    "        mask = np.zeros(new.shape, dtype = np.float32)\n",
    "    for idx,length in enumerate(trial_lens):\n",
    "        mask[idx,:length+1] += 1\n",
    "    return new, mask.astype(np.bool)\n",
    "\n",
    "def forward_filter_fn(A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z):\n",
    "    '''Calculates prior distributions based on the given posterior distributions\n",
    "        and the current residual updates posterior distributions based on the new\n",
    "        prior distributions\n",
    "    '''\n",
    "#     print('z',z.shape)\n",
    "#     print('A', A.shape)\n",
    "#     print('B',B.shape)\n",
    "#     print('u',u.shape)\n",
    "#     print('g',g.shape)\n",
    "#     print('sigma',sigma.shape)\n",
    "#     print('C', C.shape)\n",
    "#     print('l_a_posteriori', l_a_posteriori.shape)\n",
    "#     print('P_a_posteriori', P_a_posteriori.shape)\n",
    "    _I = tf.eye(int(A.shape[0]), dtype = tf.float32)\n",
    "    \n",
    "    z = tf.expand_dims(z,-1)\n",
    "    l_a_priori = tf.matmul(A,l_a_posteriori) + tf.matmul(B,u)\n",
    "#     print('l_a_priori',l_a_priori.shape)\n",
    "    P_a_priori = tf.matmul(tf.matmul(A,P_a_posteriori), A, transpose_b = True) + tf.matmul(g,g, transpose_b=True)\n",
    "#     print('P_a_priori',P_a_priori.shape)\n",
    "    y_pre = z - tf.matmul(C,l_a_priori)\n",
    "#     print('y_pre', y_pre.shape)\n",
    "    S = tf.matmul(sigma, sigma, transpose_b=True) + \\\n",
    "        tf.matmul(tf.matmul(C, P_a_priori), C, transpose_b=True)\n",
    "#     print('S', S.shape)\n",
    "    \n",
    "    S_inv = tf.linalg.inv(S)\n",
    "    K = tf.matmul(tf.matmul(P_a_priori, C, transpose_b=True), S_inv)\n",
    "#     print('K', K.shape)\n",
    "    l_a_posteriori = l_a_priori + tf.matmul(K,y_pre)\n",
    "#     print('l_a_posteriori', l_a_posteriori.shape)\n",
    "    I_KC = _I-tf.matmul(K,C)\n",
    "#     print('I-KC', I_KC.shape)\n",
    "    P_a_posteriori = tf.matmul(tf.matmul(I_KC, P_a_priori), I_KC, transpose_b=True) + \\\n",
    "                        tf.matmul(tf.matmul(K,tf.matmul(sigma, sigma, transpose_b = True)),\n",
    "                                K, transpose_b=True)\n",
    "#     print('P_a_posteriori',P_a_posteriori.shape)\n",
    "    y_post = z-tf.matmul(C,l_a_posteriori)\n",
    "#     print('y_post', y_post.shape)\n",
    "    pred = tf.matmul(C, l_a_posteriori)\n",
    "#     print('pred', pred.shape)\n",
    "        \n",
    "    return A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_SSM_model(object):\n",
    "    def __init__(self, sess, name, m = 4, n=4, r=1,\n",
    "                 learning_rate = 0.00001, lr_decay = 0.95, sigma_upper_bound = 1,\n",
    "                 sigma_lower_bound = 0, g_upper_bound = 1,\n",
    "                 g_lower_bound = 0.1, mu_0_upper_bound = 1,mu_0_lower_bound = 0,\n",
    "                 Sigma_0_upper_bound = 1, Sigma_0_lower_bound = 0, beta = 0.00001,\n",
    "                 b_upper_bound = 0.25, b_lower_bound = -0.25,thetaacc_error=0,initial_state=0.0001\n",
    "                ):\n",
    "        \n",
    "        '''thetaacc_error gives the amount of random angular acceleration that can be put on the pendulum,\n",
    "        initial_state gives the amount of variation in the initial state\n",
    "        '''\n",
    "        if name == '':\n",
    "            raise NameError(\"A model has no name\")\n",
    "\n",
    "        '''This functions assumes the state space model:\n",
    "            l_(t+1) = A_(t)l_(t)+B(t)u_(t)\n",
    "            z_(t+1) = C_(t)l_(t)\n",
    "            where:\n",
    "            l has dim m\n",
    "            u has dim r\n",
    "            z has dim m\n",
    "            A has dim mxn\n",
    "            B has dim mxr\n",
    "            C has dim mxm\n",
    "            '''\n",
    "            \n",
    "        self.sess = sess\n",
    "        \n",
    "        '''nn model hyperparameters'''\n",
    "        self.learning_rate = learning_rate\n",
    "#         self.sample_len = tf.Variable(200, name = 'sample_len', trainable=False)\n",
    "        self.global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "        self.increment_global_step = tf.assign_add(self.global_step,1, name = 'increment_global_step')\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lstm_sizes = [128,64]\n",
    "        last_lstm = self.lstm_sizes[-1]\n",
    "\n",
    "        '''dims'''\n",
    "        self.m = m # size of the state space\n",
    "        self.dim_z = m\n",
    "        self.n = n\n",
    "        self.r = r\n",
    "        self.lstm_input_dim = m+4 # previous states plus physical parameters\n",
    "        \n",
    "        \n",
    "        self.initial_variance = 1\n",
    "        \n",
    "        '''error bounds'''\n",
    "        self.sigma_upper_bound = sigma_upper_bound\n",
    "        self.g_lower_bound = g_lower_bound\n",
    "        self.g_upper_bound = g_upper_bound\n",
    "        self.mu_0_upper_bound = mu_0_upper_bound\n",
    "        self.mu_0_lower_bound = mu_0_lower_bound\n",
    "        self.Sigma_0_upper_bound = Sigma_0_upper_bound\n",
    "        self.Sigma_0_lower_bound= Sigma_0_lower_bound\n",
    "        self.b_upper_bound = b_upper_bound\n",
    "        self.b_lower_bound = b_lower_bound\n",
    "        self.beta = beta\n",
    "        \n",
    "        '''LQR parameters'''\n",
    "        self.env = gym.make('Custom_CartPole-v0', thetaacc_error=thetaacc_error, initial_state=initial_state)\n",
    "        self.gravity = self.env.gravity\n",
    "        self.cart_mass = self.env.masscart\n",
    "        self.pole_mass = self.env.masspole\n",
    "        self.pole_length = self.env.length\n",
    "        self.env_params = tf.expand_dims(np.array([self.gravity, self.cart_mass,\n",
    "                                                   self.pole_mass,self.pole_length], dtype=np.float32),0)\n",
    "        self.Q = np.eye(4)*[1,1,1,1]\n",
    "        self.R = 1\n",
    "        \n",
    "        '''Saving model stuff, don\"t need for now'''\n",
    "#         self.model_folder = 'quantitative_tmp/{}'.format(name)\n",
    "#         if not os.path.isdir(self.model_folder):\n",
    "#             print('This model has no folder')\n",
    "#             os.makedirs(self.model_folder)\n",
    "#         self.saved_model_location = '{}/model.ckpt'.format(self.model_folder)\n",
    "\n",
    "        self.losses = []\n",
    "        self.saver = None\n",
    "        \n",
    "        with tf.variable_scope('KF', reuse = tf.AUTO_REUSE):\n",
    "            self.C = tf.get_variable(initializer = tf.eye(self.dim_z),dtype = tf.float32,\n",
    "                                     name = 'C', trainable = False)\n",
    "            self.W_A = tf.get_variable(initializer = tf.random.normal([last_lstm, m*n]),\n",
    "                                       dtype = tf.float32, name = 'W_A')\n",
    "            self.bias_A = tf.get_variable(initializer = tf.zeros([1, m*n]),\n",
    "                                          dtype = tf.float32, name = 'bias_A')\n",
    "            \n",
    "            self.W_B = tf.get_variable(initializer = tf.random.normal([last_lstm, m*r]),\n",
    "                                       dtype = tf.float32, name = 'W_B')\n",
    "            self.bias_B = tf.get_variable(initializer = tf.zeros([1, m*r]),\n",
    "                                         dtype = tf.float32, name = 'bias_B')\n",
    "\n",
    "            self.W_g = tf.get_variable(initializer = tf.random.normal([last_lstm, m]),\n",
    "                                       dtype = tf.float32, name = 'W_g')\n",
    "            self.bias_g = tf.get_variable(initializer = tf.zeros([1, m]),\n",
    "                                          dtype = tf.float32, name = 'bias_g')\n",
    "\n",
    "            self.W_sigma = tf.get_variable(initializer = tf.random.normal([last_lstm, self.dim_z]),\n",
    "                                           dtype = tf.float32, name = 'W_sigma')\n",
    "            self.bias_sigma = tf.get_variable(initializer = tf.zeros([1, self.dim_z]),\n",
    "                                              dtype = tf.float32, name = 'bias_sigma')\n",
    "\n",
    "            self.W_mu_0 = tf.get_variable(initializer = tf.random.normal([last_lstm, self.m]),\n",
    "                                          dtype = tf.float32, name = 'W_mu_0')\n",
    "            self.bias_mu_0 = tf.get_variable(initializer = tf.zeros([1, self.m]),\n",
    "                                             dtype = tf.float32, name = 'bias_mu_0')\n",
    "            \n",
    "            self.W_Sigma_0 = tf.get_variable(initializer = tf.random.normal([last_lstm, self.m]),\n",
    "                                             dtype = tf.float32, name = 'W_Sigma_0')\n",
    "            self.bias_Sigma_0 = tf.get_variable(initializer = tf.zeros([1, self.m]),\n",
    "                                                dtype = tf.float32, name = 'bias_Sigma_0')\n",
    "            \n",
    "            self.P_0 = tf.Variable(self.initial_variance*tf.eye(self.m,dtype = tf.float32),\n",
    "                                   name = 'P_0', trainable = False)\n",
    "\n",
    "            self.y_0 = tf.Variable(tf.zeros([self.dim_z]), dtype = tf.float32, name = 'y_0', trainable = False)\n",
    "            self.z_0 = tf.Variable(tf.zeros([self.dim_z, self.dim_z]), dtype = tf.float32, name = 'z_0', trainable = False)\n",
    "            self.pred_0 = tf.Variable(tf.zeros([self.dim_z]), dtype = tf.float32, name = 'pred_0', trainable = False)\n",
    "            \n",
    "            \n",
    "            '''Variables for test range in LQE only'''\n",
    "#             self.A_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, m, n], name = 'A_test')\n",
    "#             self.B_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, m, r], name = 'B_test')\n",
    "#             self.g_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, self.m, 1], name = 'g_test')\n",
    "#             self.sigma_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, self.dim_z, 1], name = 'sigma_test')\n",
    "#             self.l_0_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.m, 1], name = 'l_0_test')\n",
    "#             self.final_z = tf.placeholder(tf.float32, shape = [self.batch_size, 1, self.dim_z], name = 'final_z')\n",
    "            \n",
    "#         with tf.variable_scope('LSTM', reuse = tf.AUTO_REUSE):\n",
    "#             self.lstm_input = tf.placeholder(tf.float32, shape=\n",
    "#                                              [self.batch_size, self.sample_len,self.lstm_input_dim],\n",
    "#                                              name = 'lstm_input')\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, shape = [self.dim_z],name = 'z')\n",
    "        \n",
    "        with tf.variable_scope('LSTM', reuse = tf.AUTO_REUSE):\n",
    "            self.lstm_input = tf.placeholder(tf.float32, shape = [1, self.lstm_input_dim],\n",
    "                                            name = 'lstm_input')\n",
    "\n",
    "    def affine_transformations(self):\n",
    "\n",
    "        with tf.variable_scope('affine_transformations'):\n",
    "            \n",
    "            self.lstm_output = tf.expand_dims(self.lstm_output, -1) # (bs, sample_len, last lstm_size, 1)\n",
    "            def tile_func(Weights_or_bias):\n",
    "                if len(Weights_or_bias.shape) == 2:\n",
    "                    return tf.tile(tf.expand_dims(Weights_or_bias,1), (1,self.sample_len,1))\n",
    "                elif len(Weights_or_bias.shape) == 3:\n",
    "                    return tf.tile(tf.expand_dims(Weights_or_bias,1), (1,self.sample_len,1,1))\n",
    "                else:\n",
    "                    raise ValueError('Unknown sized Weights or bias array')\n",
    "                    \n",
    "            '''TODO: Does tile work by updating all weights for the tile back to the original matrix?    \n",
    "            '''\n",
    "            \n",
    "            W_A = tile_func(self.W_A)\n",
    "            bias_A = tile_func(self.bias_A)\n",
    "            W_B = tile_func(self.W_B)\n",
    "            bias_B = tile_func(self.bias_B)\n",
    "            W_g = tile_func(self.W_g)\n",
    "            bias_g = tile_func(self.bias_g)\n",
    "            W_sigma = tile_func(self.W_sigma)\n",
    "            bias_sigma = tile_func(self.bias_sigma)\n",
    "\n",
    "            self.A = tf.reshape(tf.add(tf.matmul(W_A, self.lstm_output), bias_A),\n",
    "                                [self.batch_size,self.sample_len,self.m,self.n])\n",
    "            self.B = tf.reshape(tf.add(tf.matmul(W_B, self.lstm_output), bias_B),\n",
    "                                [self.batch_size,self.sample_len,self.m,self.r])\n",
    "\n",
    "            transition_error = tf.add(tf.matmul(W_g, self.lstm_output), bias_g)\n",
    "            self.g = ((self.g_upper_bound-self.g_lower_bound)/(1+tf.exp(-transition_error)))+self.g_lower_bound\n",
    "            \n",
    "            observation_error = tf.add(tf.matmul(W_sigma, self.lstm_output), bias_sigma)\n",
    "            self.sigma = (self.sigma_upper_bound)/(1+tf.exp(-observation_error))\n",
    "            \n",
    "            temp_mu_0 = tf.add(tf.matmul(self.W_mu_0, self.lstm_output[:,0,:]), self.bias_mu_0)\n",
    "            self.mu_0 = ((self.mu_0_upper_bound-self.mu_0_lower_bound)/(1+tf.exp(-temp_mu_0)))+self.mu_0_lower_bound\n",
    "\n",
    "            temp_Sigma_0 = tf.add(tf.matmul(self.W_Sigma_0, self.lstm_output[:,0,:]), self.bias_Sigma_0)\n",
    "            self.Sigma_0 = ((self.Sigma_0_upper_bound-self.Sigma_0_lower_bound)/(1+tf.exp(-temp_Sigma_0)))+self.Sigma_0_lower_bound\n",
    "\n",
    "            l_0_distribution = tfd.MultivariateNormalDiag(loc = self.mu_0, scale_diag = self.Sigma_0)\n",
    "\n",
    "            self.l_0 = l_0_distribution.sample()\n",
    "        return self\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.kf_train = KalmanFilter(batch_size=self.batch_size,\n",
    "                                     m=self.m,\n",
    "                                     dim_z=self.dim_z,\n",
    "                                     l_0 = self.l_0,\n",
    "                                     P_0 = self.P_0,\n",
    "                                     A = self.A,\n",
    "                                     B = self.B,\n",
    "                                     u = self.u,\n",
    "                                     C = self.C,\n",
    "                                     g = self.g,\n",
    "                                     sigma = self.sigma,\n",
    "                                     z = self.z,\n",
    "                                     y_0 = self.y_0,\n",
    "                                     pred_0 = self.pred_0\n",
    "                                    )\n",
    "        with tf.variable_scope('KF_results', reuse=tf.AUTO_REUSE):\n",
    "            self.l_filtered, self.P_filtered, self.residuals, self.filtered_predictions = self.kf_train.Kfilter()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def likelihood_fn(self, params, inputs):\n",
    "        '''Compute likelihood over a batch\n",
    "        params contains: mu, Sigma - the parameters of the likelihood distribution\n",
    "        inputs contains: calculations of mu: F, a, l_filtered==l_a_posteriori\n",
    "                        calculations of Sigma: a, F, P_a_posteriori, g, sigma\n",
    "        '''\n",
    "        A, B, u, C, g, sigma, l_filtered, P_filtered = inputs\n",
    "        mu, Sigma = params\n",
    "        '''\n",
    "        a (bs, m, 1)\n",
    "        b (bs, 1)\n",
    "        F (bs, m, m)\n",
    "        g (bs, m, 1)\n",
    "        sigma (bs, 1, 1)\n",
    "        f (bs, m, 1)\n",
    "        S (bs, m, m)\n",
    "        mu (bs, 1, 1)\n",
    "        Sigma (bs, 1, 1)\n",
    "        '''\n",
    "\n",
    "        mu = tf.matmul(C, tf.add(tf.matmul(A,l_filtered), tf.matmul(B,u)))\n",
    "#         mu = tf.add(tf.matmul(tf.matmul(a, F, transpose_a=True), f), b)\n",
    "\n",
    "        temp = tf.matmul(tf.matmul(A, P_filtered), A, transpose_b=True) + tf.matmul(g, g, transpose_b=True)\n",
    "        Sigma = tf.matmul(tf.matmul(C, temp), C, transpose_b=True) + tf.square(sigma)\n",
    "\n",
    "#         temp = tf.matmul(tf.matmul(F, S), F, transpose_b=True) + tf.matmul(g, g, transpose_b=True)\n",
    "#         Sigma = tf.matmul(tf.matmul(a, temp, transpose_a=True), a) + tf.square(sigma)\n",
    "        \n",
    "        return mu, Sigma\n",
    "    \n",
    "    def build_loss(self):\n",
    "        '''Useful shapes(Ideally):\n",
    "            l_a_posteriori(batch) - (batch_size, sample_len, m)\n",
    "            P_a_posteriori(batch) - (batch_size, sample_len, m,m)\n",
    "            \n",
    "            inputs:\n",
    "                mu_0, a, F, l_a_posteriori?\n",
    "                Sigma_0, a, R, F, P_a_posteriori, Q\n",
    "        '''\n",
    "\n",
    "\n",
    "        with tf.variable_scope('loss', reuse = tf.AUTO_REUSE):\n",
    "#             decayed_learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "#                                                        self.num_batches, self.lr_decay)\n",
    "#             print('self.C',self.C.shape)\n",
    "#             print('self.mu_0', self.mu_0.shape)\n",
    "#             print('self.B',self.B.shape)\n",
    "#             print('self.u',self.u.shape)\n",
    "            \n",
    "\n",
    "            mu_1 = tf.matmul(trans(self.C)[0], self.mu_0)+tf.matmul(trans(self.B)[0],trans(self.u)[0])\n",
    "            Sigma_1 = tf.matmul(tf.matmul(trans(self.C)[0], tf.linalg.diag(tf.squeeze(self.Sigma_0,-1))),\n",
    "                                         trans(self.C)[0], transpose_b=True)+tf.square(trans(self.sigma)[0])\n",
    "#             mu_1 = tf.add(tf.matmul(trans(self.a)[0], self.mu_0, transpose_a=True),trans(self.b)[0])\n",
    "#             Sigma_1 = tf.matmul(tf.matmul(trans(self.a)[0], tf.linalg.diag(tf.squeeze(self.Sigma_0)),\n",
    "#                                           transpose_a=True),\n",
    "#                                 trans(self.a)[0]) + tf.square(trans(self.sigma)[0])\n",
    "    \n",
    "            mu, Sigma = tf.scan(self.likelihood_fn,\n",
    "                                elems = (trans(self.A)[1:], trans(self.B)[1:],\n",
    "                                         trans(self.u)[1:], trans(self.C)[1:], trans(self.g)[1:],\n",
    "                                         trans(self.sigma)[1:],trans(self.l_filtered)[:-1],\n",
    "                                         trans(self.P_filtered)[1:]),\n",
    "                                initializer = (mu_1, Sigma_1))\n",
    "\n",
    "\n",
    "            self.mu = tf.squeeze(tf.concat([tf.expand_dims(mu_1,1), trans(mu)], 1),-1)\n",
    "            self.Sigma = tf.concat([tf.expand_dims(Sigma_1,1),trans(Sigma)], 1)\n",
    "            \n",
    "            z_distribution = tfd.MultivariateNormalDiag(loc = self.mu,\n",
    "                                                        scale_diag = tf.linalg.diag_part(self.Sigma))\n",
    "\n",
    "            z_probability = z_distribution.prob(self.z)\n",
    "            self.z_probability = tf.boolean_mask(z_probability, self.z_mask)\n",
    "            regularizers = tf.nn.l2_loss(self.W_g) + tf.nn.l2_loss(self.W_mu_0) + \\\n",
    "                        tf.nn.l2_loss(self.W_sigma) + tf.nn.l2_loss(self.W_Sigma_0) + \\\n",
    "                        tf.nn.l2_loss(self.W_A) + tf.nn.l2_loss(self.W_B)\n",
    "\n",
    "            self.loss = tf.reduce_mean(self.beta*regularizers)-tf.square(tf.reciprocal(tf.reduce_sum(tf.cast(self.z_mask, tf.float32))))*tf.reduce_sum(tf.log(self.z_probability+1e-8))\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "#             self.optimizer = tf.train.AdamOptimizer(decayed_learning_rate)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            capped_grads_and_vars = [(tf.clip_by_norm(grad, 1.), var) for grad, var in grads_and_vars]\n",
    "            self.train_op = self.optimizer.apply_gradients(capped_grads_and_vars)\n",
    "        return self\n",
    "    \n",
    "    def initialize_variables(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        try:\n",
    "            self.saver.restore(self.sess, tf.train.latest_checkpoint(self.model_folder))\n",
    "            print(\"Restoring model from {}\".format(self.saved_model_location))\n",
    "        except:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            print(\"Initializing new model at {}\".format(self.saved_model_location))\n",
    "        return self  \n",
    "    \n",
    "    def train(self, epochs):\n",
    "        start = time.time()\n",
    "        rewards = []\n",
    "#        merged = tf.summary.merge_all()\n",
    "#        writer = tf.summary.FileWriter(self.log_dir, self.sess.graph)\n",
    "        for i in range(epochs):\n",
    "            preds = []\n",
    "            epoch_loss = []\n",
    "            self.sample_from_env()\n",
    "            \n",
    "            feed_dict = {self.lstm_input: self.lstm_inputs}\n",
    "#                summary, loss_, _, _ = self.sess.run([merged, self.loss, self.train_op, self.increment_global_step],\n",
    "#                                                  feed_dict=feed_dict)\n",
    "            loss_, _, _ = self.sess.run([self.loss, self.train_op, self.increment_global_step],\n",
    "                                        feed_dict=feed_dict)\n",
    "            epoch_loss.append(loss_)\n",
    "#                writer.add_summary(summary, self.sess.run(self.global_step))\n",
    "#                if idx % 10 == 0:\n",
    "#                  print('mini epoch #{}'.format(idx))\n",
    "\n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "            self.losses.append(epoch_loss)\n",
    "            rewards.append(np.mean(self.rewards))\n",
    "            if i%50 == 0:\n",
    "                print(\"Epoch #{}\\tTime Elapsed: {}\\tNegative Log-Likelihood {}\\n\\t\\tMean Reward: {}\".\n",
    "                      format(self.sess.run(self.global_step)-1,(time.time()-start)/60,\n",
    "                             epoch_loss, np.mean(self.rewards)))\n",
    "#             if i%50 == 0:\n",
    "#                 self.saver.save(self.sess, self.saved_model_location, global_step = self.global_step)\n",
    "#                 print(\"Model Saved at {}\".format(self.saved_model_location))\n",
    "#         self.saver.save(self.sess, self.saved_model_location, global_step = self.global_step)\n",
    "#         print(\"Model Saved at {}\".format(self.saved_model_location))\n",
    "        return self.losses, rewards\n",
    "\n",
    "    def sample_from_env(self):\n",
    "        #Get A,B matrices from lstm\n",
    "        self.lstm_inputs = np.reshape(np.array([self.gravity,self.cart_mass,self.pole_mass,\n",
    "                                           self.pole_length,0,0,0,0]*self.batch_size*self.sample_len, dtype = np.float32),\n",
    "                                 (self.batch_size,self.sample_len,self.lstm_input_dim))\n",
    "        feed_dict = {self.lstm_input:self.lstm_inputs}\n",
    "        A,B = self.sess.run([self.A,self.B],feed_dict=feed_dict)\n",
    "        \n",
    "        #setup lists to hold states, controls, and rewards\n",
    "        states = [[] for _ in range(self.batch_size)]\n",
    "        u = [[] for _ in range(self.batch_size)]\n",
    "        rewards = np.array([0]*self.batch_size)\n",
    "        \n",
    "        for episode in range(self.batch_size):\n",
    "            K = [ct.lqr(A[episode,t],B[episode,t],self.Q,self.R)[0] for t in range(200)]\n",
    "            observation = self.env.reset()\n",
    "            states[episode].append(observation)\n",
    "            for t in range(self.sample_len-1):\n",
    "                u[episode].append(-np.dot(K[t],observation)[0])\n",
    "#                 u = np.random.normal(1,size=[4])\n",
    "                observation, reward, done, info = self.env.step(u[episode][t])\n",
    "                if done:\n",
    "#                     print(\"Episode finished at time step {}\".format(t+1))\n",
    "                    break\n",
    "                states[episode].append(observation)\n",
    "                rewards[episode]+=1\n",
    "#             print(\"Episode complete\")  \n",
    "#         temp = [np.asarray(x) for x in states]\n",
    "#         max_trial_len = np.max([trial.shape[0] for trial in temp])\n",
    "#         new_z = []\n",
    "#         new_control = []\n",
    "#         for trial in temp:\n",
    "#             new_z.append(np.pad(trial,((0,max_trial_len-trial.shape[0]),(0,0)),'constant', constant_values=(0)))\n",
    "#         for trial in \n",
    "        self.z, z_mask = arrayify(states)\n",
    "        self.z_mask = tf.convert_to_tensor(z_mask,dtype=tf.bool)\n",
    "        self.rewards = rewards\n",
    "        self.u = np.expand_dims(np.expand_dims(arrayify(u)[0],-1),-1)\n",
    "        return self\n",
    "#         return A, B\n",
    "\n",
    "    def build_new_LSTM(self):\n",
    "        with tf.name_scope('LSTM'):\n",
    "            with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "\n",
    "                lstms = [tf.contrib.rnn.LSTMCell(size, reuse=tf.get_variable_scope().reuse) for size in self.lstm_sizes]\n",
    "                dropouts = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = 0.5) for lstm in lstms]\n",
    "\n",
    "                self.cell = tf.contrib.rnn.MultiRNNCell(dropouts)\n",
    "#                 if prev_state:\n",
    "#                     initial_state = prev_state\n",
    "#                 else:\n",
    "#                     initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "#                 self.lstm_output, self.final_state = tf.nn.dynamic_rnn(cell, self.lstm_input, initial_state = initial_state)\n",
    "        return self\n",
    "\n",
    "    def new_affine(self,lstm_output,first=False):\n",
    "        with tf.variable_scope('affine_transformations'):\n",
    "            if first:\n",
    "                mu_0 = tf.transpose(tf.add(tf.matmul(lstm_output, self.W_mu_0), self.bias_mu_0))\n",
    "                self.mu_0 = ((self.mu_0_upper_bound-self.mu_0_lower_bound)/(1+tf.exp(-mu_0)))+self.mu_0_lower_bound\n",
    "\n",
    "                Sigma_0 = tf.transpose(tf.add(tf.matmul(lstm_output, self.W_Sigma_0), self.bias_Sigma_0))\n",
    "                self.Sigma_0 = ((self.Sigma_0_upper_bound-self.Sigma_0_lower_bound)/(1+tf.exp(-Sigma_0)))+self.Sigma_0_lower_bound\n",
    "\n",
    "                l_0_distribution = tfd.MultivariateNormalDiag(loc = self.mu_0, scale_diag = self.Sigma_0)\n",
    "                self.l_0 = l_0_distribution.sample()\n",
    "                return self\n",
    "\n",
    "            A = tf.reshape(tf.add(tf.matmul(lstm_output, self.W_A), self.bias_A),shape=(self.m,self.n))\n",
    "            B = tf.reshape(tf.add(tf.matmul(lstm_output, self.W_B), self.bias_B),shape=(self.m,self.r))\n",
    "            \n",
    "            g = tf.transpose(tf.add(tf.matmul(lstm_output, self.W_g), self.bias_g))\n",
    "            g = ((self.g_upper_bound-self.g_lower_bound)/(1+tf.exp(-g)))+self.g_lower_bound\n",
    "\n",
    "            sigma = tf.transpose(tf.add(tf.matmul(lstm_output, self.W_sigma), self.bias_sigma))\n",
    "            sigma = (self.sigma_upper_bound)/(1+tf.exp(-sigma))\n",
    "            \n",
    "        return A,B,g,sigma\n",
    "        \n",
    "    def new_train(self, epochs):\n",
    "        '''In this method, we require each epoch as 1 training sample\n",
    "            We will generate single steps at a time for input to the LSTM,\n",
    "            and then single step calculations of the optimal control\n",
    "        '''\n",
    "        \n",
    "        #Delete this section\n",
    "        Q = np.eye(4)*[10,1,1,1]\n",
    "        R = 1\n",
    "        \n",
    "        \n",
    "        start = time.time()\n",
    "        self.rewards = [0 for _ in range(epochs)]\n",
    "        self.environment_states = [[] for _ in range(epochs)]\n",
    "        self.KF_states = [[] for _ in range(epochs)]\n",
    "            # required for likelihood/loss: A,B,u,C,l_a_posteriori,P_a_posteriorig,sigma\n",
    "        for i in range(epochs):\n",
    "            done = False\n",
    "            observation = self.env.reset()\n",
    "            self.environment_states[i].append(tf.convert_to_tensor(observation,dtype=tf.float32))\n",
    "            preds = []\n",
    "            epoch_loss = []\n",
    "            initial_state = self.cell.zero_state(batch_size=1,dtype=tf.float32)\n",
    "            initial_input = tf.concat((self.env_params,np.zeros(shape=[1,4])),axis=1)\n",
    "\n",
    "            with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "                output_single,state_single = self.cell(inputs = initial_input, state = initial_state)\n",
    "                self.new_affine(output_single, first=True)\n",
    "            \n",
    "            u = tf.zeros(shape = [1,self.r], dtype=tf.float32)\n",
    "            l_a_posteriori = self.l_0\n",
    "            P_a_posteriori = self.P_0\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            while not done:\n",
    "                #Try parameterizing u by NN, include it in new_affine\n",
    "                \n",
    "                A, B, g, sigma = self.new_affine(output_single)\n",
    "                A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z = \\\n",
    "                        forward_filter_fn(A,B,u,self.C,l_a_posteriori,\n",
    "                                          P_a_posteriori,g,sigma,self.environment_states[i][-1])\n",
    "                self.KF_states.append((A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z))\n",
    "                \n",
    "                \n",
    "                #Delete this section\n",
    "                K,S,E = ct.lqr(A.eval(),B.eval(),Q,R)\n",
    "                u = -tf.matmul(K.astype(np.float32),tf.expand_dims(self.environment_states[i][-1],-1))\n",
    "                #u needs to be chosen in a better fashion\n",
    "                \n",
    "                observation,reward,done,info = self.env.step(np.squeeze(u.eval()))\n",
    "                self.environment_states[i].append(tf.convert_to_tensor(observation,dtype=tf.float32))\n",
    "                self.rewards[i] += 1\n",
    "                \n",
    "                '''Calculate filtered distributions with KF\n",
    "                    Approximate the control\n",
    "                    Call the next step with calculated control\n",
    "                    update lstm input with output of the observation\n",
    "                '''\n",
    "                next_input = tf.concat((self.env_params,tf.expand_dims(self.environment_states[i][-1],0)),axis=1)\n",
    "                output_single,state_single = self.cell(inputs=next_input,state=state_single)\n",
    "            print(\"Epoch ended after {} minutes, with {} rewards\".format((time.time()-start)/60,self.rewards[i]))\n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alon/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-28061d8d5f62>:388: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-28061d8d5f62>:391: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/alon/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch ended after 0.40029985904693605 minutes, with 37 rewards\n",
      "Epoch ended after 2.238354762395223 minutes, with 68 rewards\n",
      "Epoch ended after 5.91207184791565 minutes, with 82 rewards\n",
      "Epoch ended after 7.827035534381866 minutes, with 40 rewards\n",
      "Epoch ended after 13.441507895787558 minutes, with 81 rewards\n",
      "Epoch ended after 38.19963773886363 minutes, with 169 rewards\n",
      "Epoch ended after 42.73846125602722 minutes, with 34 rewards\n",
      "Epoch ended after 59.94321093161901 minutes, with 112 rewards\n",
      "Epoch ended after 78.75572882095973 minutes, with 108 rewards\n",
      "Epoch ended after 100.34663283427557 minutes, with 102 rewards\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    testing = LSTM_SSM_model(sess, name = 'deleteme')\n",
    "    testing.build_new_LSTM()\n",
    "    testing.sess.run(tf.global_variables_initializer())\n",
    "    testing.new_train(10)\n",
    "    rewards=testing.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    example = LSTM_SSM_model(sess, name = 'deleteme', batch_size = 3)\n",
    "#     tvars = tf.trainable_variables()\n",
    "#     tvars_vals = example.sess.run(tvars)\n",
    "#     for var, val in zip(tvars, tvars_vals):\n",
    "#         print(var.name, val.shape)\n",
    "    example.build_LSTM().affine_transformations()\n",
    "    example.sess.run(tf.global_variables_initializer())\n",
    "#     A, B = example.sample_from_env()\n",
    "    example.sample_from_env()\n",
    "    example.build_model().build_loss()\n",
    "    example.sess.run(tf.global_variables_initializer())\n",
    "    loss, rewards = example.train(epochs = 500)\n",
    "#     feed_dict = {example.lstm_input:example.lstm_inputs}\n",
    "#     z_probs,z_mask = example.sess.run([example.z_probability,example.z_mask], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        if np.linalg.matrix_rank(ct.ctrb(A[i][j],B[i][j])) != 4:\n",
    "            print(np.linalg.matrix_rank(ct.ctrb(A[i][j],B[i][j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.z[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = [example.z[0,x,0] for x in range(example.z.shape[0])]\n",
    "second = [example.z[0,x,1] for x in range(example.z.shape[0])]\n",
    "third = [example.z[0,x,2] for x in range(example.z.shape[0])]\n",
    "fourth = [example.z[0,x,3] for x in range(example.z.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(first)\n",
    "plt.plot(second)\n",
    "plt.plot(third)\n",
    "plt.plot(fourth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_magnitude = np.array([0.01,0.1,0.01,0.1])\n",
    "av_time = []\n",
    "\n",
    "env = gym.make('Custom_CartPole-v0', thetaacc_error=2, initial_state=1)\n",
    "g = env.gravity\n",
    "M = env.masscart\n",
    "m = env.masspole\n",
    "l = env.length\n",
    "Q = np.eye(4)*[10,1,1,1]\n",
    "R = 1\n",
    "\n",
    "'''System of equations'''\n",
    "A = np.array([[0,1,0,0],[0,0,-m*g/M,0],[0,0,0,1],[0,0,(M+m)*g/(l*M),0]])\n",
    "B = np.array([[0,1/M,0,-1/(l*M)]]).T\n",
    "\n",
    "\n",
    "'''LQR'''\n",
    "import time\n",
    "K,S,E = ct.lqr(A,B,Q,R)\n",
    "'''Pole Placement'''\n",
    "#K = ct.place(A,B,np.array([-1.1,-1.2,-1.3,-1.4]))\n",
    "\n",
    "\n",
    "#env.x_threshold = 5.0\n",
    "#env.theta_threshold_radians = 10.0\n",
    "\n",
    "\n",
    "states = [[] for _ in range(5)]\n",
    "rewards = np.array([0]*5)\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    states[i_episode].append(observation)\n",
    "    for t in range(500):\n",
    "#         env.render()\n",
    "        u = -np.dot(K,observation)\n",
    "        observation, reward, done, info = env.step(u[0])\n",
    "        states[i_episode].append(observation)\n",
    "        if done:\n",
    "            print(\"Episode finished at time step {}\".format(t+1))\n",
    "            break\n",
    "        rewards[i_episode]+=1\n",
    "    print(\"Episode complete\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
