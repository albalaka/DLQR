{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import control as ct\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter(object):\n",
    "    \"\"\"\n",
    "    This class defines a kalman filter\n",
    "\n",
    "    l - latent state\n",
    "    l_a_priori - A priori state estimate\n",
    "    l_a_posteriori - A posteriori state estimate\n",
    "\n",
    "    P_a_priori - A priori error covariance\n",
    "    P_a_posteriori - A posteriori error covariance\n",
    "\n",
    "    C - observation model\n",
    "    Q - covariance of the process noise    \n",
    "    a, b - observation model and bias\n",
    "    R - covariance of the observation noise\n",
    "    z - observation\n",
    "\n",
    "    y_pre - measurement pre-fit residual\n",
    "    S - Pre-fit residual covariance\n",
    "    K - Kalman gain\n",
    "    y_post - measurement post-fit residual\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, dim_z, batch_size, **kwargs):\n",
    "        self.m = m\n",
    "        self.dim_z = dim_z\n",
    "        self.dim_y = dim_z\n",
    "\n",
    "        # lambda initializer for identity matrices\n",
    "        self.eye_init = lambda shape, dtype = np.float32: np.eye(*shape, dtype = dtype)\n",
    "\n",
    "        self._I = tf.constant(self.eye_init((m, m)), name= 'I')\n",
    "\n",
    "        '''This section also cannot handle missing kwargs'''\n",
    "        self.l_0 = kwargs.pop('l_0', None)\n",
    "        self.P_0 = kwargs.pop('P_0', None)\n",
    "        self.A = kwargs.pop('A', None)\n",
    "        self.B = kwargs.pop('B', None)\n",
    "        self.u = kwargs.pop('u', None)\n",
    "        self.C = kwargs.pop('C', None)\n",
    "        self.g = kwargs.pop('g', None)\n",
    "        self.sigma = kwargs.pop('sigma', None)\n",
    "        self.y_0 = kwargs.pop('y_0', None)\n",
    "        self.z_0 = kwargs.pop('z_0', None)\n",
    "        self.pred_0 = kwargs.pop('pred_0', None)\n",
    "        self.z = kwargs.pop('z', None)\n",
    "        \n",
    "#         self.g_pred = kwargs.pop('g_pred', None)\n",
    "#         self.sigma_pred = kwargs.pop('sigma_pred', None)\n",
    "#         self.l_0_pred = kwargs.pop('l_0_pred', None)\n",
    "#         self.z_0_pred = kwargs.pop('z_0_pred', None)\n",
    "#         self.A_pred = kwargs.pop('A_pred', None)\n",
    "#         self.B_pred = kwargs.pop('B_pred', None)\n",
    "#         self.u_pred = kwargs.pop('u_pred', None)\n",
    "#         self.C_pred = kwargs.pop('C_pred', None)\n",
    "\n",
    "\n",
    "    def forward_filter_fn(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Forward step over a batch\n",
    "        params contains: l_a_posteriori, P_a_posteriori, y_pre\n",
    "        inputs contains: z, F, g, sigma, a, b\n",
    "\n",
    "        Calculates prior distributions based on the given posterior distributions and the current residual\n",
    "                updates posterior distributions based on the new prior distributions\n",
    "        \"\"\"\n",
    "        '''Shapes:\n",
    "            z = (bs, dim_z)\n",
    "            l_a_posteriori = (bs, m, dim_z)\n",
    "            P_a_posteriori = (bs, m, m)\n",
    "            F = (bs, m, m)\n",
    "            Q = (bs, m, m)\n",
    "            R = (bs, dim_z, dim_z)\n",
    "            a = (bs, m, dim_z)\n",
    "            b = (bs, dim_z)\n",
    "        '''\n",
    "        z, A, B, u, g, sigma, C = inputs\n",
    "        l_a_posteriori, P_a_posteriori, y_pre, pred = params\n",
    "\n",
    "#         print('z',z.shape)\n",
    "#         print('A', A.shape)\n",
    "#         print('B',B.shape)\n",
    "#         print('u',u.shape)\n",
    "#         print('g',g.shape)\n",
    "#         print('sigma',sigma.shape)\n",
    "#         print('C', C.shape)\n",
    "#         print('l_a_posteriori', l_a_posteriori.shape)\n",
    "#         print('P_a_posteriori', P_a_posteriori.shape)\n",
    "#         print('y_pre', y_pre.shape)\n",
    "#         print('pred', pred.shape)\n",
    "        \n",
    "        \n",
    "        l_a_priori = tf.matmul(A,l_a_posteriori) + tf.matmul(B,u)\n",
    "#         print('l_a_priori',l_a_priori.shape)\n",
    "        P_a_priori = tf.matmul(tf.matmul(A,P_a_posteriori), A, transpose_b = True) + tf.matmul(g,g, transpose_b=True)\n",
    "#         print('P_a_priori', P_a_priori.shape)\n",
    "\n",
    "        y_pre = tf.expand_dims(z,-1) - tf.matmul(C,l_a_priori)\n",
    "\n",
    "        S = tf.matmul(sigma, sigma, transpose_b=True) + \\\n",
    "            tf.matmul(tf.matmul(C, P_a_priori), C, transpose_b=True)\n",
    "       \n",
    "        '''TODO: Compute inverse using cholesky decomposition? Only works if a is matrix\n",
    "                so z must be multivariate\n",
    "        '''\n",
    "#         S_inv = tf.linalg.inv(S)\n",
    "        S_inv = tfp.math.pinv(S)\n",
    "        \n",
    "        \n",
    "        K = tf.matmul(tf.matmul(P_a_priori, C, transpose_b=True), S_inv)\n",
    "        l_a_posteriori = l_a_priori + tf.matmul(K,y_pre)\n",
    "        I_KC = self._I-tf.matmul(K,C)\n",
    "        P_a_posteriori = tf.matmul(tf.matmul(I_KC, P_a_priori), I_KC, transpose_b=True) + \\\n",
    "                         tf.matmul(tf.matmul(K,tf.matmul(sigma, sigma, transpose_b = True)),\n",
    "                                   K, transpose_b=True)\n",
    "        y_post = z - tf.squeeze(tf.matmul(C,l_a_posteriori),-1)\n",
    "        pred = tf.squeeze(tf.matmul(C, l_a_posteriori),-1)\n",
    "        \n",
    "#         print('l_a_posteriori', l_a_posteriori.shape)\n",
    "#         print('P_a_posteriori', P_a_posteriori.shape)\n",
    "#         print('y_post', y_post.shape)\n",
    "#         print('pred', pred.shape)\n",
    "        return l_a_posteriori, P_a_posteriori, y_post, pred\n",
    "\n",
    "    def forward_filter(self):\n",
    "        \"\"\"\n",
    "        Compute the forward step in Kalman Filter\n",
    "        The forward pass is initialized with p(x_1) = N(self.x, self.P)\n",
    "        We return the mean and covariance for p(x_t|x_tm1) for t=2, ..., T+1\n",
    "        and the filtering distribution p(x_t|z_1:t) for t=1, ..., T\n",
    "        \"\"\"\n",
    "#         print('z',self.z.shape)\n",
    "#         print('A', self.A.shape)\n",
    "#         print('B',self.B.shape)\n",
    "#         print('u',self.u.shape)\n",
    "#         print(type(self.u[0,0]))\n",
    "#         print('g',self.g.shape)\n",
    "#         print('sigma', self.sigma.shape)\n",
    "#         print('C', self.C.shape)\n",
    "#         print('l_0', self.l_0.shape)\n",
    "#         print('P_0', self.P_0.shape)\n",
    "#         print('y_0', self.y_0.shape)\n",
    "#         print('pred_0', self.pred_0.shape)\n",
    "        forward_states = tf.scan(self.forward_filter_fn,\n",
    "                                 elems = (trans(self.z),trans(self.A),\n",
    "                                          trans(self.B),trans(self.u),\n",
    "                                          trans(self.g),trans(self.sigma),\n",
    "                                          trans(self.C)),\n",
    "                                initializer=(self.l_0, self.P_0, self.y_0, self.pred_0))\n",
    "        \n",
    "        return forward_states\n",
    "    \n",
    "    def Kfilter(self):\n",
    "        l_filtered, P_filtered, residuals, filtered_prediction = self.forward_filter()\n",
    "        return trans(l_filtered), trans(P_filtered), trans(residuals), trans(filtered_prediction)\n",
    "        \n",
    "    def forward_predict_fn(self, params, inputs):\n",
    "        \"\"\"Forward step over a batch\n",
    "        params contains l_prev, z_prev\n",
    "        inputs contains F, g, a, b, sigma\"\"\"\n",
    "        \n",
    "        A, B, g, C, sigma = inputs\n",
    "        l_prev, z_prev = params\n",
    "        \n",
    "#         l_next = tfd.MultivariateNormalDiag(loc = tf.matmul(F, l_prev), scale_diag = g).sample()\n",
    "        l_next = tf.matmul(A, l_prev) + tf.matmul(B, u)\n",
    "#         z_next = tfd.Normal(loc = tf.matmul(a, l_prev, transpose_a=True)+b, scale = sigma).sample()\n",
    "        z_next = tf.matmul(C, l_prev)\n",
    "        return l_next, z_next\n",
    "    \n",
    "    def forward_predict(self):\n",
    "        \"\"\"\n",
    "        Compute the predictions in state space model\n",
    "        The forward pass is initialized by l_T = p(l_T|z_1:T)\n",
    "        We return the hidden states l_T+1:T+t and predictions z_T+1:T+t\n",
    "        \"\"\"\n",
    "        \n",
    "        forward_predictions = tf.scan(self.forward_predict_fn,\n",
    "                                      elems = (trans(self.A_pred), trans(self.B_pred),\n",
    "                                               trans(self.g_pred), trans(self.C_pred),\n",
    "                                               trans(self.sigma_pred)),\n",
    "                                      initializer = (self.l_0_pred, self.z_0_pred))\n",
    "        \n",
    "        return forward_predictions\n",
    "        \n",
    "    def Kpredict(self):\n",
    "        \n",
    "        l_predicted, z_predicted = self.forward_predict()\n",
    "        return trans(l_predicted), trans(z_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(tensor):\n",
    "    if len(tensor.shape)==3:\n",
    "        return tf.transpose(tensor, [1,0,2])\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1,0,2,3])\n",
    "    \n",
    "# def arrayify(l):\n",
    "#     temp = [np.asarray(x, dtype = np.float32) for x in l]\n",
    "#     max_trial_len = np.max([trial.shape[0] for trial in temp])\n",
    "#     trial_lens = []\n",
    "#     new = []\n",
    "#     for trial in temp:\n",
    "#         if len(trial.shape) == 2:\n",
    "#             new.append(np.pad(trial,((0,200-trial.shape[0]),(0,0)),'constant', constant_values=(0)))\n",
    "#             trial_lens.append(trial.shape[0])\n",
    "#         else:\n",
    "#             new.append(np.pad(trial,(0,200-trial.shape[0]),'constant',constant_values=0))\n",
    "#             trial_lens.append(trial.shape[0])\n",
    "#     new = np.stack(new,axis = 0)\n",
    "#     if len(new.shape) >2:\n",
    "#         mask = np.zeros(new.shape[:-1], dtype = np.float32)\n",
    "#     else:\n",
    "#         mask = np.zeros(new.shape, dtype = np.float32)\n",
    "#     for idx,length in enumerate(trial_lens):\n",
    "#         mask[idx,:length+1] += 1\n",
    "#     return new, mask.astype(np.bool)\n",
    "\n",
    "def forward_filter_fn(A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z):\n",
    "    '''Calculates prior distributions based on the given posterior distributions\n",
    "        and the current residual updates posterior distributions based on the new\n",
    "        prior distributions\n",
    "    '''\n",
    "#     print('z',z.shape)\n",
    "#     print('A', A.shape)\n",
    "#     print('B',B.shape)\n",
    "#     print('u',u.shape)\n",
    "#     print('g',g.shape)\n",
    "#     print('sigma',sigma.shape)\n",
    "#     print('C', C.shape)\n",
    "#     print('l_a_posteriori', l_a_posteriori.shape)\n",
    "#     print('P_a_posteriori', P_a_posteriori.shape)\n",
    "    _I = tf.eye(int(A.shape[0]), dtype = tf.float32)\n",
    "    \n",
    "    z = tf.expand_dims(z,-1)\n",
    "    l_a_priori = tf.matmul(A,l_a_posteriori) + tf.matmul(B,u)\n",
    "#     print('l_a_priori',l_a_priori.shape)\n",
    "    P_a_priori = tf.matmul(tf.matmul(A,P_a_posteriori), A, transpose_b = True) + tf.matmul(g,g, transpose_b=True)\n",
    "#     print('P_a_priori',P_a_priori.shape)\n",
    "    y_pre = z - tf.matmul(C,l_a_priori)\n",
    "#     print('y_pre', y_pre.shape)\n",
    "    S = tf.matmul(sigma, sigma, transpose_b=True) + \\\n",
    "        tf.matmul(tf.matmul(C, P_a_priori), C, transpose_b=True)\n",
    "#     print('S', S.shape)\n",
    "    \n",
    "    S_inv = tf.linalg.inv(S)\n",
    "    K = tf.matmul(tf.matmul(P_a_priori, C, transpose_b=True), S_inv)\n",
    "#     print('K', K.shape)\n",
    "    l_a_posteriori = l_a_priori + tf.matmul(K,y_pre)\n",
    "#     print('l_a_posteriori', l_a_posteriori.shape)\n",
    "    I_KC = _I-tf.matmul(K,C)\n",
    "#     print('I-KC', I_KC.shape)\n",
    "    P_a_posteriori = tf.matmul(tf.matmul(I_KC, P_a_priori), I_KC, transpose_b=True) + \\\n",
    "                        tf.matmul(tf.matmul(K,tf.matmul(sigma, sigma, transpose_b = True)),\n",
    "                                K, transpose_b=True)\n",
    "#     print('P_a_posteriori',P_a_posteriori.shape)\n",
    "    y_post = z-tf.matmul(C,l_a_posteriori)\n",
    "#     print('y_post', y_post.shape)\n",
    "    pred = tf.matmul(C, l_a_posteriori)\n",
    "#     print('pred', pred.shape)\n",
    "        \n",
    "    return A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z\n",
    "\n",
    "def dict_append(d,new_items,vals):\n",
    "            for idx,item in enumerate(vals):\n",
    "                d[item].append(new_items[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_SSM_model(object):\n",
    "    def __init__(self, sess, name, m = 4, n=4, r=1,\n",
    "                 learning_rate = 0.00001, lr_decay = 0.95, sigma_upper_bound = 1,\n",
    "                 sigma_lower_bound = 0, g_upper_bound = 1,\n",
    "                 g_lower_bound = 0.1, mu_0_upper_bound = 1,mu_0_lower_bound = 0,\n",
    "                 Sigma_0_upper_bound = 1, Sigma_0_lower_bound = 0, beta = 0.00001,\n",
    "                 b_upper_bound = 0.25, b_lower_bound = -0.25,thetaacc_error=0,initial_state=0.1\n",
    "                ):\n",
    "        \n",
    "        '''thetaacc_error gives the amount of random angular acceleration that can be put on the pendulum,\n",
    "        initial_state gives the amount of variation in the initial state\n",
    "        '''\n",
    "        if name == '':\n",
    "            raise NameError(\"A model has no name\")\n",
    "\n",
    "        '''This functions assumes the state space model:\n",
    "            l_(t+1) = A_(t)l_(t)+B(t)u_(t)\n",
    "            z_(t+1) = C_(t)l_(t)\n",
    "            where:\n",
    "            l has dim m\n",
    "            u has dim r\n",
    "            z has dim m\n",
    "            A has dim mxn\n",
    "            B has dim mxr\n",
    "            C has dim mxm\n",
    "            '''\n",
    "            \n",
    "        self.sess = sess\n",
    "        \n",
    "        '''nn model hyperparameters'''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_sample_len = tf.Variable(200, name = 'sample_len', trainable=False)\n",
    "        self.global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "        self.increment_global_step = tf.assign_add(self.global_step,1, name = 'increment_global_step')\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lstm_sizes = [128,64]\n",
    "        last_lstm = self.lstm_sizes[-1]\n",
    "\n",
    "        '''dims'''\n",
    "        self.m = m # size of the state space\n",
    "        self.dim_z = m\n",
    "        self.n = n\n",
    "        self.r = r\n",
    "        self.lstm_input_dim = m+4 # previous states plus physical parameters\n",
    "        \n",
    "        \n",
    "        self.initial_variance = 1\n",
    "        \n",
    "        '''error bounds'''\n",
    "        self.sigma_upper_bound = sigma_upper_bound\n",
    "        self.g_lower_bound = g_lower_bound\n",
    "        self.g_upper_bound = g_upper_bound\n",
    "        self.mu_0_upper_bound = mu_0_upper_bound\n",
    "        self.mu_0_lower_bound = mu_0_lower_bound\n",
    "        self.Sigma_0_upper_bound = Sigma_0_upper_bound\n",
    "        self.Sigma_0_lower_bound= Sigma_0_lower_bound\n",
    "        self.b_upper_bound = b_upper_bound\n",
    "        self.b_lower_bound = b_lower_bound\n",
    "        self.beta = beta\n",
    "        \n",
    "        '''LQR parameters'''\n",
    "        self.env = gym.make('Custom_CartPole-v0', thetaacc_error=thetaacc_error, initial_state=initial_state)\n",
    "        self.gravity = self.env.gravity\n",
    "        self.cart_mass = self.env.masscart\n",
    "        self.pole_mass = self.env.masspole\n",
    "        self.pole_length = self.env.length\n",
    "        self.env_params = tf.expand_dims(np.array([self.gravity, self.cart_mass,\n",
    "                                                   self.pole_mass,self.pole_length], dtype=np.float32),0)\n",
    "        self.Q = np.eye(4)*[1,1,1,1]\n",
    "        self.R = 1\n",
    "        \n",
    "        self.KF_states = {'A':[],'B':[],'u':[],'C':[],'l_a_posteriori':[],'P_a_posteriori':[],\n",
    "                          'g':[],'sigma':[],'z':[]}\n",
    "        self.vals = ['A','B','u','C','l_a_posteriori','P_a_posteriori','g','sigma','z']\n",
    "        '''This section for potentially using tf.assign, or scatter update of some sort to potentially\n",
    "            improve epoch time'''\n",
    "#         self.A = tf.Variable(tf.zeros([self.max_sample_len,m,n]),dtype=tf.float32,name='A',trainable=False)\n",
    "#         self.B = tf.Variable(tf.zeros([self.max_sample_len,m,r]),dtype=tf.float32,name='B',trainable=False)\n",
    "#         self.u = tf.Variable(tf.zeros([self.max_sample_len,r,1]),dtype=tf.float32,name='u',trainable=False)\n",
    "#         self.l_a_posteriori=tf.Variable(tf.zeros([self.max_sample_len,m,1]),dtype=tf.float32,\n",
    "#                                         name='l_a_posteriori',trainable=False)\n",
    "#         self.P_a_posteriori=tf.Variable(tf.zeros([self.max_sample_len,m,m]),dtype=tf.float32,\n",
    "#                                        name='P_a_posteriori',trainable=False)\n",
    "#         self.g = tf.Variable(tf.zeros([self.max_sample_len,m,1]),dtype=tf.float32,name='g',trainable=False)\n",
    "#         self.sigma=tf.Variable(tf.zeros([self.max_sample_len,self.dim_z,1]),dtype=tf.float32,name='sigma',\n",
    "#                               trainable=False)\n",
    "#         self.z = tf.Variable(tf.zeros([self.max_sample_len,self.dim_z,1]),dtype=tf.float32,name='z',\n",
    "#                             trainable=False)\n",
    "        \n",
    "        \n",
    "        self.y_0 = tf.Variable(tf.zeros([self.dim_z]), dtype = tf.float32, name = 'y_0', trainable = False)\n",
    "#         A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z\n",
    "        \n",
    "        '''Saving model stuff, don\"t need for now'''\n",
    "#         self.model_folder = 'quantitative_tmp/{}'.format(name)\n",
    "#         if not os.path.isdir(self.model_folder):\n",
    "#             print('This model has no folder')\n",
    "#             os.makedirs(self.model_folder)\n",
    "#         self.saved_model_location = '{}/model.ckpt'.format(self.model_folder)\n",
    "\n",
    "        self.losses = []\n",
    "        self.saver = None\n",
    "        \n",
    "        with tf.variable_scope('KF', reuse = tf.AUTO_REUSE):\n",
    "            self.C = tf.get_variable(initializer = tf.eye(self.dim_z),dtype = tf.float32,\n",
    "                                     name = 'C', trainable = False)\n",
    "            self.W_A = tf.get_variable(initializer = tf.random.normal([last_lstm, m*n]),\n",
    "                                       dtype = tf.float32, name = 'W_A')\n",
    "            self.bias_A = tf.get_variable(initializer = tf.zeros([1, m*n]),\n",
    "                                          dtype = tf.float32, name = 'bias_A')\n",
    "            \n",
    "            self.W_B = tf.get_variable(initializer = tf.random.normal([last_lstm, m*r]),\n",
    "                                       dtype = tf.float32, name = 'W_B')\n",
    "            self.bias_B = tf.get_variable(initializer = tf.zeros([1, m*r]),\n",
    "                                         dtype = tf.float32, name = 'bias_B')\n",
    "\n",
    "            self.W_g = tf.get_variable(initializer = tf.random.normal([last_lstm, m]),\n",
    "                                       dtype = tf.float32, name = 'W_g')\n",
    "            self.bias_g = tf.get_variable(initializer = tf.zeros([1, m]),\n",
    "                                          dtype = tf.float32, name = 'bias_g')\n",
    "\n",
    "            self.W_sigma = tf.get_variable(initializer = tf.random.normal([last_lstm, self.dim_z]),\n",
    "                                           dtype = tf.float32, name = 'W_sigma')\n",
    "            self.bias_sigma = tf.get_variable(initializer = tf.zeros([1, self.dim_z]),\n",
    "                                              dtype = tf.float32, name = 'bias_sigma')\n",
    "\n",
    "            self.W_mu_0 = tf.get_variable(initializer = tf.random.normal([last_lstm, self.m]),\n",
    "                                          dtype = tf.float32, name = 'W_mu_0')\n",
    "            self.bias_mu_0 = tf.get_variable(initializer = tf.zeros([1, self.m]),\n",
    "                                             dtype = tf.float32, name = 'bias_mu_0')\n",
    "            \n",
    "            self.W_Sigma_0 = tf.get_variable(initializer = tf.random.normal([last_lstm, self.m]),\n",
    "                                             dtype = tf.float32, name = 'W_Sigma_0')\n",
    "            self.bias_Sigma_0 = tf.get_variable(initializer = tf.zeros([1, self.m]),\n",
    "                                                dtype = tf.float32, name = 'bias_Sigma_0')\n",
    "            \n",
    "            self.P_0 = tf.Variable(self.initial_variance*tf.eye(self.m,dtype = tf.float32),\n",
    "                                   name = 'P_0', trainable = False)\n",
    "\n",
    "            self.y_0 = tf.Variable(tf.zeros([self.dim_z]), dtype = tf.float32, name = 'y_0', trainable = False)\n",
    "            self.z_0 = tf.Variable(tf.zeros([self.dim_z, self.dim_z]), dtype = tf.float32, name = 'z_0', trainable = False)\n",
    "            self.pred_0 = tf.Variable(tf.zeros([self.dim_z]), dtype = tf.float32, name = 'pred_0', trainable = False)\n",
    "            \n",
    "            \n",
    "            '''Variables for test range in LQE only'''\n",
    "#             self.A_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, m, n], name = 'A_test')\n",
    "#             self.B_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, m, r], name = 'B_test')\n",
    "#             self.g_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, self.m, 1], name = 'g_test')\n",
    "#             self.sigma_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, self.dim_z, 1], name = 'sigma_test')\n",
    "#             self.l_0_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.m, 1], name = 'l_0_test')\n",
    "#             self.final_z = tf.placeholder(tf.float32, shape = [self.batch_size, 1, self.dim_z], name = 'final_z')\n",
    "            \n",
    "#         with tf.variable_scope('LSTM', reuse = tf.AUTO_REUSE):\n",
    "#             self.lstm_input = tf.placeholder(tf.float32, shape=\n",
    "#                                              [self.batch_size, self.sample_len,self.lstm_input_dim],\n",
    "#                                              name = 'lstm_input')\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, shape = [self.dim_z],name = 'z')\n",
    "        \n",
    "        with tf.variable_scope('LSTM', reuse = tf.AUTO_REUSE):\n",
    "            self.lstm_input = tf.placeholder(tf.float32, shape = [1, self.lstm_input_dim],\n",
    "                                            name = 'lstm_input')\n",
    "\n",
    "    def affine_transformations(self):\n",
    "\n",
    "        with tf.variable_scope('affine_transformations'):\n",
    "            \n",
    "            self.lstm_output = tf.expand_dims(self.lstm_output, -1) # (bs, sample_len, last lstm_size, 1)\n",
    "            def tile_func(Weights_or_bias):\n",
    "                if len(Weights_or_bias.shape) == 2:\n",
    "                    return tf.tile(tf.expand_dims(Weights_or_bias,1), (1,self.sample_len,1))\n",
    "                elif len(Weights_or_bias.shape) == 3:\n",
    "                    return tf.tile(tf.expand_dims(Weights_or_bias,1), (1,self.sample_len,1,1))\n",
    "                else:\n",
    "                    raise ValueError('Unknown sized Weights or bias array')\n",
    "                    \n",
    "            '''TODO: Does tile work by updating all weights for the tile back to the original matrix?    \n",
    "            '''\n",
    "            \n",
    "            W_A = tile_func(self.W_A)\n",
    "            bias_A = tile_func(self.bias_A)\n",
    "            W_B = tile_func(self.W_B)\n",
    "            bias_B = tile_func(self.bias_B)\n",
    "            W_g = tile_func(self.W_g)\n",
    "            bias_g = tile_func(self.bias_g)\n",
    "            W_sigma = tile_func(self.W_sigma)\n",
    "            bias_sigma = tile_func(self.bias_sigma)\n",
    "\n",
    "            self.A = tf.reshape(tf.add(tf.matmul(W_A, self.lstm_output), bias_A),\n",
    "                                [self.batch_size,self.sample_len,self.m,self.n])\n",
    "            self.B = tf.reshape(tf.add(tf.matmul(W_B, self.lstm_output), bias_B),\n",
    "                                [self.batch_size,self.sample_len,self.m,self.r])\n",
    "\n",
    "            transition_error = tf.add(tf.matmul(W_g, self.lstm_output), bias_g)\n",
    "            self.g = ((self.g_upper_bound-self.g_lower_bound)/(1+tf.exp(-transition_error)))+self.g_lower_bound\n",
    "            \n",
    "            observation_error = tf.add(tf.matmul(W_sigma, self.lstm_output), bias_sigma)\n",
    "            self.sigma = (self.sigma_upper_bound)/(1+tf.exp(-observation_error))\n",
    "            \n",
    "            temp_mu_0 = tf.add(tf.matmul(self.W_mu_0, self.lstm_output[:,0,:]), self.bias_mu_0)\n",
    "            self.mu_0 = ((self.mu_0_upper_bound-self.mu_0_lower_bound)/(1+tf.exp(-temp_mu_0)))+self.mu_0_lower_bound\n",
    "\n",
    "            temp_Sigma_0 = tf.add(tf.matmul(self.W_Sigma_0, self.lstm_output[:,0,:]), self.bias_Sigma_0)\n",
    "            self.Sigma_0 = ((self.Sigma_0_upper_bound-self.Sigma_0_lower_bound)/(1+tf.exp(-temp_Sigma_0)))+self.Sigma_0_lower_bound\n",
    "\n",
    "            l_0_distribution = tfd.MultivariateNormalDiag(loc = self.mu_0, scale_diag = self.Sigma_0)\n",
    "\n",
    "            self.l_0 = l_0_distribution.sample()\n",
    "        return self\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.kf_train = KalmanFilter(batch_size=self.batch_size,\n",
    "                                     m=self.m,\n",
    "                                     dim_z=self.dim_z,\n",
    "                                     l_0 = self.l_0,\n",
    "                                     P_0 = self.P_0,\n",
    "                                     A = self.A,\n",
    "                                     B = self.B,\n",
    "                                     u = self.u,\n",
    "                                     C = self.C,\n",
    "                                     g = self.g,\n",
    "                                     sigma = self.sigma,\n",
    "                                     z = self.z,\n",
    "                                     y_0 = self.y_0,\n",
    "                                     pred_0 = self.pred_0\n",
    "                                    )\n",
    "        with tf.variable_scope('KF_results', reuse=tf.AUTO_REUSE):\n",
    "            self.l_filtered, self.P_filtered, self.residuals, self.filtered_predictions = self.kf_train.Kfilter()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def likelihood_fn(self, params, inputs):\n",
    "        '''Compute likelihood over a batch\n",
    "        params contains: mu, Sigma - the parameters of the likelihood distribution\n",
    "        inputs contains: calculations of mu: F, a, l_filtered==l_a_posteriori\n",
    "                        calculations of Sigma: a, F, P_a_posteriori, g, sigma\n",
    "        '''\n",
    "        A, B, u, C, g, sigma, l_filtered, P_filtered = inputs\n",
    "        mu, Sigma = params\n",
    "        '''\n",
    "        a (bs, m, 1)\n",
    "        b (bs, 1)\n",
    "        F (bs, m, m)\n",
    "        g (bs, m, 1)\n",
    "        sigma (bs, 1, 1)\n",
    "        f (bs, m, 1)\n",
    "        S (bs, m, m)\n",
    "        mu (bs, 1, 1)\n",
    "        Sigma (bs, 1, 1)\n",
    "        '''\n",
    "#         print('A',A.shape)\n",
    "#         print('B',B.shape)\n",
    "#         print('u',u.shape)\n",
    "#         print('C',C.shape)\n",
    "#         print('g',g.shape)\n",
    "#         print('sigma',sigma.shape)\n",
    "#         print('l_filtered',l_filtered.shape)\n",
    "#         print('p_filtered',P_filtered.shape)\n",
    "#         print('mu',mu.shape)\n",
    "#         print('Sigma',Sigma.shape)\n",
    "        mu = tf.matmul(C, tf.add(tf.matmul(A,l_filtered), tf.matmul(B,u)))\n",
    "#         mu = tf.add(tf.matmul(tf.matmul(a, F, transpose_a=True), f), b)\n",
    "\n",
    "        temp = tf.matmul(tf.matmul(A, P_filtered), A, transpose_b=True) + tf.matmul(g, g, transpose_b=True)\n",
    "        Sigma = tf.matmul(tf.matmul(C, temp), C, transpose_b=True) + tf.square(sigma)\n",
    "\n",
    "#         temp = tf.matmul(tf.matmul(F, S), F, transpose_b=True) + tf.matmul(g, g, transpose_b=True)\n",
    "#         Sigma = tf.matmul(tf.matmul(a, temp, transpose_a=True), a) + tf.square(sigma)\n",
    "        \n",
    "        return mu, Sigma\n",
    "    \n",
    "    def build_loss(self):\n",
    "        '''Useful shapes(Ideally):\n",
    "            l_a_posteriori(batch) - (batch_size, sample_len, m)\n",
    "            P_a_posteriori(batch) - (batch_size, sample_len, m,m)\n",
    "            \n",
    "            inputs:\n",
    "                mu_0, a, F, l_a_posteriori?\n",
    "                Sigma_0, a, R, F, P_a_posteriori, Q\n",
    "        '''\n",
    "\n",
    "\n",
    "        with tf.variable_scope('loss', reuse = tf.AUTO_REUSE):\n",
    "#             decayed_learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "#                                                        self.num_batches, self.lr_decay)\n",
    "#             print('self.C',self.C.shape)\n",
    "#             print('self.mu_0', self.mu_0.shape)\n",
    "#             print('self.B',self.B.shape)\n",
    "#             print('self.u',self.u.shape)\n",
    "            \n",
    "\n",
    "            mu_1 = tf.matmul(trans(self.C)[0], self.mu_0)+tf.matmul(trans(self.B)[0],trans(self.u)[0])\n",
    "            Sigma_1 = tf.matmul(tf.matmul(trans(self.C)[0], tf.linalg.diag(tf.squeeze(self.Sigma_0,-1))),\n",
    "                                         trans(self.C)[0], transpose_b=True)+tf.square(trans(self.sigma)[0])\n",
    "#             mu_1 = tf.add(tf.matmul(trans(self.a)[0], self.mu_0, transpose_a=True),trans(self.b)[0])\n",
    "#             Sigma_1 = tf.matmul(tf.matmul(trans(self.a)[0], tf.linalg.diag(tf.squeeze(self.Sigma_0)),\n",
    "#                                           transpose_a=True),\n",
    "#                                 trans(self.a)[0]) + tf.square(trans(self.sigma)[0])\n",
    "    \n",
    "            mu, Sigma = tf.scan(self.likelihood_fn,\n",
    "                                elems = (trans(self.A)[1:], trans(self.B)[1:],\n",
    "                                         trans(self.u)[1:], trans(self.C)[1:], trans(self.g)[1:],\n",
    "                                         trans(self.sigma)[1:],trans(self.l_filtered)[:-1],\n",
    "                                         trans(self.P_filtered)[1:]),\n",
    "                                initializer = (mu_1, Sigma_1))\n",
    "\n",
    "\n",
    "            self.mu = tf.squeeze(tf.concat([tf.expand_dims(mu_1,1), trans(mu)], 1),-1)\n",
    "            self.Sigma = tf.concat([tf.expand_dims(Sigma_1,1),trans(Sigma)], 1)\n",
    "            \n",
    "            z_distribution = tfd.MultivariateNormalDiag(loc = self.mu,\n",
    "                                                        scale_diag = tf.linalg.diag_part(self.Sigma))\n",
    "\n",
    "            z_probability = z_distribution.prob(self.z)\n",
    "            self.z_probability = tf.boolean_mask(z_probability, self.z_mask)\n",
    "            regularizers = tf.nn.l2_loss(self.W_g) + tf.nn.l2_loss(self.W_mu_0) + \\\n",
    "                        tf.nn.l2_loss(self.W_sigma) + tf.nn.l2_loss(self.W_Sigma_0) + \\\n",
    "                        tf.nn.l2_loss(self.W_A) + tf.nn.l2_loss(self.W_B)\n",
    "\n",
    "            self.loss = tf.reduce_mean(self.beta*regularizers)-tf.square(tf.reciprocal(tf.reduce_sum(tf.cast(self.z_mask, tf.float32))))*tf.reduce_sum(tf.log(self.z_probability+1e-8))\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "#             self.optimizer = tf.train.AdamOptimizer(decayed_learning_rate)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            capped_grads_and_vars = [(tf.clip_by_norm(grad, 1.), var) for grad, var in grads_and_vars]\n",
    "            self.train_op = self.optimizer.apply_gradients(capped_grads_and_vars)\n",
    "        return self\n",
    "    \n",
    "    def initialize_variables(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        try:\n",
    "            self.saver.restore(self.sess, tf.train.latest_checkpoint(self.model_folder))\n",
    "            print(\"Restoring model from {}\".format(self.saved_model_location))\n",
    "        except:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            print(\"Initializing new model at {}\".format(self.saved_model_location))\n",
    "        return self  \n",
    "    \n",
    "    def train(self, epochs):\n",
    "        start = time.time()\n",
    "        rewards = []\n",
    "#        merged = tf.summary.merge_all()\n",
    "#        writer = tf.summary.FileWriter(self.log_dir, self.sess.graph)\n",
    "        for i in range(epochs):\n",
    "            preds = []\n",
    "            epoch_loss = []\n",
    "            self.sample_from_env()\n",
    "            \n",
    "            feed_dict = {self.lstm_input: self.lstm_inputs}\n",
    "#                summary, loss_, _, _ = self.sess.run([merged, self.loss, self.train_op, self.increment_global_step],\n",
    "#                                                  feed_dict=feed_dict)\n",
    "            loss_, _, _ = self.sess.run([self.loss, self.train_op, self.increment_global_step],\n",
    "                                        feed_dict=feed_dict)\n",
    "            epoch_loss.append(loss_)\n",
    "#                writer.add_summary(summary, self.sess.run(self.global_step))\n",
    "#                if idx % 10 == 0:\n",
    "#                  print('mini epoch #{}'.format(idx))\n",
    "\n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "            self.losses.append(epoch_loss)\n",
    "            rewards.append(np.mean(self.rewards))\n",
    "            if i%50 == 0:\n",
    "                print(\"Epoch #{}\\tTime Elapsed: {}\\tNegative Log-Likelihood {}\\n\\t\\tMean Reward: {}\".\n",
    "                      format(self.sess.run(self.global_step)-1,(time.time()-start)/60,\n",
    "                             epoch_loss, np.mean(self.rewards)))\n",
    "#             if i%50 == 0:\n",
    "#                 self.saver.save(self.sess, self.saved_model_location, global_step = self.global_step)\n",
    "#                 print(\"Model Saved at {}\".format(self.saved_model_location))\n",
    "#         self.saver.save(self.sess, self.saved_model_location, global_step = self.global_step)\n",
    "#         print(\"Model Saved at {}\".format(self.saved_model_location))\n",
    "        return self.losses, rewards\n",
    "\n",
    "    def sample_from_env(self):\n",
    "        #Get A,B matrices from lstm\n",
    "        self.lstm_inputs = np.reshape(np.array([self.gravity,self.cart_mass,self.pole_mass,\n",
    "                                           self.pole_length,0,0,0,0]*self.batch_size*self.sample_len, dtype = np.float32),\n",
    "                                 (self.batch_size,self.sample_len,self.lstm_input_dim))\n",
    "        feed_dict = {self.lstm_input:self.lstm_inputs}\n",
    "        A,B = self.sess.run([self.A,self.B],feed_dict=feed_dict)\n",
    "        \n",
    "        #setup lists to hold states, controls, and rewards\n",
    "        states = [[] for _ in range(self.batch_size)]\n",
    "        u = [[] for _ in range(self.batch_size)]\n",
    "        rewards = np.array([0]*self.batch_size)\n",
    "        \n",
    "        for episode in range(self.batch_size):\n",
    "            K = [ct.lqr(A[episode,t],B[episode,t],self.Q,self.R)[0] for t in range(200)]\n",
    "            observation = self.env.reset()\n",
    "            states[episode].append(observation)\n",
    "            for t in range(self.sample_len-1):\n",
    "                u[episode].append(-np.dot(K[t],observation)[0])\n",
    "#                 u = np.random.normal(1,size=[4])\n",
    "                observation, reward, done, info = self.env.step(u[episode][t])\n",
    "                if done:\n",
    "#                     print(\"Episode finished at time step {}\".format(t+1))\n",
    "                    break\n",
    "                states[episode].append(observation)\n",
    "                rewards[episode]+=1\n",
    "#             print(\"Episode complete\")  \n",
    "#         temp = [np.asarray(x) for x in states]\n",
    "#         max_trial_len = np.max([trial.shape[0] for trial in temp])\n",
    "#         new_z = []\n",
    "#         new_control = []\n",
    "#         for trial in temp:\n",
    "#             new_z.append(np.pad(trial,((0,max_trial_len-trial.shape[0]),(0,0)),'constant', constant_values=(0)))\n",
    "#         for trial in \n",
    "        self.z, z_mask = arrayify(states)\n",
    "        self.z_mask = tf.convert_to_tensor(z_mask,dtype=tf.bool)\n",
    "        self.rewards = rewards\n",
    "        self.u = np.expand_dims(np.expand_dims(arrayify(u)[0],-1),-1)\n",
    "        return self\n",
    "#         return A, B\n",
    "\n",
    "    def build_new_LSTM(self):\n",
    "        with tf.name_scope('LSTM'):\n",
    "            with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "\n",
    "                lstms = [tf.contrib.rnn.LSTMCell(size, reuse=tf.get_variable_scope().reuse) for size in self.lstm_sizes]\n",
    "                dropouts = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = 0.5) for lstm in lstms]\n",
    "\n",
    "                self.cell = tf.contrib.rnn.MultiRNNCell(dropouts)\n",
    "#                 if prev_state:\n",
    "#                     initial_state = prev_state\n",
    "#                 else:\n",
    "#                     initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "#                 self.lstm_output, self.final_state = tf.nn.dynamic_rnn(cell, self.lstm_input, initial_state = initial_state)\n",
    "        return self\n",
    "\n",
    "    def new_affine(self,lstm_output,first=False):\n",
    "        with tf.variable_scope('affine_transformations'):\n",
    "            if first:\n",
    "                mu_0 = tf.transpose(tf.add(tf.matmul(lstm_output, self.W_mu_0), self.bias_mu_0))\n",
    "                self.mu_0 = ((self.mu_0_upper_bound-self.mu_0_lower_bound)/(1+tf.exp(-mu_0)))+self.mu_0_lower_bound\n",
    "\n",
    "                Sigma_0 = tf.transpose(tf.add(tf.matmul(lstm_output, self.W_Sigma_0), self.bias_Sigma_0))\n",
    "                self.Sigma_0 = ((self.Sigma_0_upper_bound-self.Sigma_0_lower_bound)/(1+tf.exp(-Sigma_0)))+self.Sigma_0_lower_bound\n",
    "\n",
    "                l_0_distribution = tfd.MultivariateNormalDiag(loc = self.mu_0, scale_diag = self.Sigma_0)\n",
    "                self.l_0 = l_0_distribution.sample()\n",
    "                return self\n",
    "\n",
    "            A = tf.reshape(tf.add(tf.matmul(lstm_output, self.W_A), self.bias_A),shape=(self.m,self.n))\n",
    "            B = tf.reshape(tf.add(tf.matmul(lstm_output, self.W_B), self.bias_B),shape=(self.m,self.r))\n",
    "            \n",
    "            g = tf.transpose(tf.add(tf.matmul(lstm_output, self.W_g), self.bias_g))\n",
    "            g = ((self.g_upper_bound-self.g_lower_bound)/(1+tf.exp(-g)))+self.g_lower_bound\n",
    "\n",
    "            sigma = tf.transpose(tf.add(tf.matmul(lstm_output, self.W_sigma), self.bias_sigma))\n",
    "            sigma = (self.sigma_upper_bound)/(1+tf.exp(-sigma))\n",
    "            \n",
    "        return A,B,g,sigma\n",
    "\n",
    "    def calculate_new_loss(self):\n",
    "        with tf.variable_scope('loss', reuse = tf.AUTO_REUSE):\n",
    "#             for key,value in self.KF_states.items():\n",
    "#                 print(key,value[0].shape)\n",
    "#             print('mu_0',self.mu_0.shape)\n",
    "#             print('Sigma_0',self.Sigma_0.shape)\n",
    "            mu_1 = tf.matmul(self.KF_states['C'][0], self.mu_0)+\\\n",
    "                    tf.matmul(self.KF_states['B'][0],self.KF_states['u'][0])\n",
    "            Sigma_1 = tf.matmul(tf.matmul(self.KF_states['C'][0],\n",
    "                                          tf.linalg.diag(tf.squeeze(self.Sigma_0,-1))),self.KF_states['C'][0],\n",
    "                               transpose_b=True)+tf.square(self.KF_states['sigma'][0])\n",
    "#             print('mu_1',mu_1.shape)\n",
    "#             print('Sigma_1', Sigma_1.shape)\n",
    "            \n",
    "            def make_tensor(l):\n",
    "                return tf.convert_to_tensor(l,dtype=tf.float32)\n",
    "            mu, Sigma = tf.scan(self.likelihood_fn,\n",
    "                                elems = (make_tensor(self.KF_states['A'][1:]),\n",
    "                                         make_tensor(self.KF_states['B'][1:]),\n",
    "                                         make_tensor(self.KF_states['u'][1:]),\n",
    "                                         make_tensor(self.KF_states['C'][1:]),\n",
    "                                         make_tensor(self.KF_states['g'][1:]),\n",
    "                                         make_tensor(self.KF_states['sigma'][1:]),\n",
    "                                         make_tensor(self.KF_states['l_a_posteriori'][:-1]),\n",
    "                                         make_tensor(self.KF_states['P_a_posteriori'][1:])),\n",
    "                                initializer = (mu_1, Sigma_1))\n",
    "            \n",
    "            self.mu = tf.concat([tf.expand_dims(mu_1,0), mu], 0)\n",
    "            self.Sigma = tf.concat([tf.expand_dims(Sigma_1,0),Sigma], 0)\n",
    "            print(self.mu.shape)\n",
    "            print(self.Sigma.shape)\n",
    "            \n",
    "            z_distribution = tfd.MultivariateNormalDiag(loc = self.mu,\n",
    "                                                        scale_diag = tf.linalg.diag_part(self.Sigma))\n",
    "\n",
    "            z_probability = z_distribution.prob(self.z)\n",
    "            self.z_probability = tf.boolean_mask(z_probability, self.z_mask)\n",
    "            regularizers = tf.nn.l2_loss(self.W_g) + tf.nn.l2_loss(self.W_mu_0) + \\\n",
    "                        tf.nn.l2_loss(self.W_sigma) + tf.nn.l2_loss(self.W_Sigma_0) + \\\n",
    "                        tf.nn.l2_loss(self.W_A) + tf.nn.l2_loss(self.W_B)\n",
    "\n",
    "            self.loss = tf.reduce_mean(self.beta*regularizers)-tf.square(tf.reciprocal(tf.reduce_sum(tf.cast(self.z_mask, tf.float32))))*tf.reduce_sum(tf.log(self.z_probability+1e-8))\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "#             self.optimizer = tf.train.AdamOptimizer(decayed_learning_rate)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            capped_grads_and_vars = [(tf.clip_by_norm(grad, 1.), var) for grad, var in grads_and_vars]\n",
    "            self.train_op = self.optimizer.apply_gradients(capped_grads_and_vars)\n",
    "        return self\n",
    "            \n",
    "        return\n",
    "    def new_train(self, epochs):\n",
    "        '''In this method, we require each epoch as 1 training sample\n",
    "            We will generate single steps at a time for input to the LSTM,\n",
    "            and then single step calculations of the optimal control\n",
    "        '''\n",
    "        \n",
    "        #Delete this section\n",
    "        Q = np.eye(4)*[10,1,1,1]\n",
    "        R = 1\n",
    "        \n",
    "        \n",
    "\n",
    "        self.rewards = [0 for _ in range(epochs)]\n",
    "#         self.environment_states = [[] for _ in range(epochs)]\n",
    "#         self.KF_states = [[] for _ in range(epochs)]\n",
    "            # required for likelihood/loss: A,B,u,C,l_a_posteriori,P_a_posteriorig,sigma\n",
    "        \n",
    "        \n",
    "        for i in range(epochs):\n",
    "            start = time.time()\n",
    "            self.environment_states=[]\n",
    "            self.KF_states = {'A':[],'B':[],'u':[],'C':[],'l_a_posteriori':[],'P_a_posteriori':[],\n",
    "                             'g':[],'sigma':[],'z':[]}\n",
    "            \n",
    "            done = False\n",
    "            observation = self.env.reset()\n",
    "            self.environment_states.append(tf.convert_to_tensor(observation,dtype=tf.float32))\n",
    "            preds = []\n",
    "            epoch_loss = []\n",
    "            initial_state = self.cell.zero_state(batch_size=1,dtype=tf.float32)\n",
    "            initial_input = tf.concat((self.env_params,np.zeros(shape=[1,4])),axis=1)\n",
    "\n",
    "            with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "                output_single,state_single = self.cell(inputs = initial_input, state = initial_state)\n",
    "                self.new_affine(output_single, first=True)\n",
    "            \n",
    "            u = tf.zeros(shape = [1,self.r], dtype=tf.float32)\n",
    "            l_a_posteriori = self.l_0\n",
    "            P_a_posteriori = self.P_0\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            while not done:\n",
    "                #Try parameterizing u by NN, include it in new_affine\n",
    "                \n",
    "                A, B, g, sigma = self.new_affine(output_single)\n",
    "#                 A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z = \\\n",
    "#                         forward_filter_fn(A,B,u,self.C,l_a_posteriori,\n",
    "#                                           P_a_posteriori,g,sigma,self.environment_states[-1])\n",
    "                dict_append(self.KF_states, forward_filter_fn(A,B,u,self.C,l_a_posteriori,\n",
    "                                                         P_a_posteriori,g,sigma,\n",
    "                                                         self.environment_states[-1]),\n",
    "                           self.vals)\n",
    "#                 self.KF_states.append((A,B,u,C,l_a_posteriori,P_a_posteriori,g,sigma,z))\n",
    "                \n",
    "                \n",
    "                #Delete this section\n",
    "                K,S,E = ct.lqr(self.KF_states['A'][-1].eval(),self.KF_states['B'][-1].eval(),Q,R)\n",
    "                u = -tf.matmul(K.astype(np.float32),tf.expand_dims(self.environment_states[-1],-1))\n",
    "                #u needs to be chosen in a better fashion\n",
    "                \n",
    "                observation,reward,done,info = self.env.step(np.squeeze(u.eval()))\n",
    "                self.environment_states.append(tf.convert_to_tensor(observation,dtype=tf.float32))\n",
    "                self.rewards[i] += 1\n",
    "                \n",
    "                '''Calculate filtered distributions with KF\n",
    "                    Approximate the control\n",
    "                    Call the next step with calculated control\n",
    "                    update lstm input with output of the observation\n",
    "                '''\n",
    "                next_input = tf.concat((self.env_params,tf.expand_dims(self.environment_states[-1],0)),axis=1)\n",
    "                output_single,state_single = self.cell(inputs=next_input,state=state_single)\n",
    "                \n",
    "#             for key,value in self.KF_states.items():\n",
    "#                 print(key)\n",
    "#                 for item in value:\n",
    "#                     print(item.shape)\n",
    "            self.calculate_new_loss()\n",
    "            print('Loss: {}'.format(self.loss))\n",
    "            print(\"Epoch ended after {} minutes, with {} rewards\\nTime per reward: {}\".\n",
    "                  format((time.time()-start)/60,self.rewards[i],(time.time()-start)/(60*self.rewards[i])))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 4, 1)\n",
      "(29, 4, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: (29,) and (29, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fc689bb86947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_new_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-ca14236dd439>\u001b[0m in \u001b[0;36mnew_train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;31m#                 for item in value:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;31m#                     print(item.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_new_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             print(\"Epoch ended after {} minutes, with {} rewards\\nTime per reward: {}\".\n",
      "\u001b[0;32m<ipython-input-46-ca14236dd439>\u001b[0m in \u001b[0;36mcalculate_new_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             z_distribution = tfd.MultivariateNormalDiag(loc = self.mu,\n\u001b[0;32m--> 487\u001b[0;31m                                                         scale_diag = tf.linalg.diag_part(self.Sigma))\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mz_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/mvn_diag.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale_diag, scale_identity_multiplier, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/mvn_linear_operator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m    180\u001b[0m           loc, name=\"loc\", dtype=scale.dtype)\n\u001b[1;32m    181\u001b[0m       batch_shape, event_shape = distribution_util.shapes_from_loc_and_scale(\n\u001b[0;32m--> 182\u001b[0;31m           loc, scale)\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     super(MultivariateNormalLinearOperator, self).__init__(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/internal/distribution_util.py\u001b[0m in \u001b[0;36mshapes_from_loc_and_scale\u001b[0;34m(loc, scale, name)\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;31m# This is defined in the core util module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m       \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefer_static_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_batch_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m       \u001b[0;31m# pylint: enable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/internal/distribution_util.py\u001b[0m in \u001b[0;36mprefer_static_broadcast_shape\u001b[0;34m(shape1, shape2, name)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0mshape2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tensor_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape1_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshape2_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape2_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0mshape1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_shape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mbroadcast_static_shape\u001b[0;34m(shape_x, shape_y)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbroadcasted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcommon_shapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mbroadcast_shape\u001b[0;34m(shape_x, shape_y)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreturn_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     raise ValueError(\"Incompatible shapes for broadcasting: %s and %s\"\n\u001b[0;32m--> 596\u001b[0;31m                      % (shape_x, shape_y))\n\u001b[0m\u001b[1;32m    597\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: (29,) and (29, 4)"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    testing = LSTM_SSM_model(sess, name = 'deleteme')\n",
    "    testing.build_new_LSTM()\n",
    "    testing.sess.run(tf.global_variables_initializer())\n",
    "    testing.new_train(5)\n",
    "    rewards=testing.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4VPXZ//H3N/tKErKRlSTse8iCrHXfN1xYqigo1KdUW9v61Fr7s5vdn7Za26pVsKKgEkBFcd8FUcgk7DuEwGSBJJCNhOzf3x8ZLKWBzISZOTNn7td1cWWWk5zbY+bOmTPn/hyltUYIIYT38zO6ACGEEM4hDV0IIUxCGroQQpiENHQhhDAJaehCCGES0tCFEMIkpKELIYRJ2NXQlVLRSqmVSqndSqldSqlJSqn+SqkPlFL7bF9jXF2sEEKIs7N3D/2vwLta6+HAOGAX8BDwkdZ6CPCR7b4QQgiDqN4mRZVS/YAtQJY+bWGl1B7gIq11pVIqCfhUaz3sXD8rLi5OZ2RknH/VQgjhQ4qKimq01vG9LRdgx8/KAqqBfymlxgFFwP1Aota6EsDW1BN6+mal1D3APQDp6elYLBY7/xOEEEIAKKUO2bOcPYdcAoAc4Cmt9XigCQcOr2itn9Fa52mt8+Lje/0DI4QQoo/saehlQJnWeoPt/kq6G/xR26EWbF+rXFOiEEIIe/Ta0LXWRwCrUurU8fFLgZ3AG8Bc22NzgdUuqVAIIYRd7DmGDvBdYJlSKggoAe6i+49BgVJqPnAYmOGaEoUQQtjDroautd4M5PXw1KXOLUcIIURfyaSoEEKYhDR0IYQwCWnowudZjzfz1tZKo8sQ4rxJQxc+79E1O7n3pWL2Hm00uhQhzos0dOHTqhtb+Xh39wjFc+sOGlyNEOdHGrrwaa9tKqOjSzN5UCyvbiqn5kSr0SUJ0WfS0IXP0lpTYCljfHo0j04fTVtHF0u/sisyQwiPJA1d+Kziw3XsrzrBrLw0BsVHcOnwBF788hAt7Z1GlyZEn0hDFz5rhcVKaKA/145NAmD+tEyONbWxenO5wZUJ0TfS0IVPamrt4M0tFVw7NonIkEAAJmXFMjKpH4vWHqS36wQI4YmkoQuf9Na2SpraOpmVn/b1Y0opFkzLZF/VCT7fV2NgdUL0jTR04ZNWWKxkxYWTN/A/L4V73dhkEiKDWbS2xKDKhOg7aejC5xyoPkFhaS0z8tJQSv3Hc0EBfsydnMHafTXsPtJgUIVC9I00dOFzVljK8PdT3JKT0uPzt1+QTmigP4vXyqCR8C7S0IVP6ejsYlVxGRcPiyehX0iPy0SHBTEjL5XVmyuoamxxc4VC9J00dOFTPt1TTXVjKzPy0s653F1TMmnv6mLplzJoJLyHNHThU5ZbrMRFBHHJ8IRzLpcZF85lIxJ58SsZNBLeQxq68BlVjS18vLuKm3NSCfTv/Vd/wdRMapvbebVYBo2Ed5CGLnzGa8XldHZpZual2rX8hMz+jEmJYvG6Erq6ZNBIeD5p6MIndAdxWclJj2ZwQqRd33Nq0OhAdROf7a12cYVCnD9p6MInFB+u5UB1039MhtrjmjFJDOgXwqJ1MmgkPJ80dOETCgrLCAvy59qxyQ59X6C/H/OmZPDF/mPsqKh3UXVCOIc0dGF6Ta0drNlawbVjkogIDnD4+785IZ2wIH8WyxWNhIeThi5Mr6cgLkdEhQYyMy+NN7dUcLRBBo2E55KGLkyvoNBKVnw4uWcEcTni7imZdHRpXviy1Gl1CeFs0tCFqe2vOoHlUC0zewjickR6bBhXjhzAsg2HaW7rcGKFQjiPNHRhaiuKrPj7KW4+SxCXIxZMy6SuuZ1VMmgkPJQ0dGFa7Z1drCoq5+JhCSRE9hzE5YjcgTGMS4vmuXUHZdBIeCRp6MK0Pt1TTc2JVrsnQ3ujlGLB1EwO1jTx8e4qp/xMIZxJGrowreWFVuIigrm4lyAuR1w9egAp0aEyaCQ8kjR0YUpVjS18sqeKW3JS7ArisleAvx/zJmfwVclxtpfLoJHwLNLQhSm9agvi6i33vC9mTUgjIjhArjsqPI40dGE6p4K4cgfGMDghwuk/v19IILPy01iztZLK+pNO//lC9JU0dGE6RYdqKaluYpYL9s5PmTc5gy6tWbJermgkPIc0dGE6BRarLYgryWXrSOsfxtWjk3hpwyGaWmXQSHgGaejCVE60drBmayXXjU0ivA9BXI6YPy2ThpYOVhaVuXQ9QtjLroaulCpVSm1TSm1WSllsj/VXSn2glNpn+9r3oAwhnOTtrZU0n0cQlyNy0mPISY/muS8O0imDRsIDOLKHfrHWOltrnWe7/xDwkdZ6CPCR7b4Qhlpu6Q7iykl3z/7FgmlZHDrWzIe7jrplfUKcy/kccrkRWGK7vQSYfv7lCNF3+6saKTpUy6zzDOJyxBUjE0mNCWXxWslKF8azt6Fr4H2lVJFS6h7bY4la60oA29cex/GUUvcopSxKKUt1tVyXUbjOCksZ/n6Km5wQxGWvAH8/7pqSycbS42yx1rltvUL0xN6GPkVrnQNcDdyrlPqGvSvQWj+jtc7TWufFx8f3qUghetPe2cWq4jIuGe6cIC5HzMxLJTI4QK5oJAxnV0PXWlfYvlYBrwETgKNKqSQA21dJKxKG+WR3FTUn2pjpwnPPzyYyJJDZE9J4a1sl5XUyaCSM02tDV0qFK6UiT90GrgC2A28Ac22LzQVWu6pIIXpTYLESHxnMxcOMeRc4b0omAEvWlxqyfiHAvj30RGCdUmoLsBF4S2v9LvB74HKl1D7gctt9IdyuqqGFT/ZUc3NOCgFODOJyREp0KNeMSeLlDYc5IYNGwiC9Tl5orUuAcT08fgy41BVFCeGIVbYgLiMOt5xu/tRM3txSQUGhlbunZhpai/BNMikqvJrWmhUWK/kZMQyKd34QlyOy06LJz4iRQSNhGGnowqtZDtVSUtPkkpjcvpg/NYuy2pO8v+OI0aUIHyQNXXi1gkIr4UH+XDvGdUFcjrh8ZCLp/cNYJKcwCgNIQxde60RrB29tq+S6sckuD+Kyl7+f4u4pGRQdqqX4cK3R5QgfIw1deK23tlbQ3NbJTDcEcTliRl4akSEyaCTcTxq68FrLC60Mig8nJz3a6FL+Q3hwALddkM472yqxHm82uhzhQ6ShC6+0v6qR4sN1zMp3XxCXI+ZNzsBPKZ6XQSPhRtLQhVcqsJQR4Ke4aXyq0aX0KCkqlOvGJrG80EpDS7vR5QgfIQ1deJ32zi5etQVxxUcGG13OWc2fmsWJ1g4KCq1GlyJ8hDR04XU+NjCIyxFjUqO4ILM///qilI7OLqPLET5AGrrwOgWF3UFcFxkUxOWIBdOyKK87ybsyaCTcQBq68CpHG1r4ZE8Vt+SkGhbE5YhLhyeQERvGs2sPorXEAQjX8vxXhBCnWVVcRpfuvqiEN/DzU8yfmskWa50MGgmXk4YuvEZ3EFcZEzL6k2VwEJcjbslNJSo0kEVy3VHhYtLQhdcoLK3lYE0TM7xk7/yUsKAAbr8gnfd2HOHwMRk0Eq4jDV14jQKLLYhrrGcEcTli7uQM/P0U/1ove+nCdaShC6/Q2NLOW1sruX5cMmFBnhHE5YjEfiFcPy6ZgkIr9Sdl0Ei4hjR04RXe2lrJyXbPC+JyxPypmTS1dfLKxsNGlyJMShq68ArLLVYGJ0QwPs2zgrgcMSo5ismDYnl+fSntMmgkXEAauvB4+442sulwHbPyPDOIyxELpmVSWd/C29sqjS5FmJA0dOHxCizW7iCunBSjSzlvFw1NICs+nMXrZNBIOJ80dOHR2jq6eLW4nEtHJBAX4blBXPY6NWi0tayewlIZNBLOJQ1deLSPd1dxrMnzg7gccfP4VGLCAlm0tsToUoTJSEMXHq3AYiUhMpgLh3p+EJe9QoP8mTNxIB/sOkppTZPR5QgTkYYuPNbRhhY+3VPFrbneEcTliDsmDSTQz49/fSGDRsJ5zPUqEaaysqg7iGuGiQ63nJIQGcIN2ckUWMqob5ZBI+Ec0tCFR+oO4rIyIbM/mXHhRpfjEgumZXKyvZNlGw8ZXYowCWnowiNtPHic0mPNpvow9EzDB/Rj2pA4lqwvpa1DBo3E+ZOGLjxSgaWMiOAArhkzwOhSXGr+1EyONrTy1rYKo0sRJiANXXicxpZ23t5WyfXjkrwyiMsRFw6NZ0hCBIvkikamZT3ezLdfLKK6sdXl65KGLjzOmlNBXCY+3HKKUt2DRjsqGviq5LjR5Qgna2nvZOGyItYfqKGlvdPl65OGLjzO8kIrQxIiyPbiIC5HTB+fQmx4EIvXyaCR2fzyzR1sL2/gsVnZpPUPc/n6pKELj7L3aCObrXXMyvf+IC57hQR2Dxp9uKuKkuoTRpcjnGRlURkvb7TynYsGcemIRLesUxq68CgFhd1BXNPHe38QlyPmTBxIUIAfz8mgkSnsqmzgp69tY1JWLD+8fKjb1isNXXiMto4uXt1UzmUjEk0RxOWI+MhgbspOYWVRGbVNbUaXI85DQ0s7C5cWERUayBPfHO/WKWe716SU8ldKbVJKrbHdz1RKbVBK7VNKLVdKBbmuTOELPt59lONNbczM966LQDvL/GmZtLR38ZJc0chraa15cMVWrLUn+cftOcRHunfHxJE/HfcDu067/wfgMa31EKAWmO/MwoTvWV5oJbFfMN8YYp4gLkcMTYzkwqHxPL++lNYO158RIZxv0dqDvLvjCD+5ejj5Gf3dvn67GrpSKhW4Flhku6+AS4CVtkWWANNdUaDwDUfqW/hsb7Upg7gcsWBaJtWNrby5Ra5o5G02lBzj9+/u5urRA5g/NdOQGux95TwOPAicmk+OBeq01h22+2WAb32KJZxqVbEtiCvX/Oeen8vUwXEMS4xk0doSGTTyIlWNLdz38ibS+4fxx1vHGnaGVq8NXSl1HVCltS46/eEeFu3xt08pdY9SyqKUslRXV/exTGFmWmsKLFYuyOxPhkmDuOyllGL+tEx2H2lk/YFjRpcj7NDR2cV3X9pEY0s7T83JITIk0LBa7NlDnwLcoJQqBV6h+1DL40C0UurUXHYq0GMYhdb6Ga11ntY6Lz7eN4+NinPbcPA4h0wexOWIG7OTiYsIlisaeYk/vb+XDQeP89ubxjB8QD9Da+m1oWutf6K1TtVaZwCzgY+11rcDnwC32habC6x2WZXC1AosVlsQV5LRpXiE4AB/7pw0kE/2VLO/qtHocsQ5fLDzKE9/doDbLkjn5hzjz846n0+ffgz8UCm1n+5j6oudU5LwJQ1fB3ElExrkb3Q5HuP2C9IJDvBj8bpSo0sRZ3HoWBM/LNjMmJQofnbdSKPLARxs6FrrT7XW19lul2itJ2itB2utZ2itXR8lJkxnzZZKWtq7mJUvh1tOFxsRzM05qbxaXMaxE/LS8jQt7Z0sXFqMn1I8eXsOIYGesTPiu+eHCY+w3GJlaGIE41KjjC7F48yfmklrRxfLNsigkaf5+eod7Kxs4LFZ49wSumUvaejCMHuONLLFWsfMPN8J4nLE4IQILhmewAtflrolelXYp8BiZbnFyn0XD+aS4e4J3bKXNHRhmAKLlUB/xU0+FsTliAVTM6k50cYbm+WKRp5gR0U9j7y+nSmDY/mBG0O37CUNXRiiraOL12xBXLE+FsTliEmDYhmR1I9F62TQyGj1J9v5zrJiYsKC+Ovs8fj7ed67SmnowhAf7ToVxCUfhp6LUooFUzPZe/QEa/fVGF2Oz9Ja878rtlBee5J/3D7eY9NApaELQyy3WBnQL8Rng7gccf24ZBIig1m0TrLSjfLPz0v4YOdRfnLNCHIHuj90y17S0IXbVdaf5HNbEJcnvm31NEEBfsydnMHne6vZc0QGjdztq5Jj/PHd3Vw7Jom7p2QYXc45SUMXbreqyBbElWf8ZJ23uG1COiGBfjwne+luVdXQwn0vbSIjLpw/GBi6ZS9p6MKturo0BZYyJmb1Z2CsbwdxOSImPIhbc1N5bXM51Y0yaOQOHZ1d3PfyJppaO3h6Ti4RwQG9f5PBpKELt9pw8DiHj0sQV1/cPSWTto4uln51yOhSfML/vbeHjQeP87ubxzA0MdLocuwiDV241QqLlcjgAK4eLUFcjsqKj+CyEQks/eqQDBq52Hs7jvDPz0uYMzHdqy5YLg1duE1DSztvb6/k+mwJ4uqrBdOyONbUxuubyo0uxbRKa5r434ItjEuN4hEPCd2ylzR04TZvbqnoDuKSwy19dkFmf0an9GPRuoN0dcmgkbO1tHeycFkx/v6Kf9yeQ3CAd+14SEMXblNQaGVYYiRjJYirz7oHjbLYX3WCz/bJFcCc7ZHXt7P7SAOPzcomNcZzQrfsJQ1duMXuIw1sKatnZr4EcZ2va8YkMaBfCIvXyimMzrS88DArisr47sWDuXhYgtHl9Ik0dOEWBYVlEsTlJKcGjdbtr2FXZYPR5ZjC9vJ6Hlm9g2lD4rj/Ms8L3bKXNHThcq0dnby2qYzLRybSPzzI6HJM4bYJ6YQG+rNYBo3OW31zOwuXFREbHsTjs7K9enpZGrpwuY92VVHb3C7nnjtRVFggM/NSWb25nKqGFqPL8VpdXZoHVmymsq6Fv9+W4/XJn9LQhcstL7SSFBXCNAnicqq7pmTS0aV5UQaN+uzpzw/w4a4q/t+1I8gdGGN0OedNGrpwqYq6k3y+T4K4XCEjLpzLRySy9KtDnGyTQSNHrT9Qw5/e28P145KZOznD6HKcQhq6cKlVRWVoDTNy5XCLKyyYlkVtczuvbiozuhSvcrShhe+9vInMuHB+f/MY05x55fENXWvNU58e4Hdv7zK6FOGgri7NiqIyJmXFkh7rfef0eoP8jBjGpUaxWAaN7Nbe2cV9LxXT3NbJ03NyCfeC0C17eXxDV0pxpP4k//y8hJc3ytXPvclXB491B3HlS0yuqyilmD8ti5LqJj7ZU2V0OV7hj+/uprC0lt/dPIYhXhK6ZS+Pb+gAj1w3kguHxvPI69v5Yr9chstbrLCUERkiQVyudvXoASRHhbBIBo169e72Sp5de5A7Jw3kxmzzzUR4RUMP8Pfjb7eNJys+nIVLi9hfdcLokkQv6k+28/a2Sm4Yl0xIoHflYXibQH8/5k3J4MuSY2wvrze6HI91sKaJH63Yyri0aH567Qijy3EJr2joAP1CAlk8N5+gAD/mLynkeFOb0SWJc3hzSwWtHV3MkotAu8Ws/HTCg/zlikZncbKtk4VLiwjwVzzphaFb9vKahg6Q1j+Mf96RR2V9C99+sYjWDjlVy1MVWKwMHxDJmBQJ4nKHqNBAZuan8caWCo7Uy6DR6bTW/PT1bew52sjjs8eTEh1qdEku41UNHSB3YAx/mjGOjaXHefjV7Wgtn+x7ml2VDWwtq2dmngRxudNdkzPp0poXviw1uhSP8vJGK68Wl/O9S4Zw4VBzD7d5XUMHuGFcMt+/bAirist48tMDRpcjzlBgsRLk7ydBXG6WHhvGlaMGsGzDYZrbOowuxyNsK6vnF290h25979IhRpfjcl7Z0AHuv3QIN2Yn83/v7eHtbZVGlyNsuoO4yrl8ZCIxEsTldgumZVJ/sp1VRTJoVNfcxsJlRcRFBPHX2eN9YlLZaxu6Uoo/3DKW3IEx/GD5ZrZY64wuSQAf7qyirrmdmfJhqCFyB/ZnfHq0zw8adXVpfliwhaMNLTw5J9dnUj69tqEDhAT68887comPDGbBCxbK604aXZLPW26xkhwVwtTBcUaX4rMWTM2i9FgzH+323UGjpz47wMe7q3jkupFkp0UbXY7beHVDB4iLCOa5efm0tHUy//lCTrTKsUOjVNSdZK0EcRnuylGJpESH8uzaEqNLMcQX+2v48/t7uGFcMndMHGh0OW7l9Q0dYGhiJH+/PYd9VSe4/+VNdPrwW00jrbQFcd0qQVyGCvD3464pGWw8eJytZb51KPJIfXfoVlZ8BL8zUeiWvUzR0AEuHBrPL64fyUe7q/itBHm5XXcQl5XJgySIyxPMyk8jIjjAp65odCp062R7J0/PyTFV6Ja9TNPQAe6YlMG8yRksXneQZRsk9N+dvio5hvX4SbkqkYeIDAlkdn4ab22tpMJHPlv6/Tu7sRyq5Q+3jGVwgrlCt+zVa0NXSoUopTYqpbYopXYopX5pezxTKbVBKbVPKbVcKeURHyM/ct1ILh4Wz89W72Dtvmqjy/EZBRYrkSEBXDV6gNGlCJt5UzLo0polX5YaXYrLvb2tksXrDjJvcgbXj0s2uhzD2LOH3gpcorUeB2QDVymlJgJ/AB7TWg8BaoH5rivTfv5+ir/dlsOQhAi+s6yY/VWNRpdkevUn23ln+xFuzJYgLk+SGhPG1WOSeGnDYZpMfLJASfUJHly5lfHp0Tx8jTlDt+zVa0PX3U7FGwba/mngEmCl7fElwHSXVNgHEcEBLJqbR3CAP3c9X8ixE61Gl2Rqb5wK4spLN7oUcYYFUzNpbOlghcVqdCku0dzWwcKlxQQF+PGP23IICjDVUWSH2fVfr5TyV0ptBqqAD4ADQJ3W+tSf/TLAo+a8U2PCePbOXKoaWvn2UgnycqWCwu4grtEp/YwuRZxhfHoMuQNjeO6LUtOd/aW15qevbWdvVSN/nZ1NsolDt+xlV0PXWndqrbOBVGAC0NP7mh5/W5RS9yilLEopS3W1e49pj0+P4c8zx1FYWstDq7ZJkJcL7KxoYFt5PbPyJYjLU31rWiaHjzfzwc6jRpfiVMs2HOa1TeV8/9KhTBti7tAtezn0/kRrXQd8CkwEopVSp84LSgUqzvI9z2it87TWefHx7t/o141N5oHLh/LapnL+/vF+t6/f7E4FcU034dVfzOLykQNI6x/K4nXmGTTaWlbHr97cyUXD4vnuJYONLsdj2HOWS7xSKtp2OxS4DNgFfALcaltsLrDaVUWer/suGcxN41P48wd7WbO1x787og9aOzp5fXM5l4+SIC5P5u+nuHtKJoWltWw2QeZRbVMbC5cWEx8ZzGMzs/GTqeSv2bOHngR8opTaChQCH2it1wA/Bn6olNoPxAKLXVfm+VFK8ftbxpA3MIYHCraw6XCt0SWZwgc7j1LX3M4sOffc483ISyMyJIBFXh4H0NWl+UHBZqobW3ny9hzZkTiDPWe5bNVaj9daj9Vaj9Za/8r2eInWeoLWerDWeobW2qNPJQkO6A7ySuwXwrdeKKKsttnokrze8sLuIK4pEsTl8SKCA7htQjrvbD/i1b/7//hkP5/uqeaR60cyzodCt+zlU+f4xEYE89y8PFo7OlmwxEJjS7vRJXmt8rqTrNtfw615aRLE5SXmTs4AYMn6UkPr6Kt1+2r4y4d7mZ6dzJwL5BTZnvhUQwcYnBDJk7Ygr++9vImOzi6jS/JKKy3dQVwzclONLkXYKTk6lGvHJPHKRqvX7cxU1p/ke69sYkhCBL/1wdAte/lcQweYNiSeX94wik/2VPPrtyTIy1GngrimDI4lrb8EcXmTBdMyaWztoMDiPVc0auvo4t5lxbS2d/LUnFzCgnwvdMtePtnQAeZMHMjdUzJ5fn0pL35ZanQ5XuXLkmOU1UoQlzcamxrNhIz+/OuLg17z7vR37+yi+HAdf7x1HIPiI4wux6P5bEMH+Om1I7h0eAK/eHMnn+2VIC97FVis9AsJ4MpREsTljeZPy6Ss9iTve8Gg0ZqtFfzri1LumpLBtWOTjC7H4/l0Q/f3U/z1m+MZkhDBfcuK2XtUgrx6U998KogrRYK4vNRlIxLJiA3z+FMY91ed4Mcrt5KTHs1Prvbt0C17+XRDh+7TuZ6bl09IkD93P19IjQR5ndMbW8pp6+hillwE2mv5+ynunppJ8eE6ig555kxGU2sHC5cWERzozz9ul9Ate8lWovvT/2fvzKO6sZV7XrDQ0i5BXmez3GJlZFI/RqdEGV2KOA+35qYSFRrokXEAWmsefm0b+6tP8MTs8SRFSeiWvaSh22SnRfPYrGyKD9fx41VbJcirBzsq6tle3sDMPDlV0duFBQVw2wXpvLv9CNbjnjVotPSrQ6zeXMEDlw9l6hAZWnOENPTTXDMmiR9dOYzVmyt44iMJ8jrTCktZdxDXeAniMoO5kzLwU4p/fVFqdClf22yt41drdnLJ8AS+c5GEbjlKGvoZvnPRIG7JSeWxD/fyxhYJ8jqlpb2T1zaVc8WoRKLDJD/DDAZEhXD9uGSWFx6mwQMGjWqb2rh3WTGJ/UL4y8xxErrVB9LQz6CU4rc3j2ZCRn/+d8UWj/3QyN0+2HmU+pPt8mGoycyfmklTWyfLNxp7RaOuLs33l/87dEt2GvpGGnoPggP8efqOXJKiQrjnBYvHHWM0QoHFSkp0KFMGyTFNMxmdEsXELOMHjf728X4+21vNz28YydhUCd3qK2noZ9E/PIjFc/Np7+xi/pJCj3hLapSy2ubuIK7cVHkbbEILpmZRUd/CO9uPGLL+z/dW8/hHe7l5fAq3TZDQrfMhDf0cBidE8NScXEqqm/juS74b5LWyqDv341YJ4jKlS4YnkBUXzqK1JW4/u6ui7iT3v7KJoQmR/OYmCd06X9LQezFlcByPTh/NZ3ureXTNTqPLcbuuLs0KSxlTBsVJEJdJ+dkGjbaU1bv1M6O2ji6+s6yY9k7NU3NyCA2SyePzJQ3dDt+ckM63pmWy5MtDXpsl3VfrDxyjvO4kM+Tcc1O7JSeV6LBAFq096LZ1/vbtXWy21vHHW8eSJaFbTiEN3U4PXT2Cy0Yk8ss3d/DJniqjy3EbCeLyDaFB/sy5YCDv7TzCoWNNLl/fG1sqeH59KfOnZnLNGAndchZp6Hby91P8dXY2wwf047svbWLPEfMHedU3t/PujiNMHy9BXL7gzkkDCfBz/aDRvqONPLRqK3kDY3jo6uEuXZevkYbugPDgABbPyyPMFuRV3WjuIK/VtiAuyT33DQn9QrhhXAoFFiv1za45q6uptYOFy4oJC/Ln77flEOgvLciZZGs6KCkqlMVz8znW1Mo9L5o7yGt5oZVRyRLE5UvmT82kua2TlwsPO/1na6156NVtlNhCtwZEhTh9Hb5OGnofjEmN4vFZ2Ww6XMePVpozyGt7eT24uOmqAAAORElEQVQ7Khpk79zHjEzux5TBsTz/RSntTj5N94UvD/HmlgoeuGIYkwfLgJorSEPvo6tGJ/HgVcN4c0sFj324z+hynG6FxUpQgB83ZicbXYpwswVTszjS0MLb2yqd9jOLD9fy67d2cunwBBZeOMhpP1f8J2no52HhhYOYkZvKEx/t4/VN5UaX4zQt7Z28vrmCK0cNkEwNH3Th0HgGxYfzrJMGjY43tXHfsmIGRIXwl5nZMm3sQtLQz4NSit/cNIYLMvvz4MqtWEqPG12SU7x/KohLDrf4JD8/xYJpWWwvb2DjwfP7ne7s0tz/yiZqmtp46vZcosICnVSl6Ik09PMUFODH03NySYkJ5X9eLDJFkFdBYXcQ1+RBsUaXIgxy0/gU+ocHsWjd+Q0aPfHRPtbuq+GXN4ySD9fdQBq6E8SEB7F4bh4dXZq7n/fuIC/r8Wa+OFDDjDwJ4vJlIYH+zJk4kA93HeVgTd8GjT7dU8UTH+/jlpxUZkvssltIQ3eSrPgInpqTw8GaJu5dVuy1QV4SxCVOuWPiQAL9/HiuD3vpZbXNfH/5ZoYlRvLr6aMldMtNpKE70eRBcfzmptGs3VfDL97c4XWnM3Z1aVYWlTF1cBypMRLE5eviI4OZPj6ZFUVW6prb7P6+1o5O7l1WTGen5qk5uRK65UbS0J1sVn46//ONLJZ+dZjnvSzI64sDNbYgLnl7LLrNn5pFS3sXyzbYP2j06zW72FJWz//NGEtmXLgLqxNnkobuAj++ajhXjEzk0TU7+Xj3UaPLsVuBpYyo0ECuGJlodCnCQwwbEMm0IXEsWV9KW0fvhxFXby7nxa8O8a1pmVw1WkK33E0augv4+Sken53NiKTuIK9dlQ1Gl9SruuY23ttxhJskiEucYcG0LKoaW1mz9dwXTd97tJGHVm0jPyOGB6+S0C0jSEN3kbCgABbPzSciJID5zxdS1dhidEnntHpzBW0dXZJ7Lv7LN4bEMSQhgkVrD571c6ETrR18e2kR4cEBErplINnqLjQgKoTFc/OpbW7nWy8UeXSQ1/JCK6NT+jEqWc4VFv9JKcWCaZnsrGzgy5Jj//W81pofr9pKaU0Tf/vmeBL7SeiWUaShu9jolCgen53N1rI6HijYQleX5535sr28np2VEsQlzu7G7BTiIoJY3MMVjZ5fX8pbWyv50ZXDmSTDaIaShu4GV44awENXDeetbZU89uFeo8v5LwWngrjGpRhdivBQIYH+3DExg492V3Gg+sTXjxcdquU3b+3ishGJfPvCLAMrFGBHQ1dKpSmlPlFK7VJK7VBK3W97vL9S6gOl1D7b1xjXl+u97vlGFrPy0vjbx/t5tbjM6HK+1tLeyeubyrlq1ADJ2RDnNGdiOkEB/x40OnailfteKiY5OpQ/zxwnw0MewJ499A7gAa31CGAicK9SaiTwEPCR1noI8JHtvjgLpRSPTh/NpKxYHlq1jUIPCfJ6b8cRGlo6mCWj2aIXsRHB3JKTwsqiMqobW7n/lc0ca2rjydtziAqVnQFP0GtD11pXaq2LbbcbgV1ACnAjsMS22BJguquKNIugAD+empNDakwo97xgccvFeHtTYLGSGhPKpCw59il6d/eUTFo7upj1zJes21/DozdK6JYncegYulIqAxgPbAAStdaV0N30gQRnF2dG0WFBLJ6Xjwbufr6Q+pPGBXlZjzfzxf5jzMhNkyAuYZchiZFcNCyekuomZuSmMis/3eiSxGnsbuhKqQhgFfB9rbXdkzJKqXuUUhallKW6urovNZpOZlw4T8/J5fDxZu5dVuz0S33Za0VRGUrBrXLuuXDAT68Zwd1TMnl0+mijSxFnsKuhK6UC6W7my7TWr9oePqqUSrI9nwRU9fS9WutntNZ5Wuu8+Ph4Z9RsChOzYvnNTWNYt7+Gn7/h/iCvzi7NSouVqYPjSIkOdeu6hXcbkhjJz64fKRPFHsies1wUsBjYpbX+y2lPvQHMtd2eC6x2fnnmNjMvjW9fOIiXNhxm8XleSMBRX+yvoaK+Rc49F8JEAuxYZgpwB7BNKbXZ9tjDwO+BAqXUfOAwMMM1JZrbg1cOo7Smid+8vYuM2HAuc1MwVoHFSnRYIFeMkiAuIczCnrNc1mmtldZ6rNY62/bvba31Ma31pVrrIbavnnEenpfx81M8Niub0clRfO+VTeyoqHf5Omub2nh/x1GmZ6cQHCBvm4UwC5kU9QChQf4smptHVGggC5ZYqGpwbZDX6s3ltHV2yeEWIUxGGrqHSOwXwqK5edSfbGfBCxZOtrkmyEtrzXJLGWNSohiZ3M8l6xBCGEMaugcZlRzFX2ePZ1t5PT8s2OySIK/t5Q3sqmxgppyqKITpSEP3MJePTOThq0fwzvYj/On9PU7/+QUWK8EBftyQLUFcQpiNPWe5CDdbMC2TkpoTPPnpATLjwp12jc+W9k5e31zOVaMHSPaGECYke+geSCnFr24czZTBsTz82jY29HBRgb54b8cRGls6mCUfhgphStLQPVSgvx9P3pZLWv8w/mdpEaU15x/ktbzQSlr/UCZKEJcQpiQN3YNFhQXy3Nx8wBbk1dz3IC/r8WbWH5AgLiHMTBq6h8uIC+efc3Kx1jazcFlRn4O8VlisKAW35MrZLUKYlTR0L3BBViy/v3ks6w8c45HXtzsc5NXZpVlZVMa0IfESxCWEiUlD9xK35KZy78WDeKXQyqIeLtR7LutsQVzyYagQ5ianLXqRBy4fxsGaJn77zi4GxoZxxagBdn1fgcVKTFggl42Ua5AIYWayh+5F/PwUf56RzdiUKO5/ZTPby3sP8qptauODHUeZPl6CuIQwO2noXiY0yJ9n78wjJqw7yOtI/bmDvF6XIC4hfIY0dC+U0C+ERXPzaWxpZ8ELhTS3dfS4nNaa5YVWxqZGMSJJgriEMDtp6F5qZHI/nvjmeHZUNPCD5T0HeW0rr2f3kUanRQcIITybNHQvdumIRH56zQje23GUP77330FeXwdxjUs2oDohhLvJWS5ebv7UTEpqmnj6swNkxYd/fay8pb2T1ZsruFqCuITwGdLQvZxSil/eMArr8WYefnUbaTFhTBoUy7vbu4O4ZubL4RYhfIUccjGBQH8//n5bDhlx4Xx7aREl1Sf+HcSVKUFcQvgKaegmERXaHeTlp+COxRv5suQYMyWISwifIg3dRNJjw3jmzjyqG1sliEsIHyTH0E0mP6M/z9yZi7X2JMkSxCWET5GGbkIXDZPMFiF8kRxyEUIIk5CGLoQQJiENXQghTEIauhBCmIQ0dCGEMAlp6EIIYRLS0IUQwiSkoQshhEkorf/7wgguW5lS1cChPn57HFDjxHKcRepyjNTlGKnLMWata6DWOr63hdza0M+HUsqitc4zuo4zSV2OkbocI3U5xtfrkkMuQghhEtLQhRDCJLypoT9jdAFnIXU5RupyjNTlGJ+uy2uOoQshhDg3b9pDF0IIcQ4e19CVUlcppfYopfYrpR7q4flgpdRy2/MblFIZHlLXPKVUtVJqs+3fAjfU9JxSqkoptf0szyul1BO2mrcqpXJcXZOddV2klKo/bVv9zE11pSmlPlFK7VJK7VBK3d/DMm7fZnbW5fZtppQKUUptVEptsdX1yx6Wcfvr0c663P56PG3d/kqpTUqpNT0859rtpbX2mH+AP3AAyAKCgC3AyDOW+Q7wtO32bGC5h9Q1D/i7m7fXN4AcYPtZnr8GeAdQwERgg4fUdRGwxoDfryQgx3Y7Etjbw/9Ht28zO+ty+zazbYMI2+1AYAMw8YxljHg92lOX21+Pp637h8BLPf3/cvX28rQ99AnAfq11ida6DXgFuPGMZW4ElthurwQuVUq5+krI9tTldlrrz4Hj51jkRuAF3e0rIFopleQBdRlCa12ptS623W4EdgEpZyzm9m1mZ11uZ9sGJ2x3A23/zvzQze2vRzvrMoRSKhW4Flh0lkVcur08raGnANbT7pfx37/YXy+jte4A6oFYD6gL4Bbb2/SVSqk0F9dkD3vrNsIk21vmd5RSo9y9cttb3fF0792dztBtdo66wIBtZjt8sBmoAj7QWp91e7nx9WhPXWDM6/Fx4EGg6yzPu3R7eVpD7+kv1Zl/ee1ZxtnsWeebQIbWeizwIf/+K2wkI7aVPYrpHmUeB/wNeN2dK1dKRQCrgO9rrRvOfLqHb3HLNuulLkO2mda6U2udDaQCE5RSo89YxJDtZUddbn89KqWuA6q01kXnWqyHx5y2vTytoZcBp/8lTQUqzraMUioAiML1b+97rUtrfUxr3Wq7+yyQ6+Ka7GHP9nQ7rXXDqbfMWuu3gUClVJw71q2UCqS7aS7TWr/awyKGbLPe6jJym9nWWQd8Clx1xlNGvB57rcug1+MU4AalVCndh2UvUUotPWMZl24vT2vohcAQpVSmUiqI7g8N3jhjmTeAubbbtwIfa9snDEbWdcZx1hvoPg5qtDeAO21nbkwE6rXWlUYXpZQacOq4oVJqAt2/h8fcsF4FLAZ2aa3/cpbF3L7N7KnLiG2mlIpXSkXbbocClwG7z1jM7a9He+oy4vWotf6J1jpVa51Bd4/4WGs954zFXLq9Apz1g5xBa92hlLoPeI/uM0ue01rvUEr9CrBord+g+xf/RaXUfrr/ss32kLq+p5S6Aeiw1TXP1XUppV6m++yHOKVUGfBzuj8gQmv9NPA23Wdt7AeagbtcXZOddd0KLFRKdQAngdlu+KMM3XtQdwDbbMdfAR4G0k+rzYhtZk9dRmyzJGCJUsqf7j8gBVrrNUa/Hu2sy+2vx7Nx5/aSSVEhhDAJTzvkIoQQoo+koQshhElIQxdCCJOQhi6EECYhDV0IIUxCGroQQpiENHQhhDAJaehCCGES/x/g80obgxuOeAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing using lists with initially creating tensor and then updating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A_assign_ = []\n",
    "A_access_ = []\n",
    "B_assign_ = []\n",
    "B_access_ = []\n",
    "\n",
    "for x in range(4):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        A = tf.Variable(tf.eye(10,batch_shape=[10**x]),dtype=tf.float32,trainable=False)\n",
    "        B = [[] for _ in range(5)]\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        A_start = time.time()\n",
    "        for _ in range(5):\n",
    "            for i in range(A.shape[0]):\n",
    "                A = tf.scatter_update(A,[i], tf.expand_dims(tf.math.scalar_mul(2,A[i,:,:]),0))\n",
    "        A_assign = time.time()-A_start\n",
    "\n",
    "        A_access_start = time.time()\n",
    "        for _ in range(5):\n",
    "            for i in range(A.shape[0]):\n",
    "                tf.matmul(A[i],tf.eye(10))\n",
    "        A_access = time.time()-A_access_start\n",
    "\n",
    "        B_start = time.time()\n",
    "        for i in range(5):\n",
    "            for j in range(A.shape[0]):\n",
    "                B[i].append(tf.expand_dims(tf.math.scalar_mul(2,A[j,:,:]),0))\n",
    "        B_assign = time.time()-B_start\n",
    "\n",
    "        B_access_start = time.time()\n",
    "        for i in range(5):\n",
    "            for j in range(len(B[i])):\n",
    "                tf.matmul(B[i][j],tf.expand_dims(tf.eye(10),0))\n",
    "        B_access = time.time()-B_access_start\n",
    "\n",
    "        A_assign_.append(A_assign)\n",
    "        A_access_.append(A_access)\n",
    "        B_assign_.append(B_assign)\n",
    "        B_access_.append(B_access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(A_assign_, color='r')\n",
    "plt.plot(B_assign_, color='k')\n",
    "plt.show()\n",
    "plt.plot(A_access_, color='r')\n",
    "plt.plot(B_access_, color = 'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    example = LSTM_SSM_model(sess, name = 'deleteme', batch_size = 3)\n",
    "#     tvars = tf.trainable_variables()\n",
    "#     tvars_vals = example.sess.run(tvars)\n",
    "#     for var, val in zip(tvars, tvars_vals):\n",
    "#         print(var.name, val.shape)\n",
    "    example.build_LSTM().affine_transformations()\n",
    "    example.sess.run(tf.global_variables_initializer())\n",
    "#     A, B = example.sample_from_env()\n",
    "    example.sample_from_env()\n",
    "    example.build_model().build_loss()\n",
    "    example.sess.run(tf.global_variables_initializer())\n",
    "    loss, rewards = example.train(epochs = 500)\n",
    "#     feed_dict = {example.lstm_input:example.lstm_inputs}\n",
    "#     z_probs,z_mask = example.sess.run([example.z_probability,example.z_mask], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        if np.linalg.matrix_rank(ct.ctrb(A[i][j],B[i][j])) != 4:\n",
    "            print(np.linalg.matrix_rank(ct.ctrb(A[i][j],B[i][j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.z[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = [example.z[0,x,0] for x in range(example.z.shape[0])]\n",
    "second = [example.z[0,x,1] for x in range(example.z.shape[0])]\n",
    "third = [example.z[0,x,2] for x in range(example.z.shape[0])]\n",
    "fourth = [example.z[0,x,3] for x in range(example.z.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(first)\n",
    "plt.plot(second)\n",
    "plt.plot(third)\n",
    "plt.plot(fourth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_magnitude = np.array([0.01,0.1,0.01,0.1])\n",
    "av_time = []\n",
    "\n",
    "env = gym.make('Custom_CartPole-v0', thetaacc_error=2, initial_state=1)\n",
    "g = env.gravity\n",
    "M = env.masscart\n",
    "m = env.masspole\n",
    "l = env.length\n",
    "Q = np.eye(4)*[10,1,1,1]\n",
    "R = 1\n",
    "\n",
    "'''System of equations'''\n",
    "A = np.array([[0,1,0,0],[0,0,-m*g/M,0],[0,0,0,1],[0,0,(M+m)*g/(l*M),0]])\n",
    "B = np.array([[0,1/M,0,-1/(l*M)]]).T\n",
    "\n",
    "\n",
    "'''LQR'''\n",
    "import time\n",
    "K,S,E = ct.lqr(A,B,Q,R)\n",
    "'''Pole Placement'''\n",
    "#K = ct.place(A,B,np.array([-1.1,-1.2,-1.3,-1.4]))\n",
    "\n",
    "\n",
    "#env.x_threshold = 5.0\n",
    "#env.theta_threshold_radians = 10.0\n",
    "\n",
    "\n",
    "states = [[] for _ in range(5)]\n",
    "rewards = np.array([0]*5)\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    states[i_episode].append(observation)\n",
    "    for t in range(500):\n",
    "#         env.render()\n",
    "        u = -np.dot(K,observation)\n",
    "        observation, reward, done, info = env.step(u[0])\n",
    "        states[i_episode].append(observation)\n",
    "        if done:\n",
    "            print(\"Episode finished at time step {}\".format(t+1))\n",
    "            break\n",
    "        rewards[i_episode]+=1\n",
    "    print(\"Episode complete\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
