{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import control as ct\n",
    "import tensorflow as tf\n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.regularizers as reg\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter(object):\n",
    "    \"\"\"\n",
    "    This class defines a kalman filter\n",
    "\n",
    "    l - latent state\n",
    "    l_a_priori - A priori state estimate\n",
    "    l_a_posteriori - A posteriori state estimate\n",
    "\n",
    "    P_a_priori - A priori error covariance\n",
    "    P_a_posteriori - A posteriori error covariance\n",
    "\n",
    "    F - state-transition model\n",
    "    Q - covariance of the process noise    \n",
    "    a, b - observation model and bias\n",
    "    R - covariance of the observation noise\n",
    "    z - observation\n",
    "\n",
    "    y_pre - measurement pre-fit residual\n",
    "    S - Pre-fit residual covariance\n",
    "    K - Kalman gain\n",
    "    y_post - measurement post-fit residual\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_l, dim_z, batch_size, **kwargs):\n",
    "        self.dim_l = dim_l\n",
    "        self.dim_z = dim_z\n",
    "        self.dim_y = dim_z\n",
    "\n",
    "        # lambda initializer for identity matrices\n",
    "        self.eye_init = lambda shape, dtype = np.float32: np.eye(*shape, dtype = dtype)\n",
    "\n",
    "        self._I = tf.constant(self.eye_init((dim_l, dim_l)), name= 'I')\n",
    "\n",
    "\n",
    "        '''This section requires these kwargs to exist, cannot handle missing args'''\n",
    "\n",
    "\n",
    "        '''This section also cannot handle missing kwargs'''\n",
    "        self.l_0 = kwargs.pop('l_0', None)\n",
    "        self.P_0 = kwargs.pop('P_0', None)\n",
    "        self.F = kwargs.pop('F', None)\n",
    "        self.g = kwargs.pop('g', None)\n",
    "        self.a = kwargs.pop('a', None)\n",
    "        self.b = kwargs.pop('b', None)\n",
    "        self.sigma = kwargs.pop('sigma', None)\n",
    "        self.y_0 = kwargs.pop('y_0', None)\n",
    "        self.z_0 = kwargs.pop('z_0', None)\n",
    "        self.pred_0 = kwargs.pop('pred_0', None)\n",
    "        self.z = kwargs.pop('z', None)\n",
    "        self.g_pred = kwargs.pop('g_pred', None)\n",
    "        self.sigma_pred = kwargs.pop('sigma_pred', None)\n",
    "        self.l_0_pred = kwargs.pop('l_0_pred', None)\n",
    "        self.z_0_pred = kwargs.pop('z_0_pred', None)\n",
    "        self.F_pred = kwargs.pop('F_pred', None)\n",
    "        self.a_pred = kwargs.pop('a_pred', None)\n",
    "        self.b_pred = kwargs.pop('b_pred', None)\n",
    "\n",
    "\n",
    "    def forward_filter_fn(self, params, inputs):\n",
    "        \"\"\"\n",
    "        Forward step over a batch\n",
    "        params contains: l_a_posteriori, P_a_posteriori, y_pre\n",
    "        inputs contains: z, F, g, sigma, a, b\n",
    "\n",
    "        Calculates prior distributions based on the given posterior distributions and the current residual\n",
    "                updates posterior distributions based on the new prior distributions\n",
    "        \"\"\"\n",
    "        '''Shapes:\n",
    "            z = (bs, dim_z)\n",
    "            l_a_posteriori = (bs, dim_l, dim_z)\n",
    "            P_a_posteriori = (bs, dim_l, dim_l)\n",
    "            F = (bs, dim_l, dim_l)\n",
    "            Q = (bs, dim_l, dim_l)\n",
    "            R = (bs, dim_z, dim_z)\n",
    "            a = (bs, dim_l, dim_z)\n",
    "            b = (bs, dim_z)\n",
    "        '''\n",
    "        \n",
    "        z, F, g, sigma, a, b = inputs\n",
    "        l_a_posteriori, P_a_posteriori, y_pre, pred = params\n",
    "\n",
    "\n",
    "        \n",
    "        l_a_priori = tf.matmul(F,l_a_posteriori)\n",
    "        P_a_priori = tf.matmul(tf.matmul(F,P_a_posteriori), F, transpose_b = True) + tf.matmul(g,g, transpose_b=True)\n",
    "\n",
    "\n",
    "        y_pre = tf.expand_dims(z - tf.squeeze(tf.add(tf.matmul(a, l_a_priori, transpose_a=True), b),-1),-1)\n",
    "\n",
    "        S = tf.matmul(sigma, sigma, transpose_b=True) + \\\n",
    "            tf.matmul(tf.matmul(a, P_a_priori, transpose_a=True), a)\n",
    "        S_inv = tf.reciprocal(S)\n",
    "        '''TODO: Compute inverse using cholesky decomposition? Only works if a is matrix\n",
    "                so z must be multivariate\n",
    "        '''\n",
    "        \n",
    "        K = tf.matmul(tf.matmul(P_a_priori, a), S_inv)\n",
    "        l_a_posteriori = l_a_priori + tf.matmul(K,y_pre)\n",
    "        I_Ka = self._I-tf.matmul(K,a, transpose_b=True)\n",
    "        P_a_posteriori = tf.matmul(tf.matmul(I_Ka, P_a_priori), I_Ka, transpose_b=True) + \\\n",
    "                         tf.matmul(tf.matmul(K,tf.matmul(sigma, sigma, transpose_b = True)),\n",
    "                                   K, transpose_b=True)\n",
    "        y_post = z - tf.squeeze(tf.add(tf.matmul(a, l_a_posteriori, transpose_a=True), b), -1)\n",
    "        pred = tf.squeeze(tf.add(tf.matmul(a, l_a_posteriori, transpose_a=True),b), -1)\n",
    "        return l_a_posteriori, P_a_posteriori, y_post, pred\n",
    "\n",
    "    def forward_filter(self):\n",
    "        \"\"\"\n",
    "        Compute the forward step in Kalman Filter\n",
    "        The forward pass is initialized with p(x_1) = N(self.x, self.P)\n",
    "        We return the mean and covariance for p(x_t|x_tm1) for t=2, ..., T+1\n",
    "        and the filtering distribution p(x_t|z_1:t) for t=1, ..., T\n",
    "        \"\"\"\n",
    "\n",
    "        forward_states = tf.scan(self.forward_filter_fn,\n",
    "                                 elems = (trans(self.z),trans(self.F),\n",
    "                                          trans(self.g),trans(self.sigma),\n",
    "                                          trans(self.a),trans(self.b)),\n",
    "                                initializer=(self.l_0, self.P_0, self.y_0, self.pred_0))\n",
    "        \n",
    "        return forward_states\n",
    "    \n",
    "    def Kfilter(self):\n",
    "        l_filtered, P_filtered, residuals, filtered_prediction = self.forward_filter()\n",
    "        return trans(l_filtered), trans(P_filtered), trans(residuals), trans(filtered_prediction)\n",
    "        \n",
    "    def forward_predict_fn(self, params, inputs):\n",
    "        \"\"\"Forward step over a batch\n",
    "        params contains l_prev, z_prev\n",
    "        inputs contains F, g, a, b, sigma\"\"\"\n",
    "        \n",
    "        F, g, a, b, sigma = inputs\n",
    "        l_prev, z_prev = params\n",
    "        \n",
    "        l_next = tfd.MultivariateNormalDiag(loc = tf.matmul(F, l_prev), scale_diag = g).sample()\n",
    "        z_next = tfd.Normal(loc = tf.matmul(a, l_prev, transpose_a=True)+b, scale = sigma).sample()\n",
    "        return l_next, z_next\n",
    "    \n",
    "    def forward_predict(self):\n",
    "        \"\"\"\n",
    "        Compute the predictions in state space model\n",
    "        The forward pass is initialized by l_T = p(l_T|z_1:T)\n",
    "        We return the hidden states l_T+1:T+t and predictions z_T+1:T+t\n",
    "        \"\"\"\n",
    "        \n",
    "        forward_predictions = tf.scan(self.forward_predict_fn,\n",
    "                                      elems = (trans(self.F_pred), trans(self.g_pred),\n",
    "                                               trans(self.a_pred), trans(self.b_pred),\n",
    "                                               trans(self.sigma_pred)),\n",
    "                                      initializer = (self.l_0_pred, self.z_0_pred))\n",
    "        \n",
    "        return forward_predictions\n",
    "        \n",
    "    def Kpredict(self):\n",
    "        \n",
    "        l_predicted, z_predicted = self.forward_predict()\n",
    "        return trans(l_predicted), trans(z_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(tensor):\n",
    "    if len(tensor.shape)==3:\n",
    "        return tf.transpose(tensor, [1,0,2])\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1,0,2,3])\n",
    "\n",
    "class NameError(Exception):\n",
    "    pass\n",
    "\n",
    "def find_avg(parameter):\n",
    "    '''Must be passed a numpy array'''\n",
    "    return np.mean(np.reshape(parameter, [-1]+[x for x in parameter.shape[2:]]), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_SSM_model(object):\n",
    "    def __init__(self, sess, name, dim_l = 31, dim_z = 1, num_samples = 963, feature_len = 994,\n",
    "                 learning_rate = 0.00001, lr_decay = 0.95, sigma_upper_bound = 0.1,\n",
    "                 sigma_lower_bound = 0.001, g_upper_bound = 0.1,\n",
    "                 g_lower_bound = 0.01, mu_0_upper_bound = 0.9,mu_0_lower_bound = 0,\n",
    "                 Sigma_0_upper_bound = 0.5, Sigma_0_lower_bound = 0, beta = 0.00001,\n",
    "                 b_upper_bound = 0.05, b_lower_bound = -0.05\n",
    "                ):\n",
    "        if name == '':\n",
    "            raise NameError(\"A model has no name\")\n",
    "\n",
    "            \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = 32\n",
    "        self.train_range = 672\n",
    "        self.test_range = 168\n",
    "        self.num_samples, self.sample_len, feature_len = (num_samples,self.train_range,feature_len)\n",
    "        self.num_batches = self.num_samples//self.batch_size\n",
    "        self.global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "        self.increment_global_step = tf.assign_add(self.global_step,1, name = 'increment_global_step')\n",
    "        self.lr_decay = lr_decay\n",
    "#        self.decayed_learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "#                                                               self.num_batches, self.lr_decay)\n",
    "\n",
    "        \n",
    "        self.dim_l = dim_l # seasonal model, 7 days\n",
    "        self.dim_z = dim_z\n",
    "        self.initial_variance = 1\n",
    "        \n",
    "        self.sigma_upper_bound = sigma_upper_bound\n",
    "        self.g_lower_bound = g_lower_bound\n",
    "        self.g_upper_bound = g_upper_bound\n",
    "        self.mu_0_upper_bound = mu_0_upper_bound\n",
    "        self.mu_0_lower_bound = mu_0_lower_bound\n",
    "        self.Sigma_0_upper_bound = Sigma_0_upper_bound\n",
    "        self.Sigma_0_lower_bound= Sigma_0_lower_bound\n",
    "        self.b_upper_bound = b_upper_bound\n",
    "        self.b_lower_bound = b_lower_bound\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.lstm_sizes = [128,64]\n",
    "        last_lstm = self.lstm_sizes[-1]\n",
    "        \n",
    "        self.model_folder = 'traffic/{}'.format(name)\n",
    "        if not os.path.isdir(self.model_folder):\n",
    "            print('This model has no folder')\n",
    "            os.makedirs(self.model_folder)\n",
    "        self.saved_model_location = '{}/model.ckpt'.format(self.model_folder)\n",
    "#        self.log_dir = 'log/'+name+time.ctime().replace(' ','_')\n",
    "\n",
    "        self.losses = []\n",
    "        self.saver = None\n",
    "        \n",
    "        with tf.variable_scope('KF', reuse = tf.AUTO_REUSE):\n",
    "#             self.F = tf.get_variable(initializer = tf.tile(tf.expand_dims(tf.expand_dims(tf.eye(self.dim_l),0),0),\n",
    "#                                              (self.batch_size,self.sample_len,1,1)),dtype = tf.float32, name = 'F', trainable = False)\n",
    "            self.W_F = tf.get_variables(initializer = tf.random.normal([self.batch_size,self.dim_l*self.dim_l,last_lstm]),\n",
    "                                       dtype = tf.float32, name = 'W_F')\n",
    "            self.bias_F = tf.get_variable(initializer = tf.zeros([self.batch_size, self.dim_l, self.dim_l]),\n",
    "                                         dtype = tf.float32, name = 'bias_F')\n",
    "            self.W_a = tf.get_variable(initializer = tf.random.normal([self.batch_size, self.dim_l, last_lstm]),\n",
    "                                       dtype = tf.float32, name = 'W_a')\n",
    "            self.bias_a = tf.get_variable(initializer = tf.zeros([self.batch_size, self.dim_l, self.dim_z]),\n",
    "                                          dtype = tf.float32, name = 'bias_a')\n",
    "            \n",
    "            self.W_b = tf.get_variable(initializer = tf.random.normal([self.batch_size, self.dim_z, last_lstm]),\n",
    "                                       dtype = tf.float32, name = 'W_b')\n",
    "            self.bias_b = tf.get_variable(initializer = tf.zeros([self.batch_size, self.dim_z, self.dim_z]),\n",
    "                                         dtype = tf.float32, name = 'bias_b')\n",
    "\n",
    "            self.W_g = tf.get_variable(initializer = tf.random.normal([self.batch_size, self.dim_l,\n",
    "                                                                       last_lstm]),\n",
    "                                       dtype = tf.float32, name = 'W_g')\n",
    "            self.bias_g = tf.get_variable(initializer = tf.zeros([self.batch_size, self.dim_l, 1]),\n",
    "                                          dtype = tf.float32, name = 'bias_g')\n",
    "\n",
    "            self.W_sigma = tf.get_variable(initializer = tf.random.normal([self.batch_size, self.dim_z,\n",
    "                                                                           last_lstm]),\n",
    "                                           dtype = tf.float32, name = 'W_sigma')\n",
    "            self.bias_sigma = tf.get_variable(initializer = tf.zeros([self.batch_size, self.dim_z, 1]),\n",
    "                                              dtype = tf.float32, name = 'bias_sigma')\n",
    "\n",
    "            self.W_mu_0 = tf.get_variable(initializer = tf.random.normal([self.batch_size, self.dim_l,\n",
    "                                                                          last_lstm]),\n",
    "                                         dtype = tf.float32, name = 'W_mu_0')\n",
    "            self.bias_mu_0 = tf.get_variable(initializer = tf.zeros([self.batch_size, self.dim_l, 1]),\n",
    "                                            dtype = tf.float32, name = 'bias_mu_0')\n",
    "            \n",
    "            self.W_Sigma_0 = tf.get_variable(initializer = tf.random.normal([self.batch_size, self.dim_l,\n",
    "                                                                             last_lstm]),\n",
    "                                            dtype = tf.float32, name = 'W_Sigma_0')\n",
    "            self.bias_Sigma_0 = tf.get_variable(initializer = tf.zeros([self.batch_size, self.dim_l, 1]),\n",
    "                                                dtype = tf.float32, name = 'bias_Sigma_0')\n",
    "            \n",
    "            \n",
    "            self.P_0 = tf.Variable(tf.tile(tf.expand_dims(self.initial_variance*tf.eye(self.dim_l,dtype = tf.float32),0),\n",
    "                               (self.batch_size, 1, 1)), name = 'P_0', trainable = False)\n",
    "\n",
    "            self.y_0 = tf.Variable(tf.zeros([self.batch_size, self.dim_z]), dtype = tf.float32, name = 'y_0', trainable = False)\n",
    "            self.z_0 = tf.Variable(tf.zeros([self.batch_size, self.dim_z, self.dim_z]), dtype = tf.float32, name = 'z_0', trainable = False)\n",
    "            self.pred_0 = tf.Variable(tf.zeros([self.batch_size, self.dim_z]), dtype = tf.float32, name = 'pred_0', trainable = False)\n",
    "            \n",
    "            self.F_test = tf.placeholder(tf.float32, shape = [self.batch_size,self.test_range, self.dim_l, self.dim_l], name 'F_test')\n",
    "            self.a_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, self.dim_l, self.dim_z], name = 'a_test')\n",
    "            self.b_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, self.dim_z, 1], name = 'b_test')\n",
    "            self.g_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, self.dim_l, 1], name = 'g_test')\n",
    "            self.sigma_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.test_range, self.dim_z, 1], name = 'sigma_test')\n",
    "            self.l_0_test = tf.placeholder(tf.float32, shape = [self.batch_size, self.dim_l, 1], name = 'l_0_test')\n",
    "            self.final_z = tf.placeholder(tf.float32, shape = [self.batch_size, 1, self.dim_z], name = 'final_z')\n",
    "            \n",
    "        with tf.variable_scope('LSTM', reuse = tf.AUTO_REUSE):\n",
    "            self.lstm_input = tf.placeholder(tf.float32, shape= [None, self.sample_len, feature_len], name = 'lstm_input')\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, shape = [None, self.sample_len, self.dim_z], name = 'z')\n",
    "        \n",
    "    def build_LSTM(self, prev_state = None):\n",
    "        with tf.name_scope('LSTM'):\n",
    "            with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "\n",
    "                lstms = [tf.contrib.rnn.LSTMCell(size, reuse=tf.get_variable_scope().reuse) for size in self.lstm_sizes]\n",
    "                dropouts = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = 0.5) for lstm in lstms]\n",
    "\n",
    "                cell = tf.contrib.rnn.MultiRNNCell(dropouts)\n",
    "                if prev_state:\n",
    "                    initial_state = prev_state\n",
    "                else:\n",
    "                    initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "                self.lstm_output, self.final_state = tf.nn.dynamic_rnn(cell, self.lstm_input, initial_state = initial_state)\n",
    "        return self\n",
    "\n",
    "    def affine_transformations(self):\n",
    "\n",
    "        with tf.variable_scope('affine_transformations'):\n",
    "            \n",
    "            self.lstm_output = tf.expand_dims(self.lstm_output, -1) # (32, 672, 64, 1)\n",
    "\n",
    "            def tile_func(Weights_or_bias):\n",
    "                if len(Weights_or_bias.shape) == 2:\n",
    "                    return tf.tile(tf.expand_dims(Weights_or_bias,1), (1,self.sample_len,1))\n",
    "                elif len(Weights_or_bias.shape) == 3:\n",
    "                    return tf.tile(tf.expand_dims(Weights_or_bias,1), (1,self.sample_len,1,1))\n",
    "                else:\n",
    "                    raise ValueError('Unknown sized Weights or bias array')\n",
    "                    \n",
    "            '''TODO: Does tile work by updating all weights for the tile back to the original matrix?    \n",
    "            '''\n",
    "            \n",
    "            W_F = tile_func(self.W_F)\n",
    "            bias_F = tile_func(self.bias_F)\n",
    "            W_a = tile_func(self.W_a)\n",
    "            bias_a = tile_func(self.bias_a)\n",
    "            W_b = tile_func(self.W_b)\n",
    "            bias_b = tile_func(self.bias_b)\n",
    "            W_g = tile_func(self.W_g)\n",
    "            bias_g = tile_func(self.bias_g)\n",
    "            W_sigma = tile_func(self.W_sigma)\n",
    "            bias_sigma = tile_func(self.bias_sigma)\n",
    "\n",
    "    \n",
    "            self.F = tf.reshape(tf.add(tf.matmul(W_F,self.lstm_output),bias_F),[self.batch_size,self.sample_len,self.dim_l,self.dim_l]\n",
    "    \n",
    "            self.a = tf.add(tf.matmul(W_a, self.lstm_output), bias_a)\n",
    "\n",
    "            temp_b = tf.add(tf.matmul(W_b, self.lstm_output), bias_b)\n",
    "            self.b = ((self.b_upper_bound-self.b_lower_bound)/(1+tf.exp(-temp_b)))+self.b_lower_bound\n",
    "\n",
    "            transition_error = tf.add(tf.matmul(W_g, self.lstm_output), bias_g)\n",
    "            self.g = ((self.g_upper_bound-self.g_lower_bound)/(1+tf.exp(-transition_error)))+self.g_lower_bound\n",
    "    \n",
    "            observation_error = tf.add(tf.matmul(W_sigma, self.lstm_output), bias_sigma)\n",
    "            self.sigma = (self.sigma_upper_bound)/(1+tf.exp(-observation_error))\n",
    "            \n",
    "            temp_mu_0 = tf.add(tf.matmul(self.W_mu_0, self.lstm_output[:,-1,:]), self.bias_mu_0)\n",
    "            self.mu_0 = ((self.mu_0_upper_bound-self.mu_0_lower_bound)/(1+tf.exp(-temp_mu_0)))+self.mu_0_lower_bound\n",
    "\n",
    "            temp_Sigma_0 = tf.add(tf.matmul(self.W_Sigma_0, self.lstm_output[:,-1,:]), self.bias_Sigma_0)\n",
    "            self.Sigma_0 = ((self.Sigma_0_upper_bound-self.Sigma_0_lower_bound)/(1+tf.exp(-temp_Sigma_0)))+self.Sigma_0_lower_bound\n",
    "            \n",
    "            l_0_distribution = tfd.MultivariateNormalDiag(loc = self.mu_0, scale_diag = self.Sigma_0)\n",
    "\n",
    "            self.l_0 = l_0_distribution.sample()\n",
    "        return self\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.kf_train = KalmanFilter(batch_size=self.batch_size,\n",
    "                                     dim_l=self.dim_l,\n",
    "                                     dim_z=self.dim_z,\n",
    "                                     l_0 = self.l_0,\n",
    "                                     P_0 = self.P_0,\n",
    "                                     F = self.F,\n",
    "                                     g = self.g,\n",
    "                                     a = self.a,\n",
    "                                     b = self.b,\n",
    "                                     sigma = self.sigma,\n",
    "                                     z = self.z,\n",
    "                                     y_0 = self.y_0,\n",
    "                                     pred_0 = self.pred_0\n",
    "                                    )\n",
    "        with tf.variable_scope('KF_results', reuse=tf.AUTO_REUSE):\n",
    "            self.l_filtered, self.P_filtered, self.residuals, self.filtered_predictions = self.kf_train.Kfilter()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def likelihood_fn(self, params, inputs):\n",
    "        '''Compute likelihood over a batch\n",
    "        params contains: mu, Sigma - the parameters of the likelihood distribution\n",
    "        inputs contains: calculations of mu: F, a, f==l_a_posteriori  <-???\n",
    "                        calculations of Sigma: a, F, S == P_a_posteriori????, g, sigma\n",
    "        '''\n",
    "        a, b, F, g, sigma, f, S = inputs\n",
    "        mu, Sigma = params\n",
    "        '''\n",
    "        a (bs, dim_l, 1)\n",
    "        b (bs, 1)\n",
    "        F (bs, dim_l, dim_l)\n",
    "        g (bs, dim_l, 1)\n",
    "        sigma (bs, 1, 1)\n",
    "        f (bs, dim_l, 1)\n",
    "        S (bs, dim_l, dim_l)\n",
    "        mu (bs, 1, 1)\n",
    "        Sigma (bs, 1, 1)\n",
    "        '''\n",
    "\n",
    "        mu = tf.add(tf.matmul(tf.matmul(a, F, transpose_a=True), f), b)\n",
    "\n",
    "        temp = tf.matmul(tf.matmul(F, S), F, transpose_b=True) + tf.matmul(g, g, transpose_b=True)\n",
    "        Sigma = tf.matmul(tf.matmul(a, temp, transpose_a=True), a) + tf.square(sigma)\n",
    "        \n",
    "        return mu, Sigma\n",
    "    \n",
    "    def build_loss(self):\n",
    "        '''Useful shapes(Ideally):\n",
    "            l_a_posteriori(batch) - (batch_size, sample_len, dim_l)\n",
    "            P_a_posteriori(batch) - (batch_size, sample_len, dim_l,dim_l)\n",
    "            \n",
    "            inputs:\n",
    "                mu_0, a, F, l_a_posteriori?\n",
    "                Sigma_0, a, R, F, P_a_posteriori, Q\n",
    "        '''\n",
    "\n",
    "\n",
    "        with tf.variable_scope('loss', reuse = tf.AUTO_REUSE):\n",
    "            decayed_learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "                                                       self.num_batches, self.lr_decay)\n",
    "\n",
    "\n",
    "                \n",
    "            mu_1 = tf.add(tf.matmul(trans(self.a)[0], self.mu_0, transpose_a=True),trans(self.b)[0])\n",
    "            Sigma_1 = tf.matmul(tf.matmul(trans(self.a)[0], tf.linalg.diag(tf.squeeze(self.Sigma_0)),\n",
    "                                          transpose_a=True),\n",
    "                                trans(self.a)[0]) + tf.square(trans(self.sigma)[0])\n",
    "    \n",
    "            mu, Sigma = tf.scan(self.likelihood_fn,\n",
    "                                elems = (trans(self.a)[1:], trans(self.b)[1:],\n",
    "                                         trans(self.F)[1:], trans(self.g)[1:],\n",
    "                                         trans(self.sigma)[1:],trans(self.l_filtered)[:-1],\n",
    "                                         trans(self.P_filtered)[1:]),\n",
    "                                initializer = (mu_1, Sigma_1))\n",
    "            self.mu = tf.concat([mu_1, tf.squeeze(trans(mu),-1)], 1)\n",
    "            self.Sigma = tf.concat([Sigma_1, tf.squeeze(trans(Sigma),-1)], 1)\n",
    "\n",
    "            z_distribution = tfd.Normal(loc = self.mu, scale = self.Sigma)\n",
    "            self.z_probability = z_distribution.prob(self.z)\n",
    "\n",
    "            regularizers = tf.nn.l2_loss(self.W_g) + tf.nn.l2_loss(self.W_mu_0) + \\\n",
    "                        tf.nn.l2_loss(self.W_sigma) + tf.nn.l2_loss(self.W_Sigma_0) + \\\n",
    "                        tf.nn.l2_loss(self.W_a) + tf.nn.l2_loss(self.W_b)\n",
    "            self.loss = tf.reduce_mean(self.beta*regularizers)-tf.reduce_sum(tf.log(self.z_probability+1e-8))\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(decayed_learning_rate)\n",
    "            grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            capped_grads_and_vars = [(tf.clip_by_norm(grad, 1.), var) for grad, var in grads_and_vars]\n",
    "            self.train_op = self.optimizer.apply_gradients(capped_grads_and_vars)\n",
    "        return self\n",
    "    \n",
    "    def initialize_variables(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        try:\n",
    "            self.saver.restore(self.sess, tf.train.latest_checkpoint(self.model_folder))\n",
    "            print(\"Restoring model from {}\".format(self.saved_model_location))\n",
    "        except:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            print(\"Initializing new model at {}\".format(self.saved_model_location))\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        start = time.time()\n",
    "#        merged = tf.summary.merge_all()\n",
    "#        writer = tf.summary.FileWriter(self.log_dir, self.sess.graph)\n",
    "        for i in range(epochs):\n",
    "            preds = []\n",
    "            epoch_loss = []\n",
    "            perm = np.random.permutation(self.num_samples)\n",
    "            for idx in range(self.num_batches):\n",
    "\n",
    "\n",
    "                slc = np.array([perm[j] for j in range(idx*self.batch_size, (idx+1)*self.batch_size)])\n",
    "                lane_id_onehot = np.zeros([self.batch_size, self.train_range, self.num_samples])\n",
    "                for k in range(self.batch_size):\n",
    "                    lane_id_onehot[k, :,slc[k]] = 1\n",
    "                train_x = np.concatenate([self.batch_x_seasonality, lane_id_onehot], axis = 2)\n",
    "\n",
    "                feed_dict = {self.lstm_input: train_x,self.z: self.train_z[slc]}\n",
    "#                summary, loss_, _, _ = self.sess.run([merged, self.loss, self.train_op, self.increment_global_step],\n",
    "#                                                  feed_dict=feed_dict)\n",
    "                loss_, _, _ = self.sess.run([self.loss, self.train_op, self.increment_global_step],\n",
    "                                            feed_dict=feed_dict)\n",
    "                epoch_loss.append(loss_)\n",
    "                print(tf.trainable_variables())\n",
    "#                writer.add_summary(summary, self.sess.run(self.global_step))\n",
    "#                if idx % 10 == 0:\n",
    "#                  print('mini epoch #{}'.format(idx))\n",
    "\n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "            self.losses.append(epoch_loss)\n",
    "            print(\"Epoch #{}\\tTime Elapsed: {}\\tNegative Log-Likelihood {}\".\n",
    "                  format(self.sess.run(self.global_step)/self.num_batches,\n",
    "                         (time.time()-start)/60, epoch_loss))\n",
    "            if (i+1)%50 == 0:\n",
    "                self.saver.save(self.sess, self.saved_model_location, global_step = self.global_step)\n",
    "                print(\"Model Saved at {}\".format(self.saved_model_location))\n",
    "#        self.saver.save(self.sess, self.saved_model_location, global_step = self.global_step)\n",
    "#        print(\"Model Saved at {}\".format(self.saved_model_location))\n",
    "        return self.losses\n",
    "    \n",
    "    def get_test_variables(self):\n",
    "        start = time.time()\n",
    "        preds = []\n",
    "        gs = []\n",
    "        sigmas = []\n",
    "        Q50_numerator = 0\n",
    "        Q90_numerator = 0\n",
    "        \n",
    "        for idx in range((self.num_samples//self.batch_size)):\n",
    "            beg,end = [idx*self.batch_size, (idx+1)*self.batch_size]\n",
    "            \n",
    "            # Get the final hidden state from kalman filter\n",
    "            lane_id_onehot = np.zeros([self.batch_size, self.train_range, self.num_samples])\n",
    "            for k in range(beg, end):\n",
    "                lane_id_onehot[k%self.batch_size, :, k] = 1\n",
    "            train_x = np.concatenate([self.batch_x_seasonality, lane_id_onehot], axis = 2)\n",
    "            \n",
    "            feed_dict = {self.lstm_input: train_x, self.z: self.train_z[beg:end]}\n",
    "        \n",
    "            filtered_train, g_train, sigma_train, self.l_filtered_test, self.final_lstm_state = self.sess.run([self.filtered_predictions, self.g, self.sigma,\n",
    "                                                                                                               self.l_filtered, self.final_state],\n",
    "                                                                                                              feed_dict=feed_dict)\n",
    "\n",
    "            l_0_test = self.l_filtered_test[:,-1,:,:]\n",
    "            \n",
    "            self.build_LSTM(self.final_lstm_state).affine_transformations()\n",
    "            \n",
    "            feed_dict = {self.lstm_input: train_x}\n",
    "            g_test, sigma_test, a_test, b_test, F_test= self.sess.run([self.g, self.sigma, self.a, self.b, self.F],\n",
    "                                                         feed_dict=feed_dict)\n",
    "            feed_dict = {self.final_z: self.train_z[beg:end,-1,:,np.newaxis], self.l_0_test: l_0_test,\n",
    "                         self.g_test: g_test[:,:self.test_range], self.sigma_test: sigma_test[:,:self.test_range],\n",
    "                         self.a_test: a_test[:,:self.test_range], self.b_test: b_test[:,:self.test_range],\n",
    "                         self.F_test: F_test[:.:self.test_range]\n",
    "                        }\n",
    "            \n",
    "            z_preds = np.squeeze(self.sess.run([self.z_preds_test], feed_dict=feed_dict)[0])\n",
    "            preds.append(np.concatenate([np.squeeze(filtered_train),z_preds], axis = 1))\n",
    "            gs.append(np.squeeze(np.concatenate([g_train,g_test[:,:self.test_range]], axis = 1)))\n",
    "            sigmas.append(np.squeeze(np.concatenate([sigma_train,sigma_test[:,:self.test_range]], axis = 1)))\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                for j in range(len(z_preds[i])):\n",
    "                    Q50_numerator += self.P(0.5, z_preds[i][j], self.test_z[beg+i][j])\n",
    "                    Q90_numerator += self.P(0.9, z_preds[i][j], self.test_z[beg+i][j])\n",
    "\n",
    "        preds = np.asarray(preds)\n",
    "        preds = np.reshape(preds, (-1,preds.shape[-1]))\n",
    "        gs = np.asarray(gs)\n",
    "        gs = np.reshape(gs, (-1, gs.shape[-2], gs.shape[-1]))\n",
    "        sigmas = np.asarray(sigmas)\n",
    "        sigmas = np.reshape(sigmas, (-1, sigmas.shape[-1]))\n",
    "        Q_denomenator = np.sum(np.abs(self.test_z))\n",
    "        pq50_loss = 2*Q50_numerator/Q_denomenator\n",
    "        pq90_loss = 2*Q90_numerator/Q_denomenator\n",
    "        \n",
    "        return preds, gs, sigmas, pq50_loss, pq90_loss\n",
    "    \n",
    "    \n",
    "    def build_test(self):\n",
    "        self.kf_test = KalmanFilter(batch_size=self.batch_size,\n",
    "                                    test_range = self.test_range,\n",
    "                                    dim_l=self.dim_l,\n",
    "                                    dim_z=self.dim_z,\n",
    "                                    l_0_pred = self.l_0_test,\n",
    "                                    g_pred = self.g_test,\n",
    "                                    sigma_pred = self.sigma_test,\n",
    "                                    F_pred = self.F_test,#F_pred = self.F[:,:self.test_range],\n",
    "                                    a_pred = self.a_test,\n",
    "                                    b_pred = self.b_test,\n",
    "                                    z_0_pred = self.final_z\n",
    "                                   )\n",
    "        with tf.name_scope('KF_Predictions'):\n",
    "            with tf.variable_scope('KF_Predictions'):\n",
    "                self.l_preds_test, self.z_preds_test = self.kf_test.Kpredict()\n",
    "        return self\n",
    "            \n",
    "    def load_data(self):\n",
    "        all_z = np.load('formatted_traffic.npy')\n",
    "        train_range = 28\n",
    "        self.train_z = np.reshape(all_z[:,:train_range,:], [all_z.shape[0], -1, 1])\n",
    "        self.test_z = np.reshape(all_z[:,train_range:,:], [all_z.shape[0], -1])\n",
    "        del all_z\n",
    "        x_seasonality = np.load('x_seasonality.npy')\n",
    "        self.batch_x_seasonality = np.repeat(np.expand_dims(x_seasonality, 0), self.batch_size, axis = 0)\n",
    "        return self\n",
    "    \n",
    "    def P(self, rho, z, z_pred):\n",
    "        if z > z_pred:\n",
    "            return rho*(z-z_pred)\n",
    "        else:\n",
    "            return (1-rho)*(z_pred-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alon/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "This model has no folder\n",
      "WARNING:tensorflow:From <ipython-input-4-1454cf2c28b7>:117: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-1454cf2c28b7>:120: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-1454cf2c28b7>:125: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/alon/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Initializing new model at traffic/testing/model.ckpt\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Resource __per_step_2/loss/gradients/AddN_13/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node loss/gradients/AddN_13/tmp_var}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Resource __per_step_2/loss/gradients/AddN_13/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node loss/gradients/AddN_13/tmp_var}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-97ac1029a0b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine_transformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1454cf2c28b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;31m#                                                  feed_dict=feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 loss_, _, _ = self.sess.run([self.loss, self.train_op, self.increment_global_step],\n\u001b[0;32m--> 302\u001b[0;31m                                             feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Resource __per_step_2/loss/gradients/AddN_13/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node loss/gradients/AddN_13/tmp_var}}]]"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "with tf.Session() as sess:\n",
    "    model = LSTM_SSM_model(sess, name = 'testing', learning_rate = 0.0001, lr_decay = 0.999)\n",
    "    model.build_LSTM().affine_transformations().build_model().build_loss().load_data()\n",
    "    model.initialize_variables()\n",
    "    loss = model.train(epochs = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
