\documentclass[twoside]{article}
\usepackage{amsmath,amsfonts,listings}
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\begin{document}
\section{Ideas/Issues}
Even if I train the obseravtion model and the controls at the same time succesfully, how do I motivate the controls to do what I want?\\

Use the built in reward (+1 for every time frame where the pole is "up")? Add an extra term to loss that depends on both time and controls?\\

Devise a better loss function. The current loss function is $\mathcal{L}(\phi)=\sum_{i=1}^{N}{\log{p_{SS}}(z_{1:T_{i}}|\Theta_{1:T_{i}}})$\\

Maybe use unscented Kalman Filter??\\

Multiply the loss term by a factor of $1/t$ or $1/t^{2}$. Something along those lines to get implicit reliance on time component. Suggests that a system that learns to stay upright longer has less error/smaller gradient than a system that falls over quickly.\\

Run x trials with the current model, train the model on those trials with the loss multiplier. Repeat with the updated parameters.\\

Need a separate optimization problem for learning the control? Maybe learn mapping from observations to control directly with nn.\\
Design a policy function/mapping from observations to controls

\subsection{Training}
The considerations for training are:
\begin{itemize}
\item What/when I'm training (observation model, control, 
\item what the goal of that training is(learn to control the system, or just explore, i.e. learn to predict the next state)
\item How to get training samples(online/offline)
\item LSTM fixed/variable length
\item Inputs to LSTM(physical specifications, previous outputs, etc.)
\end{itemize}
\subsection{Thoughts}
\begin{itemize}
\item Train transition model with LQR, hope that the model learns how to linearize given any particular observation. Then separately train the learned transition/control models to put the pole into any place of our choosing. 
Loss function: euclidean distance from our desired state? Only works if we have knowledge of what the states mean
\item Suppose that I include the previous state as input to the LSTM, the system becomes auto-regressive, do the assumptions leading to the kalman filter, and the log-likelihood function still hold?
\end{itemize}

\section{Assumptions}
We have the linear dynamical system:\[\begin{array}{lclr}l_{t+1} & = & A_{t}l_{t}+B_{t}u_{t}+g_{t}\epsilon_{t}&\epsilon_{t}\sim\mathcal{N}(0,1) \\ Z_{t} & = & C_{t}l_{t} + D_{t}u_{t} + \sigma_{t}\varepsilon_{t} & \varepsilon_{t}\sim\mathcal{N}(0,1)\end{array}\]
where $A\in\mathbb{R}^{m\times n}, B\in\mathbb{R}^{m\times r}, C\in\mathbb{R}^{z\times m}, D\in\mathbb{R}^{z\times r}, l\in\mathbb{R}^{m}, u\in\mathbb{R}^{r}, Z\in\mathbb{R}^{z}, \epsilon\in\mathbb{R}^{m}, \varepsilon\in\mathbb{R}^{z}$
However, it simplifies if, as in our situation, C = I, and D = 0.

\section{Potential mistakes}
In computing the likelihood: $\mathcal{N}(z_{t}|\mu_{t},\Sigma_{t})$
Compare the paper's calculation:\[\begin{array}{lcl} \mu_{1} & = & a_{1}^{T}\mu_{0} \\ \Sigma_{1} & = & a_{1}^{T}\Sigma_{0}a_{1} + \sigma_{1}^{2} \\ \mu_{t} & = & a_{t}F_{t}f_{t-1} \\ \Sigma_{t} & = & a_{t}^{T}(F_{t}S_{t}F_{t}^{T}+g_{t}g_{t}^{T})a_{t}+\sigma_{t}^{2}\end{array}\]
with my control version:
\begin{lstlisting}
mu_1 = tf.matmul(trans(self.C)[0], self.mu_0)
mu = tf.matmul(C, tf.add(tf.matmul(A,l_filtered), tf.matmul(B,u)))

Sigma_1 = tf.matmul(tf.matmul(trans(self.C)[0], tf.linalg.diag(tf.squeeze(self.Sigma_0))),trans(self.C)[0], transpose_b=True)+tf.square(trans(self.sigma)[0])

temp = tf.matmul(tf.matmul(A, P_filtered), A, transpose_b=True) + tf.matmul(g, g, transpose_b=True)
Sigma = tf.matmul(tf.matmul(C, temp), C, transpose_b=True) + tf.square(sigma)
\end{lstlisting}
\end{document}