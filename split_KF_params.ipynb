{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import control as ct\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as reg\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_forward_filter_fn(A,B,u,g,C,sigma,l_a_posteriori,P_a_posteriori,z):\n",
    "    '''Calculates prior distribution based on the previous posterior distribution\n",
    "        and the current residual updates posterior distribution based on the new\n",
    "        prior distribution\n",
    "    '''\n",
    "    print('z',z)\n",
    "    print('A', A)\n",
    "    print('B',B)\n",
    "    print('u',u)\n",
    "    print('g',g)\n",
    "    print('sigma',sigma)\n",
    "    print('C', C)\n",
    "    print('l_a_posteriori', l_a_posteriori)\n",
    "    print('P_a_posteriori', P_a_posteriori)\n",
    "    _I = tf.eye(int(A.shape[0]), dtype = tf.float64)\n",
    "    \n",
    "    z = tf.expand_dims(z,-1)\n",
    "    l_a_priori = tf.matmul(A,l_a_posteriori) + tf.matmul(B,u)\n",
    "#     print('l_a_priori',l_a_priori)\n",
    "    P_a_priori = tf.matmul(tf.matmul(A,P_a_posteriori), A, transpose_b = True) + tf.matmul(g,g, transpose_b=True)\n",
    "#     print('P_a_priori',P_a_priori)\n",
    "    y_pre = z - tf.matmul(C,l_a_priori)\n",
    "#     print('y_pre', y_pre)\n",
    "    S = tf.square(sigma) + tf.matmul(tf.matmul(C, P_a_priori), C, transpose_b=True)\n",
    "#     print('S',S)\n",
    "    S_inv = tf.math.reciprocal(S)\n",
    "#     print('S_inv', S_inv)\n",
    "    K = tf.matmul(tf.matmul(P_a_priori, C, transpose_b=True), S_inv)\n",
    "#     print('K', K)\n",
    "    l_a_posteriori = l_a_priori + tf.matmul(K,y_pre)\n",
    "#     print('l_a_posteriori', l_a_posteriori)\n",
    "    I_KC = _I-tf.matmul(K,C)\n",
    "#     print('I-KC', I_KC)\n",
    "    P_a_posteriori = tf.matmul(tf.matmul(I_KC, P_a_priori), I_KC, transpose_b=True) + \\\n",
    "                        tf.matmul(tf.matmul(K,tf.matmul(sigma, sigma, transpose_b = True)),\n",
    "                                K, transpose_b=True)\n",
    "#     print('P_a_posteriori',P_a_posteriori)\n",
    "    y_post = z-tf.matmul(C,l_a_posteriori)\n",
    "    squared_error = tf.squeeze(tf.matmul(y_post,y_post, transpose_a=True))\n",
    "#     print(squared_error)\n",
    "#     print('y_post', y_post)\n",
    "    pred = tf.matmul(C, l_a_posteriori)\n",
    "#     print('pred', pred)\n",
    "        \n",
    "    return l_a_posteriori,P_a_posteriori,z, pred, squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class split_KF_Model(object):\n",
    "    def __init__(self, view = False, initial_state_variation = [0,0,0,0], control = False):\n",
    "        self.view = view\n",
    "        self.m = 4\n",
    "        self.dim_z = self.m\n",
    "        self.n = 4\n",
    "        self.r = 1\n",
    "        self.lstm_input_dim = self.m+4\n",
    "        self.sigma_upper_bound = 1\n",
    "        self.sigma_lower_bound = 0\n",
    "        self.g_upper_bound = 1\n",
    "        self.g_lower_bound = 0\n",
    "        self.mu_0_upper_bound = 1\n",
    "        self.mu_0_lower_bound = 0\n",
    "        self.Sigma_0_upper_bound = 1\n",
    "        self.Sigma_0_lower_bound = 0\n",
    "        self.weight_beta = 100\n",
    "        self.bias_beta = 1000\n",
    "        thetaacc_error = 0\n",
    "        self.global_epoch = 0\n",
    "        \n",
    "        '''Temporary LQR variables'''\n",
    "        self.Q = np.eye(4)*[1,1,1,1]\n",
    "        self.R = 1\n",
    "\n",
    "        self.initial_variance_estimate = 1\n",
    "\n",
    "        self.lstm_sizes = [128,64]\n",
    "        self.env = gym.make('Custom_CartPole-v0', thetaacc_error=thetaacc_error, initial_state_variation=initial_state_variation)\n",
    "        gravity = self.env.gravity\n",
    "        cart_mass = self.env.masscart\n",
    "        pole_mass = self.env.masspole\n",
    "        pole_length = self.env.length\n",
    "        self.env_params = tf.expand_dims(np.array([gravity, cart_mass,pole_mass,pole_length],\n",
    "                                             dtype=np.float64),0)\n",
    "        self.control = control\n",
    "        self.variables = []\n",
    "        \n",
    "    def build_LSTM(self):\n",
    "        lstms = [tf.contrib.rnn.LSTMCell(size, reuse=tf.get_variable_scope().reuse) for size in self.lstm_sizes]\n",
    "        dropouts = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = 0.5) for lstm in lstms]\n",
    "\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell(dropouts)\n",
    "#         print(self.cell.trainable_variables)\n",
    "#         print(self.cell.trainable_weights)\n",
    "#         self.variables.append(self.cell.trainable_variables)\n",
    "        return self\n",
    "    \n",
    "    def get_variables(self):\n",
    "        return self.variables\n",
    "    def reset_variables(self):\n",
    "        self.variables = []\n",
    "        return self\n",
    "    def set_control(self, control):\n",
    "        self.control = control\n",
    "        return self\n",
    "    \n",
    "    def likelihood_fn(self, params, inputs):\n",
    "        A, B, u, g, C, sigma, l_filtered, P_filtered = inputs\n",
    "        mu_1, Sigma_1 = params\n",
    "#         print('A',len(A))\n",
    "#         print('B',len(B))\n",
    "#         print('u',len(u))\n",
    "#         print('C',len(C))\n",
    "#         print('g',len(g))\n",
    "#         print('sigma',len(sigma))\n",
    "#         print('l_filtered',len(l_filtered))\n",
    "#         print('p_filtered',len(P_filtered))\n",
    "#         print('mu_1',mu_1.shape)\n",
    "#         print('Sigma_1',Sigma_1.shape)\n",
    "        mu = [mu_1]\n",
    "        Sigma = [Sigma_1]\n",
    "        assert(len(A)==len(B) and len(B)==len(u) and len(u)==len(sigma) and \n",
    "               len(sigma)==len(l_filtered) and len(l_filtered)==len(P_filtered)),\"Not all sequences are same length\"\n",
    "        for i in range(len(A)):\n",
    "            mu.append(tf.matmul(C, tf.add(tf.matmul(A[i],l_filtered[i]), tf.matmul(B[i],u[i]))))\n",
    "            temp = tf.matmul(tf.matmul(A[i], P_filtered[i]), A[i], transpose_b=True) + \\\n",
    "                        tf.matmul(g[i], g[i], transpose_b=True)\n",
    "            Sigma.append(tf.matmul(tf.matmul(C, temp), C, transpose_b=True) + \\\n",
    "                        tf.matmul(sigma[i],sigma[i],transpose_b=True))\n",
    "        return mu,Sigma\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.reset_variables()\n",
    "        rewards = 0\n",
    "        A_all = []\n",
    "        B_all = []\n",
    "        u_all = []\n",
    "        g_all = []\n",
    "        C_1 = tf.Variable(np.array([[1,0,0,0]]), dtype = tf.float64, trainable=False)\n",
    "        C_2 = tf.Variable(np.array([[0,1,0,0]]), dtype = tf.float64, trainable=False)\n",
    "        C_3 = tf.Variable(np.array([[0,0,1,0]]), dtype = tf.float64, trainable=False)\n",
    "        C_4 = tf.Variable(np.array([[0,0,0,1]]), dtype = tf.float64, trainable=False)\n",
    "        sigma1_all = []\n",
    "        sigma2_all = []\n",
    "        sigma3_all = []\n",
    "        sigma4_all = []\n",
    "        l_a_posteriori1 = []\n",
    "        l_a_posteriori2 = []\n",
    "        l_a_posteriori3 = []\n",
    "        l_a_posteriori4 = []\n",
    "        P_a_posteriori1 = []\n",
    "        P_a_posteriori2 = []\n",
    "        P_a_posteriori3 = []\n",
    "        P_a_posteriori4 = []\n",
    "        env_states1 = []\n",
    "        env_states2 = []\n",
    "        env_states3 = []\n",
    "        env_states4 = []\n",
    "        preds1 = []\n",
    "        preds2 = []\n",
    "        preds3 = []\n",
    "        preds4 = []\n",
    "        squared_error1 = []\n",
    "        squared_error2 = []\n",
    "        squared_error3 = []\n",
    "        squared_error4 = []\n",
    "        rbf = []\n",
    "#         all_KF_params = [A_all,B_all,u_all,g_all,C_all,sigma1_all,sigma2_all,sigma3_all,sigma4_all,\n",
    "#                          l_a_posteriori1,l_a_posteriori2,l_a_posteriori3,l_a_posteriori4,\n",
    "#                          P_a_posteriori1,P_a_posteriori2,P_a_posteriori3,P_a_posteriori4,\n",
    "#                          env_states,preds,squared_error,rbf]\n",
    "        KF1_params = [l_a_posteriori1,P_a_posteriori1,env_states1, preds1, squared_error1]\n",
    "        KF2_params = [l_a_posteriori2,P_a_posteriori2,env_states2, preds2, squared_error2]\n",
    "        KF3_params = [l_a_posteriori3,P_a_posteriori3,env_states3, preds3, squared_error3]\n",
    "        KF4_params = [l_a_posteriori4,P_a_posteriori4,env_states4, preds4, squared_error4]\n",
    "\n",
    "        '''p-quantile loss'''\n",
    "        Q50_numerator = np.zeros(4)\n",
    "        Q90_numerator = np.zeros(4)\n",
    "        \n",
    "        '''Build LSTM'''\n",
    "        self.build_LSTM()\n",
    "        \n",
    "        '''Start gym environment'''\n",
    "        observation=self.env.reset()\n",
    "\n",
    "        '''Get initial lstm state and input, get first output/state'''\n",
    "        initial_state = self.cell.get_initial_state(batch_size=1,dtype = tf.float64)\n",
    "        initial_input = tf.concat((self.env_params,\n",
    "                                   tf.expand_dims(tf.convert_to_tensor(observation,dtype=tf.float64),0)),\n",
    "                                  axis=1)\n",
    "#         initial_input = tf.concat((self.env_params, np.zeros(shape=(1,4)),axis=1)\n",
    "        output_single, state_single = self.cell(inputs=initial_input, state=initial_state)\n",
    "        self.variables.extend(self.cell.trainable_variables)\n",
    "\n",
    "#         print('LSTM cell trainable',len(self.cell.trainable_variables))\n",
    "#         print('Rewards', self.rewards)\n",
    "#         print('VARIABLES',[x.name for x in self.cell.trainable_variables])\n",
    "#         print('\\n\\n\\nWEIGHTS',[x.name for x in self.cell.trainable_weights])\n",
    "\n",
    "        '''Calculate mu_0,Sigma_0, distribution using initial LSTM output'''\n",
    "        container = tf.contrib.eager.EagerVariableStore()\n",
    "        with container.as_default():\n",
    "            mu_0 = tf.layers.dense(output_single, self.m, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                       bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                       name = 'mu_0dense', reuse = True)\n",
    "            Sigma_0 = tf.layers.dense(output_single, self.m, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                          bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                          name = 'Sigma_0dense', reuse = True)\n",
    "        mu_0 = tf.reshape(mu_0, shape = (self.m,1))\n",
    "        mu_0 = ((self.mu_0_upper_bound-self.mu_0_lower_bound)/(1+tf.exp(-mu_0)))+self.mu_0_lower_bound\n",
    "\n",
    "        Sigma_0 = tf.reshape(Sigma_0, shape = (self.m,1))\n",
    "        Sigma_0 = tf.matmul(Sigma_0,Sigma_0,transpose_b=True)+tf.eye(4, dtype=tf.float64)*1e-8\n",
    "\n",
    "        l_0_dist = tfd.MultivariateNormalFullCovariance(loc = tf.squeeze(mu_0),\n",
    "                                                                covariance_matrix= Sigma_0,\n",
    "                                                                validate_args=True)\n",
    "        l_0 = tf.expand_dims(l_0_dist.sample(),1)\n",
    "        l_a_posteriori1.append(l_0)\n",
    "        l_a_posteriori2.append(l_0)\n",
    "        l_a_posteriori3.append(l_0)\n",
    "        l_a_posteriori4.append(l_0)\n",
    "        P_a_posteriori1.append(self.initial_variance_estimate)\n",
    "        P_a_posteriori2.append(self.initial_variance_estimate)\n",
    "        P_a_posteriori3.append(self.initial_variance_estimate)\n",
    "        P_a_posteriori4.append(self.initial_variance_estimate)\n",
    "\n",
    "        first_pass = True\n",
    "        done = False\n",
    "        while not done:\n",
    "            if self.view:\n",
    "                self.env.render()\n",
    "            '''Get lstm outputs'''\n",
    "            with container.as_default():\n",
    "                A = tf.layers.dense(output_single, self.m*self.n, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                 bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                 name = 'A_dense', reuse = True)\n",
    "                if self.control:\n",
    "                    B = tf.layers.dense(output_single, self.m*self.r, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                        bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                        name = 'B_dense', reuse = True)\n",
    "                g = tf.layers.dense(output_single, self.m, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                    bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                    name = 'g_dense', reuse = True)\n",
    "                sigma1 = tf.layers.dense(output_single, 1, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                        bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                        name = 'sigma1_dense', reuse = True)\n",
    "                sigma2 = tf.layers.dense(output_single, 1, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                        bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                        name = 'sigma2_dense', reuse = True)\n",
    "                sigma3 = tf.layers.dense(output_single, 1, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                        bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                        name = 'sigma3_dense', reuse = True)\n",
    "                sigma4 = tf.layers.dense(output_single, 1, kernel_regularizer = reg.l1(self.weight_beta),\n",
    "                                        bias_regularizer = reg.l2(self.bias_beta),\n",
    "                                        name = 'sigma4_dense', reuse = True)\n",
    "                \n",
    "#             print('container total',len(container.variables()))\n",
    "#             print('container trainable',len(container.trainable_variables()))\n",
    "            '''If this is first pass in loop, add variables to graph'''\n",
    "            if first_pass:\n",
    "                self.variables.extend(container.trainable_variables())\n",
    "                first_pass = False\n",
    "            '''Reshape and transform variables'''\n",
    "            A = tf.reshape(A, shape = (self.m,self.n))\n",
    "            if self.control:\n",
    "                B = tf.reshape(B, shape = (self.m,self.r))\n",
    "            else:\n",
    "                B = tf.zeros(shape = (self.m,self.r), dtype = tf.float64)\n",
    "            g = tf.reshape(g, shape = (self.m, 1))\n",
    "            g = ((self.g_upper_bound-self.g_lower_bound)/(1+tf.exp(-g)))+self.g_lower_bound\n",
    "#             sigma = tf.reshape(sigma, shape = (self.dim_z,1))\n",
    "            sigma1 = ((self.sigma_upper_bound-self.sigma_lower_bound)/(1+tf.exp(-sigma1)))+self.sigma_lower_bound\n",
    "            sigma2 = ((self.sigma_upper_bound-self.sigma_lower_bound)/(1+tf.exp(-sigma2)))+self.sigma_lower_bound\n",
    "            sigma3 = ((self.sigma_upper_bound-self.sigma_lower_bound)/(1+tf.exp(-sigma3)))+self.sigma_lower_bound\n",
    "            sigma4 = ((self.sigma_upper_bound-self.sigma_lower_bound)/(1+tf.exp(-sigma4)))+self.sigma_lower_bound\n",
    "            \n",
    "            if self.control:\n",
    "                K,S,E = ct.lqr(A.numpy(),B.numpy(),self.Q,self.R)\n",
    "                u = -tf.matmul(K.astype(np.float64),\n",
    "                               tf.expand_dims(tf.convert_to_tensor(observation,dtype=tf.float64),-1))\n",
    "            else:\n",
    "                u = tf.zeros(shape = [1,self.r], dtype=tf.float64)\n",
    "#             C = tf.eye(self.dim_z, dtype = tf.float64)\n",
    "            observation, reward, done, info = self.env.step(tf.squeeze(u))\n",
    "#             print(observation.shape)\n",
    "            '''Calculate:\n",
    "                A,B,u,g,C,sigma,l_a_posteriori,P_a_posteriori,env_states'''\n",
    "#             KF_update = split_forward_filter_fn(A, B, u,g, C, sigma,l_a_posteriori[-1],P_a_posteriori[-1],\n",
    "#                                           tf.convert_to_tensor(observation,dtype=tf.float64))\n",
    "            KF1_update = split_forward_filter_fn(A,B,u,g,C_1,sigma1,l_a_posteriori1[-1],P_a_posteriori1[-1],\n",
    "                                                tf.convert_to_tensor(observation[0],dtype=tf.float64))\n",
    "            KF2_update = split_forward_filter_fn(A,B,u,g,C_2,sigma2,l_a_posteriori2[-1],P_a_posteriori2[-1],\n",
    "                                                tf.convert_to_tensor(observation[1],dtype=tf.float64))\n",
    "            KF3_update = split_forward_filter_fn(A,B,u,g,C_3,sigma3,l_a_posteriori3[-1],P_a_posteriori3[-1],\n",
    "                                                tf.convert_to_tensor(observation[2],dtype=tf.float64))\n",
    "            KF4_update = split_forward_filter_fn(A,B,u,g,C_4,sigma4,l_a_posteriori4[-1],P_a_posteriori4[-1],\n",
    "                                                tf.convert_to_tensor(observation[3],dtype=tf.float64))\n",
    "            A_all.append(A)\n",
    "            B_all.append(B)\n",
    "            u_all.append(u)\n",
    "            g_all.append(g)\n",
    "            sigma1_all.append(sigma1)\n",
    "            sigma2_all.append(sigma2)\n",
    "            sigma3_all.append(sigma3)\n",
    "            sigma4_all.append(sigma4)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            '''Update lists:\n",
    "                A_all,B_all,u_all,g_all,C_all,sigma_all,l_a_posteriori,P_a_posteriori,env_states'''\n",
    "#             for KF_single,KF_param  in zip(KF_update,all_KF_params):\n",
    "#                 KF_param.append(KF_single)\n",
    "            for KF_single,KF_param in zip(KF1_update,KF1_params):\n",
    "                KF_param.append(KF_single)\n",
    "            for KF_single,KF_param in zip(KF2_update,KF2_params):\n",
    "                KF_param.append(KF_single)\n",
    "            for KF_single,KF_param in zip(KF3_update,KF3_params):\n",
    "                KF_param.append(KF_single)\n",
    "            for KF_single,KF_param in zip(KF4_update,KF4_params):\n",
    "                KF_param.append(KF_single)\n",
    "                \n",
    "            rewards+=1\n",
    "\n",
    "            next_input = tf.concat((self.env_params,env_states1[-1],env_states2[-1],\n",
    "                                    env_states3[-1],env_states4[-1]),axis=1)\n",
    "            output_single,state_single=self.cell(inputs=next_input,state=state_single)\n",
    "        if self.view:\n",
    "            self.env.close()\n",
    "\n",
    "#         param_names = ['A_all','B_all','u_all','g_all','C_all','sigma_all',\n",
    "#                        'l_a_posteriori','P_a_posteriori','env_states','preds']\n",
    "#             for name,KF_param in zip(param_names,all_KF_params):\n",
    "#                 print(name,len(KF_param), KF_param[0].shape)\n",
    "\n",
    "\n",
    "        mu_11 = tf.add(tf.matmul(A_all[0], mu_0),tf.matmul(B_all[0],u_all[0]))\n",
    "        Sigma_11 = tf.add(tf.matmul(tf.matmul(C_1,Sigma_0),C_1, transpose_b=True),\n",
    "                     tf.matmul(sigma1_all[0],sigma1_all[0],transpose_b=True))\n",
    "        mu_12 = tf.add(tf.matmul(A_all[0], mu_0),tf.matmul(B_all[0],u_all[0]))\n",
    "        Sigma_12 = tf.add(tf.matmul(tf.matmul(C_2,Sigma_0),C_2, transpose_b=True),\n",
    "                     tf.matmul(sigma2_all[0],sigma2_all[0],transpose_b=True))\n",
    "        mu_13 = tf.add(tf.matmul(A_all[0], mu_0),tf.matmul(B_all[0],u_all[0]))\n",
    "        Sigma_13 = tf.add(tf.matmul(tf.matmul(C_3,Sigma_0),C_3, transpose_b=True),\n",
    "                     tf.matmul(sigma3_all[0],sigma3_all[0],transpose_b=True))\n",
    "        mu_14 = tf.add(tf.matmul(A_all[0], mu_0),tf.matmul(B_all[0],u_all[0]))\n",
    "        Sigma_14 = tf.add(tf.matmul(tf.matmul(C_4,Sigma_0),C_4, transpose_b=True),\n",
    "                     tf.matmul(sigma4_all[0],sigma4_all[0],transpose_b=True))\n",
    "#         print(mu_1.shape)\n",
    "#         print(Sigma_1.shape)\n",
    "        if rewards > 1:\n",
    "            mu1,Sigma1 = self.likelihood_fn((mu_11,Sigma_11),(A_all,B_all,u_all,g_all,\n",
    "                                                     C_1,sigma1_all,\n",
    "                                                     l_a_posteriori1[1:],\n",
    "                                                     P_a_posteriori1[1:]))\n",
    "            mu2,Sigma2 = self.likelihood_fn((mu_12,Sigma_12),(A_all,B_all,u_all,g_all,\n",
    "                                                     C_2,sigma2_all,\n",
    "                                                     l_a_posteriori2[1:],\n",
    "                                                     P_a_posteriori2[1:]))\n",
    "            mu3,Sigma3 = self.likelihood_fn((mu_13,Sigma_13),(A_all,B_all,u_all,g_all,\n",
    "                                                     C_3,sigma3_all,\n",
    "                                                     l_a_posteriori3[1:],\n",
    "                                                     P_a_posteriori3[1:]))\n",
    "            mu4,Sigma4 = self.likelihood_fn((mu_14,Sigma_14),(A_all,B_all,u_all,g_all,\n",
    "                                                     C_4,sigma4_all,\n",
    "                                                     l_a_posteriori4[1:],\n",
    "                                                     P_a_posteriori4[1:]))\n",
    "\n",
    "        '''p-quantile loss'''\n",
    "#         for i in range(Q50_numerator.shape[0]):\n",
    "#             for j in range(rewards):\n",
    "#                 Q50_numerator[i] += QL(0.5, preds[j][i], env_states[j][i])\n",
    "#                 Q90_numerator[i] += QL(0.9, preds[j][i], env_states[j][i])\n",
    "\n",
    "#         Q_denomenator = np.sum(np.abs(np.squeeze(np.array(env_states))), axis = 0)\n",
    "#         for idx,slot in enumerate(Q_denomenator):\n",
    "#             if slot==0:\n",
    "#                 Q_denomenator[idx]+=np.abs(np.random.normal(loc = 0.0, scale = 1e-10))\n",
    "#         pq50_loss = 2*np.divide(Q50_numerator,Q_denomenator)\n",
    "#         pq90_loss = 2*np.divide(Q90_numerator,Q_denomenator)\n",
    "\n",
    "        \n",
    "        '''Compute Likelihood of observations given KF evaluation'''\n",
    "#         likelihoods = []\n",
    "#         for i in range(rewards):\n",
    "#             z_distribution = MVNFull(loc = mu[i], covariance_matrix = Sigma[i])\n",
    "#             likelihoods.append(z_distribution.log_prob(env_states[i]))\n",
    "        z1_distribution = tfd.Normal(loc = mu1, scale = Sigma1)\n",
    "        z1_likelihood = z1_distribution.log_prob(env_states1)\n",
    "        z2_distribution = tfd.Normal(loc = mu2, scale = Sigma2)\n",
    "        z2_likelihood = z2_distribution.log_prob(env_states2)\n",
    "        z3_distribution = tfd.Normal(loc = mu3, scale = Sigma3)\n",
    "        z3_likelihood = z3_distribution.log_prob(env_states3)\n",
    "        z4_distribution = tfd.Normal(loc = mu4, scale = Sigma4)\n",
    "        z4_likelihood = z4_distribution.log_prob(env_states4)\n",
    "        print(z1_likelihood)\n",
    "        self.global_epoch += 1\n",
    "        return likelihoods, rewards, pq50_loss, pq90_loss, preds, env_states, squared_error, rbf\n",
    "\n",
    "def QL(rho, z, z_pred):\n",
    "    if z > z_pred:\n",
    "        return rho*(z-z_pred)\n",
    "    else:\n",
    "        return (1-rho)*(z_pred-z)\n",
    "    \n",
    "def standard_loss(model):\n",
    "    likelihoods, rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf = model()\n",
    "    loss = tf.Variable([0.0], trainable = False, dtype = tf.float64)\n",
    "    for loss_term in likelihoods:\n",
    "        loss = tf.add(loss,-loss_term)\n",
    "    return loss, rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf\n",
    "\n",
    "def inverse_multiplicative_loss(model):\n",
    "    '''This gives loss terms which are a multiple of their time step'''\n",
    "    likelihoods, rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf = model()\n",
    "    loss = tf.Variable([0.0], trainable = False, dtype = tf.float64)\n",
    "    for t,loss_term in enumerate(likelihoods):\n",
    "        loss = tf.add(loss,-(loss_term*(1/(t+1))))\n",
    "    return loss, rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf\n",
    "\n",
    "def multiplicative_loss(model):\n",
    "    '''This gives loss terms which are a multiple of their time step'''\n",
    "    likelihoods, rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf = model()\n",
    "    loss = tf.Variable([0.0], trainable = False, dtype = tf.float64)\n",
    "    for t,loss_term in enumerate(likelihoods):\n",
    "        loss = tf.add(loss,-(loss_term*t))\n",
    "    return loss, rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf\n",
    "\n",
    "def exponential_loss(model, alpha):\n",
    "    '''For alpha > 1 this gives exponentially increasing loss\n",
    "        For 0<alpha<1 this gives discounted loss'''\n",
    "    likelihoods, rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf = model()\n",
    "    loss = tf.Variable([0.0], trainable = False, dtype = tf.float64)\n",
    "    for t,loss_term in enumerate(likelihoods):\n",
    "        loss = tf.add(loss,-(tf.exponent(alpha,t)*loss_term))\n",
    "    return loss, rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf\n",
    "    \n",
    "\n",
    "def compute_gradient(model, loss_type, alpha = None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        if loss_type == 'standard':\n",
    "            loss_value, rewards, pq50_loss, pq90_loss, preds,\\\n",
    "            trajectory, squared_error, rbf = standard_loss(model)\n",
    "        elif loss_type == 'inverse_multiplicative':\n",
    "            loss_value, rewards, pq50_loss, pq90_loss, preds,\\\n",
    "            trajectory, squared_error, rbf = inverse_multiplicative_loss(model)\n",
    "        elif loss_type == 'multiplicative':\n",
    "            loss_value, rewards, pq50_loss, pq90_loss, preds,\\\n",
    "            trajectory, squared_error, rbf = multiplicative_loss(model)\n",
    "        elif loss_type == 'exponential':\n",
    "            loss_value, rewards, pq50_loss, pq90_loss, preds,\\\n",
    "            trajectory, squared_error, rbf = exponential_loss(model, alpha)\n",
    "#         print(tape.gradient(loss_value,epoch.get_variables()))\n",
    "#         print(len(tape.watched_variables()))\n",
    "#         for var in tape.watched_variables():\n",
    "#               print(var.name)\n",
    "    return (tape.gradient(loss_value, model.get_variables()), loss_value.numpy(),\\\n",
    "            rewards, pq50_loss, pq90_loss, preds, trajectory, squared_error, rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, optimizer, loss_type='standard'):\n",
    "    start = time.time()\n",
    "    for i in range(num_epochs):\n",
    "        grads, loss_, reward_, pq50, pq90, pred, trajectory,\\\n",
    "            squared_error, rbf = compute_gradient(model, loss_type)\n",
    "        \n",
    "        '''Keep track of loss, rewards, etc.'''\n",
    "        losses.extend(loss_)\n",
    "        rewards.append(reward_)\n",
    "        p50_losses.append(pq50)\n",
    "        p90_losses.append(pq90)\n",
    "        predicted_trajectories.append(pred)\n",
    "        actual_trajectories.append(trajectory)\n",
    "        squared_errors.append(squared_error)\n",
    "        rbfs.append(rbf)\n",
    "#         for idx, grad in enumerate(grads):\n",
    "#             grad_norms[idx].append(np.linalg.norm(grad))\n",
    "            \n",
    "        '''clip gradients'''\n",
    "#         clipped_grads = [tf.clip_by_value(grad_, -1.,1.) for grad_ in grads]\n",
    "        clipped_grads = [tf.clip_by_norm(grad, 1.) for grad in grads]\n",
    "        \n",
    "        \n",
    "        '''Apply the gradient update to variables'''\n",
    "        optimizer.apply_gradients(zip(clipped_grads,model.get_variables()))\n",
    "\n",
    "        \n",
    "        if (model.global_epoch+1)%50 == 0:\n",
    "            print('Epoch {}'.format(model.global_epoch+1))\n",
    "            print('Minutes elapsed: {}'.format((time.time()-start)/60))\n",
    "            print('Last 50 averages: Loss: {}, reward: {}, loss/reward: {}'.format(np.mean(losses[-50:]), np.mean(rewards[-50:]),(np.mean(losses[-50:])/np.mean(rewards[-50:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_model = split_KF_Model(initial_state_variation=[0,0,0.01,0])\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "losses = []\n",
    "rewards = []\n",
    "p50_losses = []\n",
    "p90_losses = []\n",
    "model_vars = ['A_dense/bias:0', 'A_dense/kernel:0', 'B_dense/bias:0', 'B_dense/kernel:0',\n",
    "              'Sigma_0dense/bias:0', 'Sigma_0dense/kernel:0', 'g_dense/bias:0', 'g_dense/kernel:0',\n",
    "              'mu_0dense/bias:0', 'mu_0dense/kernel:0', 'sigma_dense/bias:0', 'sigma_dense/kernel:0',\n",
    "              'multi_rnn_cell/cell_0/lstm_cell/kernel:0', 'multi_rnn_cell/cell_0/lstm_cell/bias:0',\n",
    "              'multi_rnn_cell/cell_1/lstm_cell/kernel:0', 'multi_rnn_cell/cell_1/lstm_cell/bias:0']\n",
    "grad_norms = [[var] for var in model_vars]\n",
    "predicted_trajectories = []\n",
    "actual_trajectories = []\n",
    "squared_errors = []\n",
    "rbfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "A tf.Tensor(\n",
      "[[-4.15157582e-02 -1.04603008e-02 -5.22044297e-02  2.73356410e-02]\n",
      " [-1.25735581e-03  5.63757044e-02 -6.25807272e-03  2.01553834e-02]\n",
      " [-2.83623327e-02  5.74025595e-02 -4.40334040e-05  5.02410253e-02]\n",
      " [-1.98348055e-02 -4.87299512e-02  3.90018747e-02  3.19151717e-02]], shape=(4, 4), dtype=float64)\n",
      "B tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]], shape=(4, 1), dtype=float64)\n",
      "u tf.Tensor([[0.]], shape=(1, 1), dtype=float64)\n",
      "g tf.Tensor(\n",
      "[[0.49500106]\n",
      " [0.52143307]\n",
      " [0.50224302]\n",
      " [0.46577478]], shape=(4, 1), dtype=float64)\n",
      "sigma tf.Tensor([[0.5049371]], shape=(1, 1), dtype=float64)\n",
      "C <tf.Variable 'Variable:0' shape=(1, 4) dtype=float64, numpy=array([[1., 0., 0., 0.]])>\n",
      "l_a_posteriori tf.Tensor(\n",
      "[[0.49578362]\n",
      " [0.49041773]\n",
      " [0.56442157]\n",
      " [0.47222269]], shape=(4, 1), dtype=float64)\n",
      "P_a_posteriori 1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute MatMul as input #0(zero-based) was expected to be a int32 tensor but is a double tensor [Op:MatMul] name: MatMul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-03cd4583617c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# train(model,200,tf.train.AdamOptimizer(1e-6),'multiplicative')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-54af0d2439d0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, optimizer, loss_type)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0msquared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m'''Keep track of loss, rewards, etc.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-84d0e138d9f4>\u001b[0m in \u001b[0;36mcompute_gradient\u001b[0;34m(model, loss_type, alpha)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'standard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq50_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq90_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mtrajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'inverse_multiplicative'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq50_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq90_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-84d0e138d9f4>\u001b[0m in \u001b[0;36mstandard_loss\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstandard_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0mlikelihoods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq50_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq90_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mloss_term\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlikelihoods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-84d0e138d9f4>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;31m#                                           tf.convert_to_tensor(observation,dtype=tf.float64))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             KF1_update = split_forward_filter_fn(A,B,u,g,C_1,sigma1,l_a_posteriori1[-1],P_a_posteriori1[-1],\n\u001b[0;32m--> 244\u001b[0;31m                                                 tf.convert_to_tensor(observation[0],dtype=tf.float64))\n\u001b[0m\u001b[1;32m    245\u001b[0m             KF2_update = split_forward_filter_fn(A,B,u,g,C_2,sigma2,l_a_posteriori2[-1],P_a_posteriori2[-1],\n\u001b[1;32m    246\u001b[0m                                                 tf.convert_to_tensor(observation[1],dtype=tf.float64))\n",
      "\u001b[0;32m<ipython-input-14-026a01ce9c6e>\u001b[0m in \u001b[0;36msplit_forward_filter_fn\u001b[0;34m(A, B, u, g, C, sigma, l_a_posteriori, P_a_posteriori, z)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ml_a_priori\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml_a_posteriori\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#     print('l_a_priori',l_a_priori)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mP_a_priori\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_a_posteriori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#     print('P_a_priori',P_a_priori)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml_a_priori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2453\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2455\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5321\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5322\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5323\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5324\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5325\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute MatMul as input #0(zero-based) was expected to be a int32 tensor but is a double tensor [Op:MatMul] name: MatMul/"
     ]
    }
   ],
   "source": [
    "train(split_model,1000,tf.train.AdamOptimizer())\n",
    "# train(model,200,tf.train.AdamOptimizer(1e-6),'multiplicative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.title('rewards')\n",
    "plt.show()\n",
    "plt.plot(losses)\n",
    "plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1000,50):\n",
    "    f, ax = plt.subplots(figsize=(25,8))\n",
    "    ax.plot(squared_errors[i])\n",
    "    plt.title(i)\n",
    "    ax.set_xlim([0.0,200])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedlabels = ['predicted x', 'predicted x dot', 'predicted theta', 'predicted theta dot']\n",
    "truelabels = ['true x','true x dot', 'true theta','true theta dot']\n",
    "for k in range(0,1000,50):\n",
    "    plt.figure(figsize=(15,6))\n",
    "    for j in range(4):\n",
    "        plt.plot([predicted_trajectories[k][i][j] for i in range(len(predicted_trajectories[k]))], label = predictedlabels[j])\n",
    "        plt.plot([actual_trajectories[k][i][j] for i in range(len(actual_trajectories[k]))], label = truelabels[j])\n",
    "#     plt.legend(['x', 'x dot','theta', 'theta dot', 'x', 'x dot','theta', 'theta dot'])\n",
    "    plt.legend()\n",
    "    plt.title(k)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
